name: Predict with TensorFlow model on ApacheParquet data
metadata:
  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Predict/on_ApacheParquet/component.yaml'}
inputs:
- {name: dataset, type: ApacheParquet}
- {name: model, type: TensorflowSavedModel}
- {name: label_column_name, type: String, optional: true}
- {name: batch_size, type: Integer, default: '1000', optional: true}
outputs:
- {name: predictions}
implementation:
  container:
    image: tensorflow/tensorflow:2.9.1
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'tensorflow-io==0.26.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
      --quiet --no-warn-script-location 'tensorflow-io==0.26.0' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def predict_with_TensorFlow_model_on_ApacheParquet_data(
          dataset_path,
          model_path,
          predictions_path,
          label_column_name = None,
          batch_size = 1000,
      ):
          import numpy
          import tensorflow as tf
          import tensorflow_io as tfio

          FEATURES_COLUMN_NAME = "features"

          model = tf.saved_model.load(export_dir=model_path)

          def stack_feature_batches(columns_batch_dict):
              label_batch = columns_batch_dict.pop(label_column_name.encode())
              if FEATURES_COLUMN_NAME in columns_batch_dict:
                  features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]
              else:
                  # Need to stack individual feature columns to create a single feature tensor
                  # Need to cast all column tensor types to float
                  list_of_feature_batches = list(
                      tf.cast(x=feature_batch, dtype=tf.float32)
                      for feature_batch in columns_batch_dict.values()
                  )
                  features_batch = tf.stack(list_of_feature_batches, axis=-1)
              return features_batch, label_batch

          # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values
          dataset = tfio.IODataset.from_parquet(filename=dataset_path)

          dataset = dataset.batch(
              batch_size=batch_size,
              num_parallel_calls=tf.data.AUTOTUNE,
              deterministic=True,
          ).map(
              map_func=stack_feature_batches,
              num_parallel_calls=tf.data.AUTOTUNE,
              deterministic=True,
          )

          with open(predictions_path, "w") as predictions_file:
              for features_batch, label_batch in dataset:
                  predictions_tensor = model(features_batch)
                  numpy.savetxt(predictions_file, predictions_tensor.numpy())

      import argparse
      _parser = argparse.ArgumentParser(prog='Predict with TensorFlow model on ApacheParquet data', description='')
      _parser.add_argument("--dataset", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = predict_with_TensorFlow_model_on_ApacheParquet_data(**_parsed_args)
    args:
    - --dataset
    - {inputPath: dataset}
    - --model
    - {inputPath: model}
    - if:
        cond: {isPresent: label_column_name}
        then:
        - --label-column-name
        - {inputValue: label_column_name}
    - if:
        cond: {isPresent: batch_size}
        then:
        - --batch-size
        - {inputValue: batch_size}
    - --predictions
    - {outputPath: predictions}
