annotations: {canonical_location: https://raw.githubusercontent.com/Ark-kun/pipeline_components/pipeline_component_feed/pipeline_component_feed.yaml}
components:
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml',
    digest: 9ad7aee6ca2fb82841c30cf2995b75c063f19b1f9c935bda1a9ef3d8ac1efed6}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert apache arrow feather to apache parquet
    description: |-
      Converts Apache Arrow Feather to Apache Parquet.

          [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
          [Apache Parquet](https://parquet.apache.org/)

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: ApacheArrowFeather}
    outputs:
    - {name: output_data, type: ApacheParquet}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_apache_arrow_feather_to_apache_parquet(
              data_path,
              output_data_path,
          ):
              '''Converts Apache Arrow Feather to Apache Parquet.

              [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
              [Apache Parquet](https://parquet.apache.org/)

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pyarrow import feather, parquet

              table = feather.read_table(data_path)
              parquet.write_table(table, output_data_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert apache arrow feather to apache parquet', description='Converts Apache Arrow Feather to Apache Parquet.\n\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = convert_apache_arrow_feather_to_apache_parquet(**_parsed_args)

          _output_serializers = [

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data
        - {inputPath: data}
        - --output-data
        - {outputPath: output_data}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_CSV/component.yaml',
    digest: 01be9da9b7e2c1d63a72090cb1c47abf9a581b037c4a477584fbfdf1dcf85c86}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert csv to apache parquet
    description: |-
      Converts CSV table to Apache Parquet.

          [Apache Parquet](https://parquet.apache.org/)

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: CSV}
    outputs:
    - {name: output_data, type: ApacheParquet}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_csv_to_apache_parquet(
              data_path,
              output_data_path,
          ):
              '''Converts CSV table to Apache Parquet.

              [Apache Parquet](https://parquet.apache.org/)

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pyarrow import csv, parquet

              table = csv.read_csv(data_path)
              parquet.write_table(table, output_data_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert csv to apache parquet', description='Converts CSV table to Apache Parquet.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = convert_csv_to_apache_parquet(**_parsed_args)

          _output_serializers = [

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data
        - {inputPath: data}
        - --output-data
        - {outputPath: output_data}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_TSV/component.yaml',
    digest: 50b66cb4e967d5fd1ea3ca0e9de8956b1f313c9f3abb271ca426b050d4573eac}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert tsv to apache parquet
    description: |-
      Converts TSV table to Apache Parquet.

          [Apache Parquet](https://parquet.apache.org/)

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: TSV}
    outputs:
    - {name: output_data, type: ApacheParquet}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_TSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_tsv_to_apache_parquet(
              data_path,
              output_data_path,
          ):
              '''Converts TSV table to Apache Parquet.

              [Apache Parquet](https://parquet.apache.org/)

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pyarrow import csv, parquet

              table = csv.read_csv(data_path, parse_options=csv.ParseOptions(delimiter='\t'))
              parquet.write_table(table, output_data_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert tsv to apache parquet', description='Converts TSV table to Apache Parquet.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = convert_tsv_to_apache_parquet(**_parsed_args)

          _output_serializers = [

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data
        - {inputPath: data}
        - --output-data
        - {outputPath: output_data}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml',
    digest: 12bef6256a23378cfc3f2e55aec28cee0855d1d1c558631ab2f79fb59f03b9dd}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert apache parquet to apache arrow feather
    description: |-
      Converts Apache Parquet to Apache Arrow Feather.

          [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
          [Apache Parquet](https://parquet.apache.org/)

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: ApacheParquet}
    outputs:
    - {name: output_data, type: ApacheArrowFeather}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pyarrow==0.17.1' 'pandas==1.0.3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_apache_parquet_to_apache_arrow_feather(
              data_path,
              output_data_path,
          ):
              '''Converts Apache Parquet to Apache Arrow Feather.

              [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)
              [Apache Parquet](https://parquet.apache.org/)

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pyarrow import feather, parquet

              data_frame = parquet.read_pandas(data_path).to_pandas()
              feather.write_feather(data_frame, output_data_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert apache parquet to apache arrow feather', description='Converts Apache Parquet to Apache Arrow Feather.\n\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = convert_apache_parquet_to_apache_arrow_feather(**_parsed_args)

          _output_serializers = [

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data
        - {inputPath: data}
        - --output-data
        - {outputPath: output_data}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/to_CSV/component.yaml',
    digest: 3618310a0fc658e73205ab0ea4f82f87b23e4919d00fe77057caff7f0908bcda}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert apache parquet to csv
    description: |-
      Converts Apache Parquet to CSV.

          [Apache Parquet](https://parquet.apache.org/)

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: ApacheParquet}
    outputs:
    - {name: output_data, type: CSV}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pyarrow==0.17.1' 'pandas==1.0.3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_apache_parquet_to_csv(
              data_path,
              output_data_path,
          ):
              '''Converts Apache Parquet to CSV.

              [Apache Parquet](https://parquet.apache.org/)

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pyarrow import parquet

              data_frame = parquet.read_pandas(data_path).to_pandas()
              data_frame.to_csv(
                  output_data_path,
                  index=False,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert apache parquet to csv', description='Converts Apache Parquet to CSV.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_apache_parquet_to_csv(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --output-data
        - {outputPath: output_data}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/to_TSV/component.yaml',
    digest: 5aa482972c8e325b242493c8075145549b5e845cec5d03ba4106eb1cdaa9e0d7}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert apache parquet to tsv
    description: |-
      Converts Apache Parquet to TSV.

          [Apache Parquet](https://parquet.apache.org/)

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: ApacheParquet}
    outputs:
    - {name: output_data, type: TSV}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_TSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pyarrow==0.17.1' 'pandas==1.0.3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_apache_parquet_to_tsv(
              data_path,
              output_data_path,
          ):
              '''Converts Apache Parquet to TSV.

              [Apache Parquet](https://parquet.apache.org/)

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pyarrow import parquet

              data_frame = parquet.read_pandas(data_path).to_pandas()
              data_frame.to_csv(
                  output_data_path,
                  index=False,
                  sep='\t',
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert apache parquet to tsv', description='Converts Apache Parquet to TSV.\n\n    [Apache Parquet](https://parquet.apache.org/)\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_apache_parquet_to_tsv(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --output-data
        - {outputPath: output_data}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/KerasModelHdf5/to_TensorflowSavedModel/component.yaml',
    digest: c17ff399e9d2465a81491adc275e6f6d31d181bf1bdfe007638cdb68063a7747}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Keras convert hdf5 model to tf saved model
    description: Converts Keras HDF5 model to Tensorflow SavedModel format.
    inputs:
    - {name: model, type: KerasModelHdf5, description: Keras model in HDF5 format.}
    outputs:
    - {name: converted_model, type: TensorflowSavedModel, description: Keras model in Tensorflow SavedModel format.}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/KerasModelHdf5/to_TensorflowSavedModel/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'h5py==2.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'h5py==2.10.0' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def keras_convert_hdf5_model_to_tf_saved_model(
              model_path,
              converted_model_path,
          ):
              '''Converts Keras HDF5 model to Tensorflow SavedModel format.

              Args:
                  model_path: Keras model in HDF5 format.
                  converted_model_path: Keras model in Tensorflow SavedModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pathlib import Path
              from tensorflow import keras

              model = keras.models.load_model(filepath=model_path)
              keras.models.save_model(model=model, filepath=converted_model_path, save_format='tf')

          import argparse
          _parser = argparse.ArgumentParser(prog='Keras convert hdf5 model to tf saved model', description='Converts Keras HDF5 model to Tensorflow SavedModel format.')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = keras_convert_hdf5_model_to_tf_saved_model(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --converted-model
        - {outputPath: converted_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/OnnxModel/from_KerasModelHdf5/component.yaml',
    digest: 7725a240ffcbc97f060c7f8a74ba2f52a022547756bb5c4fbd2f308bb56b2fab}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: To ONNX from Keras HDF5 model
    inputs:
    - {name: Model, type: KerasModelHdf5}
    outputs:
    - {name: Model, type: OnnxModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_KerasModelHdf5/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -exc
        - python3 -m pip install tf2onnx==1.6.3 && "$0" "$@"
        - python3
        - -m
        - tf2onnx.convert
        - --keras
        - {inputPath: Model}
        - --output
        - {outputPath: Model}
        - --fold_const
        - --verbose
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml',
    digest: d44f6b94387edfc6560bf452c5662de431d2e1141b670e7ab9e921eba25396a8}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: To ONNX from Tensorflow SavedModel
    inputs:
    - {name: Model, type: TensorflowSavedModel}
    outputs:
    - {name: Model, type: OnnxModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -exc
        - python3 -m pip install tf2onnx==1.6.3 && "$0" "$@"
        - python3
        - -m
        - tf2onnx.convert
        - --saved-model
        - {inputPath: Model}
        - --output
        - {outputPath: Model}
        - --fold_const
        - --verbose
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml',
    digest: b88f4b31bfc66bff3d9768655b2993bbc27b3b1d5f745faec9025ca7e01e66c6}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Convert to tensorflow saved model from onnx model\nmetadata:\n  annotations:\
    \ \n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location:\
    \ 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml'\n\
    inputs:\n- {name: model, type: OnnxModel}\noutputs:\n- {name: converted_model,\
    \ type: TensorflowModel}\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.4.1\n\
    \    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3\
    \ -m pip install --quiet --no-warn-script-location\n      'onnx-tf==1.7.0' 'onnx==1.8.0'\
    \ || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m\n      pip install --quiet --no-warn-script-location\
    \ 'onnx-tf==1.7.0' 'onnx==1.8.0'\n      --user) && \"$0\" \"$@\"\n    - sh\n \
    \   - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" >\
    \ \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n    \
    \  def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n\
    \          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return\
    \ file_path\n\n      def convert_to_tensorflow_saved_model_from_onnx_model(\n\
    \          model_path,\n          converted_model_path,\n      ):\n          import\
    \ onnx\n          import onnx_tf\n\n          onnx_model = onnx.load(model_path)\n\
    \          tf_rep = onnx_tf.backend.prepare(onnx_model)\n          tf_rep.export_graph(converted_model_path)\n\
    \n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert\
    \ to tensorflow saved model from onnx model', description='')\n      _parser.add_argument(\"\
    --model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
    \      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\"\
    , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
    \      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_tensorflow_saved_model_from_onnx_model(**_parsed_args)\n\
    \    args:\n    - --model\n    - {inputPath: model}\n    - --converted-model\n\
    \    - {outputPath: converted_model}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/TensorflowJSGraphModel/from_KerasModelHdf5/component.yaml',
    digest: 6242b06e95773082710318e3b842504ff0364601ff6dcea1f89be112a9002a85}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert Keras HDF5 model to Tensorflow JS GraphModel
    inputs:
    - {name: Model, type: KerasModelHdf5}
    outputs:
    - {name: Model, type: TensorflowJSGraphModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/TensorflowJSGraphModel/from_KerasModelHdf5/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -exc
        - |
          # Manually installing prerequisites so that tensorflowjs does not re-install tensorflow-cpu on top of tensorflow. See https://github.com/tensorflow/tfjs/issues/3953
          python3 -m pip install --quiet 'h5py>=2.8.0' 'numpy>=1.16.4,<1.19.0' 'six>=1.12.0' 'tensorflow-hub==0.7.0' 'PyInquirer==1.0.3'
          python3 -m pip install --quiet tensorflowjs==2.4.0 --no-dependencies
          "$0" "$*"
        - tensorflowjs_converter
        - --input_format=keras
        - --output_format=tfjs_graph_model
        - inputPath: Model
        - outputPath: Model
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/TensorflowJSGraphModel/from_TensorflowSavedModel/component.yaml',
    digest: a7898de04ef30eec1bb0c343a4a7009b72938ccbee1625a84c232f8361baf5fe}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert Tensorflow SavedModel to Tensorflow JS GraphModel
    inputs:
    - {name: Model, type: TensorflowSavedModel}
    outputs:
    - {name: Model, type: TensorflowJSGraphModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/TensorflowJSGraphModel/from_TensorflowSavedModel/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -exc
        - |
          # Manually installing prerequisites so that tensorflowjs does not re-install tensorflow-cpu on top of tensorflow. See https://github.com/tensorflow/tfjs/issues/3953
          python3 -m pip install --quiet 'h5py>=2.8.0' 'numpy>=1.16.4,<1.19.0' 'six>=1.12.0' 'tensorflow-hub==0.7.0' 'PyInquirer==1.0.3'
          python3 -m pip install --quiet tensorflowjs==2.4.0 --no-dependencies
          "$0" "$*"
        - tensorflowjs_converter
        - --input_format=tf_saved_model
        - --output_format=tfjs_graph_model
        - inputPath: Model
        - outputPath: Model
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/TensorflowJSLayersModel/from_KerasModelHdf5/component.yaml',
    digest: e802215dce7357620a8b9ba2dc3f55fa76c1485986e8f3a1cd981af6a4894bf8}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert Keras HDF5 model to Tensorflow JS LayersModel
    inputs:
    - {name: Model, type: KerasModelHdf5}
    outputs:
    - {name: Model, type: TensorflowJSLayersModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/TensorflowJSLayersModel/from_KerasModelHdf5/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -exc
        - |
          # Manually installing prerequisites so that tensorflowjs does not re-install tensorflow-cpu on top of tensorflow. See https://github.com/tensorflow/tfjs/issues/3953
          python3 -m pip install --quiet 'h5py>=2.8.0' 'numpy>=1.16.4,<1.19.0' 'six>=1.12.0' 'tensorflow-hub==0.7.0' 'PyInquirer==1.0.3'
          python3 -m pip install --quiet tensorflowjs==2.4.0 --no-dependencies
          "$0" "$*"
        - tensorflowjs_converter
        - --input_format=keras
        - --output_format=tfjs_layers_model
        - inputPath: Model
        - outputPath: Model
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/TensorflowJSLayersModel/from_TensorflowSavedModel/component.yaml',
    digest: 3a758a3328bfc352f8b2badd39c9743fb259de5094c6407da27a9f4720019f9b}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert Keras SavedModel to Tensorflow JS LayersModel
    inputs:
    - {name: Model, type: TensorflowSavedModel}
    outputs:
    - {name: Model, type: TensorflowJSLayersModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/TensorflowJSLayersModel/from_TensorflowSavedModel/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.3.0
        command:
        - sh
        - -exc
        - |
          # Manually installing prerequisites so that tensorflowjs does not re-install tensorflow-cpu on top of tensorflow. See https://github.com/tensorflow/tfjs/issues/3953
          python3 -m pip install --quiet 'h5py>=2.8.0' 'numpy>=1.16.4,<1.19.0' 'six>=1.12.0' 'tensorflow-hub==0.7.0' 'PyInquirer==1.0.3'
          python3 -m pip install --quiet tensorflowjs==2.4.0 --no-dependencies
          "$0" "$*"
        - tensorflowjs_converter
        - --input_format=keras_saved_model
        - --output_format=tfjs_layers_model
        - inputPath: Model
        - outputPath: Model
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/TensorflowLiteModel/from_KerasModelHdf5/component.yaml',
    digest: 6b405198fc1ecc30b63e02c97c1fe53c742adaaf41599fc9342dbc2e76bb44b7}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Convert Keras HDF5 model to Tensorflow Lite model\ninputs:\n- {name:\
    \ Model, type: KerasModelHdf5}\noutputs:\n- {name: Model, type: TensorflowLiteModel}\n\
    metadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\
    \    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/TensorflowLiteModel/from_KerasModelHdf5/component.yaml'\n\
    implementation:\n  container:\n    image: tensorflow/tensorflow:2.3.0\n    command:\n\
    \    - sh\n    - -exc\n    - |\n      model_path=\"$0\"\n      output_model_path=\"\
    $1\"\n      mkdir -p \"$(dirname \"$output_model_path\")\"\n      \n      tflite_convert\
    \ --keras_model_file \"$model_path\" --output_file \"$output_model_path\"\n  \
    \  - {inputPath: Model}\n    - {outputPath: Model}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/TensorflowLiteModel/from_TensorflowSavedModel/component.yaml',
    digest: b528673384cd88edeeb92379aea489202504d33850ee1e90c889b19ab97dba95}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Convert Tensorflow SavedModel to Tensorflow Lite model\ninputs:\n-\
    \ {name: Model, type: TensorflowSavedModel}\noutputs:\n- {name: Model, type: TensorflowLiteModel}\n\
    metadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\
    \    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/TensorflowLiteModel/from_TensorflowSavedModel/component.yaml'\n\
    implementation:\n  container:\n    image: tensorflow/tensorflow:2.3.0\n    command:\n\
    \    - sh\n    - -exc\n    - |\n      model_path=\"$0\"\n      output_model_path=\"\
    $1\"\n      mkdir -p \"$(dirname \"$output_model_path\")\"\n      \n      tflite_convert\
    \ --saved_model_dir \"$model_path\" --output_file \"$output_model_path\"\n   \
    \ - {inputPath: Model}\n    - {outputPath: Model}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml',
    digest: f82823ba99ab6fc6696e37685a00918aa94a9186b0a1979206b86ca3818099a4}
  annotations:
    GitHub commit:
      sha: 4fca0aa607c00e60d5eb342630acc175e6f51fc2
      html_url: https://github.com/Ark-kun/pipeline_components/commit/4fca0aa607c00e60d5eb342630acc175e6f51fc2
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/4fca0aa607c00e60d5eb342630acc175e6f51fc2
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-04T08:20:20Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-04T08:20:20Z'}
        message: Converters - XGBoostJsonModel - Added converters to and from XGBoostModel
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert to XGBoostJsonModel from XGBoostModel
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml'}
    inputs:
    - {name: model, type: XGBoostModel}
    outputs:
    - {name: converted_model, type: XGBoostJsonModel}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'xgboost==1.5.0' --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_to_XGBoostJsonModel_from_XGBoostModel(
              model_path,
              converted_model_path,
          ):
              import os
              import xgboost

              model = xgboost.Booster(model_file=model_path)

              # The file path needs to have .json extension so that the model is saved in the JSON format.
              tmp_converted_model_path = converted_model_path + ".json"
              model.save_model(tmp_converted_model_path)
              os.rename(tmp_converted_model_path, converted_model_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert to XGBoostJsonModel from XGBoostModel', description='')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_to_XGBoostJsonModel_from_XGBoostModel(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --converted-model
        - {outputPath: converted_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml',
    digest: b45de01b867736c16b2a68f53aa808c02f8e0caa2bffe2172aad7a20cdd17063}
  annotations:
    GitHub commit:
      sha: 4fca0aa607c00e60d5eb342630acc175e6f51fc2
      html_url: https://github.com/Ark-kun/pipeline_components/commit/4fca0aa607c00e60d5eb342630acc175e6f51fc2
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/4fca0aa607c00e60d5eb342630acc175e6f51fc2
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-04T08:20:20Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-04T08:20:20Z'}
        message: Converters - XGBoostJsonModel - Added converters to and from XGBoostModel
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert to XGBoostModel from XGBoostJsonModel
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml'}
    inputs:
    - {name: model, type: XGBoostJsonModel}
    outputs:
    - {name: converted_model, type: XGBoostModel}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'xgboost==1.5.0' --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_to_XGBoostModel_from_XGBoostJsonModel(
              model_path,
              converted_model_path,
          ):
              import os
              import shutil
              import tempfile
              import xgboost

              # The file path needs to have .json extension so that the model is loaded as JSON format.
              with tempfile.NamedTemporaryFile(suffix=".json") as tmp_model_file:
                  tmp_model_path = tmp_model_file.name
                  shutil.copy(model_path, tmp_model_path)
                  model = xgboost.Booster(model_file=tmp_model_path)

              tmp_converted_model_path = converted_model_path + ".bst"
              model.save_model(tmp_converted_model_path)
              os.rename(tmp_converted_model_path, converted_model_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert to XGBoostModel from XGBoostJsonModel', description='')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_to_XGBoostModel_from_XGBoostJsonModel(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --converted-model
        - {outputPath: converted_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/basics/Calculate_hash/component.yaml',
    digest: 9ab66addfecfb4cf2705a1d521d01ee2f930e1a5ff9d05e202c54a99eb9a1827}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Calculate data hash\ninputs:\n- {name: Data}\n- {name: Hash algorithm,\
    \ type: String, default: SHA256, description: \"Hash algorithm to use. Supported\
    \ values are MD5, SHA1, SHA256, SHA512, SHA3\"}\noutputs:\n- {name: Hash}\nmetadata:\n\
    \  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location:\
    \ 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/basics/Calculate_hash/component.yaml'\n\
    implementation:\n  container:\n    image: alpine\n    command:\n    - sh\n   \
    \ - -exc\n    - |\n      data_path=\"$0\"\n      hash_algorithm=\"$1\"\n     \
    \ hash_path=\"$2\"\n      mkdir -p \"$(dirname \"$hash_path\")\"\n      \n   \
    \   hash_algorithm=$(echo \"$hash_algorithm\" | tr '[:upper:]' '[:lower:]')\n\
    \      case \"$hash_algorithm\" in\n          md5|sha1|sha256|sha512|sha3)  hash_program=\"\
    ${hash_algorithm}sum\";;\n          *)  echo \"Unsupported hash algorithm $hash_algorithm\"\
    ; exit 1;;\n      esac\n      \n      if [ -d \"$data_path\" ]; then\n       \
    \   # Calculating hash for directory\n          cd \"$data_path\"\n          find\
    \ . -type f -print0 |\n              sort -z |\n              xargs -0 \"$hash_program\"\
    \ |\n              \"$hash_program\" |\n              cut -d ' ' -f 1 > \"$hash_path\"\
    \         \n      else\n          # Calculating hash for file\n          \"$hash_program\"\
    \ \"$data_path\" |\n              cut -d ' ' -f 1 > \"$hash_path\"\n      fi\n\
    \    - {inputPath: Data}\n    - {inputValue: Hash algorithm}\n    - {outputPath:\
    \ Hash}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/convert_CatBoostModel_to_AppleCoreMLModel/component.yaml',
    digest: 7b3f74e0c930c08b702da5fa055bedd9f4a06777a19b0d91efd8fdf8b6f68f47}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert CatBoostModel to AppleCoreMLModel
    description: |-
      Convert CatBoost model to Apple CoreML format.

          Args:
              model_path: Path of a trained model in binary CatBoost model format.
              converted_model_path: Output path for the converted model.

          Outputs:
              converted_model: Model in Apple CoreML format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: model, type: CatBoostModel}
    outputs:
    - {name: converted_model, type: AppleCoreMLModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/convert_CatBoostModel_to_AppleCoreMLModel/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_CatBoostModel_to_AppleCoreMLModel(
              model_path,
              converted_model_path,
          ):
              '''Convert CatBoost model to Apple CoreML format.

              Args:
                  model_path: Path of a trained model in binary CatBoost model format.
                  converted_model_path: Output path for the converted model.

              Outputs:
                  converted_model: Model in Apple CoreML format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from catboost import CatBoost

              model = CatBoost()
              model.load_model(model_path)
              model.save_model(
                  converted_model_path,
                  format="coreml",
                  # export_parameters={'prediction_type': 'probability'},
                  # export_parameters={'prediction_type': 'raw'},
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert CatBoostModel to AppleCoreMLModel', description='Convert CatBoost model to Apple CoreML format.\n\n    Args:\n        model_path: Path of a trained model in binary CatBoost model format.\n        converted_model_path: Output path for the converted model.\n\n    Outputs:\n        converted_model: Model in Apple CoreML format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_CatBoostModel_to_AppleCoreMLModel(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --converted-model
        - {outputPath: converted_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml',
    digest: f56b73b1e1e6b9e8745a4dfb49423ec33194adde2aac0b9d20d4c7bf77731907}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert CatBoostModel to ONNX
    description: |-
      Convert CatBoost model to ONNX format.

          Args:
              model_path: Path of a trained model in binary CatBoost model format.
              converted_model_path: Output path for the converted model.

          Outputs:
              converted_model: Model in ONNX format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: model, type: CatBoostModel}
    outputs:
    - {name: converted_model, type: ONNX}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_CatBoostModel_to_ONNX(
              model_path,
              converted_model_path,
          ):
              '''Convert CatBoost model to ONNX format.

              Args:
                  model_path: Path of a trained model in binary CatBoost model format.
                  converted_model_path: Output path for the converted model.

              Outputs:
                  converted_model: Model in ONNX format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from catboost import CatBoost

              model = CatBoost()
              model.load_model(model_path)
              model.save_model(converted_model_path, format="onnx")

          import argparse
          _parser = argparse.ArgumentParser(prog='Convert CatBoostModel to ONNX', description='Convert CatBoost model to ONNX format.\n\n    Args:\n        model_path: Path of a trained model in binary CatBoost model format.\n        converted_model_path: Output path for the converted model.\n\n    Outputs:\n        converted_model: Model in ONNX format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_CatBoostModel_to_ONNX(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --converted-model
        - {outputPath: converted_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml',
    digest: 9074952dc787000001b2f75291a287a1cc968648c711a8f648605fd0053999bb}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Catboost predict class probabilities
    description: |-
      Predict class probabilities with a CatBoost model.

          Args:
              data_path: Path for the data in CSV format.
              model_path: Path for the trained model in binary CatBoostModel format.
              label_column: Column containing the label data.
              predictions_path: Output path for the predictions.

          Outputs:
              predictions: Predictions in text format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: CSV}
    - {name: model, type: CatBoostModel}
    - {name: label_column, type: Integer, optional: true}
    outputs:
    - {name: predictions}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def catboost_predict_class_probabilities(
              data_path,
              model_path,
              predictions_path,

              label_column = None,
          ):
              '''Predict class probabilities with a CatBoost model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import tempfile

              from catboost import CatBoost, Pool
              import numpy

              if label_column:
                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))
              else:
                  column_description_path = None

              eval_data = Pool(
                  data_path,
                  column_description=column_description_path,
                  has_header=True,
                  delimiter=',',
              )

              model = CatBoost()
              model.load_model(model_path)

              predictions = model.predict(eval_data, prediction_type='Probability')
              numpy.savetxt(predictions_path, predictions)

          import argparse
          _parser = argparse.ArgumentParser(prog='Catboost predict class probabilities', description='Predict class probabilities with a CatBoost model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = catboost_predict_class_probabilities(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --model
        - {inputPath: model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - --predictions
        - {outputPath: predictions}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_classes/from_CSV/component.yaml',
    digest: 6e45f6a3b7a7805aa3c99a9d6c1bbf7434c365ad6f23de930014902bf19dc78f}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Catboost predict classes
    description: |-
      Predict classes using the CatBoost classifier model.

          Args:
              data_path: Path for the data in CSV format.
              model_path: Path for the trained model in binary CatBoostModel format.
              label_column: Column containing the label data.
              predictions_path: Output path for the predictions.

          Outputs:
              predictions: Class predictions in text format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: CSV}
    - {name: model, type: CatBoostModel}
    - {name: label_column, type: Integer, optional: true}
    outputs:
    - {name: predictions}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_classes/from_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.22' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def catboost_predict_classes(
              data_path,
              model_path,
              predictions_path,

              label_column = None,
          ):
              '''Predict classes using the CatBoost classifier model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Class predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import tempfile

              from catboost import CatBoostClassifier, Pool
              import numpy

              if label_column:
                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))
              else:
                  column_description_path = None

              eval_data = Pool(
                  data_path,
                  column_description=column_description_path,
                  has_header=True,
                  delimiter=',',
              )

              model = CatBoostClassifier()
              model.load_model(model_path)

              predictions = model.predict(eval_data)
              numpy.savetxt(predictions_path, predictions, fmt='%s')

          import argparse
          _parser = argparse.ArgumentParser(prog='Catboost predict classes', description='Predict classes using the CatBoost classifier model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Class predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = catboost_predict_classes(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --model
        - {inputPath: model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - --predictions
        - {outputPath: predictions}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_values/from_CSV/component.yaml',
    digest: 64f4651cd80d1bdbd524ff0cd2272916ebd3855f7b1f71d8abf474ccb0ec3dfb}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Catboost predict values
    description: |-
      Predict values with a CatBoost model.

          Args:
              data_path: Path for the data in CSV format.
              model_path: Path for the trained model in binary CatBoostModel format.
              label_column: Column containing the label data.
              predictions_path: Output path for the predictions.

          Outputs:
              predictions: Predictions in text format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: CSV}
    - {name: model, type: CatBoostModel}
    - {name: label_column, type: Integer, optional: true}
    outputs:
    - {name: predictions}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_values/from_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def catboost_predict_values(
              data_path,
              model_path,
              predictions_path,

              label_column = None,
          ):
              '''Predict values with a CatBoost model.

              Args:
                  data_path: Path for the data in CSV format.
                  model_path: Path for the trained model in binary CatBoostModel format.
                  label_column: Column containing the label data.
                  predictions_path: Output path for the predictions.

              Outputs:
                  predictions: Predictions in text format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import tempfile

              from catboost import CatBoost, Pool
              import numpy

              if label_column:
                  column_descriptions = {label_column: 'Label'}
                  column_description_path = tempfile.NamedTemporaryFile(delete=False).name
                  with open(column_description_path, 'w') as column_description_file:
                      for idx, kind in column_descriptions.items():
                          column_description_file.write('{}\t{}\n'.format(idx, kind))
              else:
                  column_description_path = None

              eval_data = Pool(
                  data_path,
                  column_description=column_description_path,
                  has_header=True,
                  delimiter=',',
              )

              model = CatBoost()
              model.load_model(model_path)

              predictions = model.predict(eval_data, prediction_type='RawFormulaVal')
              numpy.savetxt(predictions_path, predictions)

          import argparse
          _parser = argparse.ArgumentParser(prog='Catboost predict values', description='Predict values with a CatBoost model.\n\n    Args:\n        data_path: Path for the data in CSV format.\n        model_path: Path for the trained model in binary CatBoostModel format.\n        label_column: Column containing the label data.\n        predictions_path: Output path for the predictions.\n\n    Outputs:\n        predictions: Predictions in text format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = catboost_predict_values(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --model
        - {inputPath: model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - --predictions
        - {outputPath: predictions}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_classifier/from_CSV/component.yaml',
    digest: f4ef9b474377248847c6487a827aa7b14d092a8d031c368168f2857a8ff11cad}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Catboost train classifier
    description: |-
      Train a CatBoost classifier model.

          Args:
              training_data_path: Path for the training data in CSV format.
              model_path: Output path for the trained model in binary CatBoostModel format.
              starting_model_path: Path for the existing trained model to start from.
              label_column: Column containing the label data.

              loss_function: The metric to use in training and also selector of the machine learning
                  problem to solve. Default = 'Logloss'
              num_iterations: Number of trees to add to the ensemble.
              learning_rate: Step size shrinkage used in update to prevents overfitting.
                  Default value is selected automatically for binary classification with other parameters set to default.
                  In all other cases default is 0.03.
              depth: Depth of a tree. All trees are the same depth. Default = 6
              random_seed: Random number seed. Default = 0

              cat_features: A list of Categorical features (indices or names).
              text_features: A list of Text features (indices or names).
              additional_training_options: A dictionary with additional options to pass to CatBoostClassifier

          Outputs:
              model: Trained model in binary CatBoostModel format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: training_data, type: CSV}
    - {name: starting_model, type: CatBoostModel, optional: true}
    - {name: label_column, type: Integer, default: '0', optional: true}
    - {name: loss_function, type: String, default: Logloss, optional: true}
    - {name: num_iterations, type: Integer, default: '500', optional: true}
    - {name: learning_rate, type: Float, optional: true}
    - {name: depth, type: Integer, default: '6', optional: true}
    - {name: random_seed, type: Integer, default: '0', optional: true}
    - {name: cat_features, type: JsonArray, optional: true}
    - {name: text_features, type: JsonArray, optional: true}
    - {name: additional_training_options, type: JsonObject, default: '{}', optional: true}
    outputs:
    - {name: model, type: CatBoostModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_classifier/from_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def catboost_train_classifier(
              training_data_path,
              model_path,
              starting_model_path = None,
              label_column = 0,

              loss_function = 'Logloss',
              num_iterations = 500,
              learning_rate = None,
              depth = 6,
              random_seed = 0,

              cat_features = None,
              text_features = None,

              additional_training_options = {},
          ):
              '''Train a CatBoost classifier model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary CatBoostModel format.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.

                  loss_function: The metric to use in training and also selector of the machine learning
                      problem to solve. Default = 'Logloss'
                  num_iterations: Number of trees to add to the ensemble.
                  learning_rate: Step size shrinkage used in update to prevents overfitting.
                      Default value is selected automatically for binary classification with other parameters set to default.
                      In all other cases default is 0.03.
                  depth: Depth of a tree. All trees are the same depth. Default = 6
                  random_seed: Random number seed. Default = 0

                  cat_features: A list of Categorical features (indices or names).
                  text_features: A list of Text features (indices or names).
                  additional_training_options: A dictionary with additional options to pass to CatBoostClassifier

              Outputs:
                  model: Trained model in binary CatBoostModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import tempfile
              from pathlib import Path

              from catboost import CatBoostClassifier, Pool

              column_descriptions = {label_column: 'Label'}
              column_description_path = tempfile.NamedTemporaryFile(delete=False).name
              with open(column_description_path, 'w') as column_description_file:
                  for idx, kind in column_descriptions.items():
                      column_description_file.write('{}\t{}\n'.format(idx, kind))

              train_data = Pool(
                  training_data_path,
                  column_description=column_description_path,
                  has_header=True,
                  delimiter=',',
              )

              model = CatBoostClassifier(
                  iterations=num_iterations,
                  depth=depth,
                  learning_rate=learning_rate,
                  loss_function=loss_function,
                  random_seed=random_seed,
                  verbose=True,
                  **additional_training_options,
              )

              model.fit(
                  train_data,
                  cat_features=cat_features,
                  text_features=text_features,
                  init_model=starting_model_path,
                  #verbose=False,
                  #plot=True,
              )
              Path(model_path).parent.mkdir(parents=True, exist_ok=True)
              model.save_model(model_path)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Catboost train classifier', description="Train a CatBoost classifier model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary CatBoostModel format.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n\n        loss_function: The metric to use in training and also selector of the machine learning\n            problem to solve. Default = 'Logloss'\n        num_iterations: Number of trees to add to the ensemble.\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\n            Default value is selected automatically for binary classification with other parameters set to default.\n            In all other cases default is 0.03.\n        depth: Depth of a tree. All trees are the same depth. Default = 6\n        random_seed: Random number seed. Default = 0\n\n        cat_features: A list of Categorical features (indices or names).\n        text_features: A list of Text features (indices or names).\n        additional_training_options: A dictionary with additional options to pass to CatBoostClassifier\n\n    Outputs:\n        model: Trained model in binary CatBoostModel format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
          _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--loss-function", dest="loss_function", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--depth", dest="depth", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--cat-features", dest="cat_features", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--text-features", dest="text_features", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--additional-training-options", dest="additional_training_options", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = catboost_train_classifier(**_parsed_args)
        args:
        - --training-data
        - {inputPath: training_data}
        - if:
            cond: {isPresent: starting_model}
            then:
            - --starting-model
            - {inputPath: starting_model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - if:
            cond: {isPresent: loss_function}
            then:
            - --loss-function
            - {inputValue: loss_function}
        - if:
            cond: {isPresent: num_iterations}
            then:
            - --num-iterations
            - {inputValue: num_iterations}
        - if:
            cond: {isPresent: learning_rate}
            then:
            - --learning-rate
            - {inputValue: learning_rate}
        - if:
            cond: {isPresent: depth}
            then:
            - --depth
            - {inputValue: depth}
        - if:
            cond: {isPresent: random_seed}
            then:
            - --random-seed
            - {inputValue: random_seed}
        - if:
            cond: {isPresent: cat_features}
            then:
            - --cat-features
            - {inputValue: cat_features}
        - if:
            cond: {isPresent: text_features}
            then:
            - --text-features
            - {inputValue: text_features}
        - if:
            cond: {isPresent: additional_training_options}
            then:
            - --additional-training-options
            - {inputValue: additional_training_options}
        - --model
        - {outputPath: model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_regression/from_CSV/component.yaml',
    digest: ac04356d7b08e9d3e90279befd4df446832641517efb34dea75354312347295a}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Catboost train regression
    description: |-
      Train a CatBoost classifier model.

          Args:
              training_data_path: Path for the training data in CSV format.
              model_path: Output path for the trained model in binary CatBoostModel format.
              starting_model_path: Path for the existing trained model to start from.
              label_column: Column containing the label data.

              loss_function: The metric to use in training and also selector of the machine learning
                  problem to solve. Default = 'RMSE'. Possible values:
                  'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'
              num_iterations: Number of trees to add to the ensemble.
              learning_rate: Step size shrinkage used in update to prevents overfitting.
                  Default value is selected automatically for binary classification with other parameters set to default.
                  In all other cases default is 0.03.
              depth: Depth of a tree. All trees are the same depth. Default = 6
              random_seed: Random number seed. Default = 0

              cat_features: A list of Categorical features (indices or names).
              additional_training_options: A dictionary with additional options to pass to CatBoostRegressor

          Outputs:
              model: Trained model in binary CatBoostModel format.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: training_data, type: CSV}
    - {name: starting_model, type: CatBoostModel, optional: true}
    - {name: label_column, type: Integer, default: '0', optional: true}
    - {name: loss_function, type: String, default: RMSE, optional: true}
    - {name: num_iterations, type: Integer, default: '500', optional: true}
    - {name: learning_rate, type: Float, optional: true}
    - {name: depth, type: Integer, default: '6', optional: true}
    - {name: random_seed, type: Integer, default: '0', optional: true}
    - {name: cat_features, type: JsonArray, optional: true}
    - {name: additional_training_options, type: JsonObject, default: '{}', optional: true}
    outputs:
    - {name: model, type: CatBoostModel}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_regression/from_CSV/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'catboost==0.23' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def catboost_train_regression(
              training_data_path,
              model_path,
              starting_model_path = None,
              label_column = 0,

              loss_function = 'RMSE',
              num_iterations = 500,
              learning_rate = None,
              depth = 6,
              random_seed = 0,

              cat_features = None,

              additional_training_options = {},
          ):
              '''Train a CatBoost classifier model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary CatBoostModel format.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.

                  loss_function: The metric to use in training and also selector of the machine learning
                      problem to solve. Default = 'RMSE'. Possible values:
                      'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'
                  num_iterations: Number of trees to add to the ensemble.
                  learning_rate: Step size shrinkage used in update to prevents overfitting.
                      Default value is selected automatically for binary classification with other parameters set to default.
                      In all other cases default is 0.03.
                  depth: Depth of a tree. All trees are the same depth. Default = 6
                  random_seed: Random number seed. Default = 0

                  cat_features: A list of Categorical features (indices or names).
                  additional_training_options: A dictionary with additional options to pass to CatBoostRegressor

              Outputs:
                  model: Trained model in binary CatBoostModel format.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import tempfile
              from pathlib import Path

              from catboost import CatBoostRegressor, Pool

              column_descriptions = {label_column: 'Label'}
              column_description_path = tempfile.NamedTemporaryFile(delete=False).name
              with open(column_description_path, 'w') as column_description_file:
                  for idx, kind in column_descriptions.items():
                      column_description_file.write('{}\t{}\n'.format(idx, kind))

              train_data = Pool(
                  training_data_path,
                  column_description=column_description_path,
                  has_header=True,
                  delimiter=',',
              )

              model = CatBoostRegressor(
                  iterations=num_iterations,
                  depth=depth,
                  learning_rate=learning_rate,
                  loss_function=loss_function,
                  random_seed=random_seed,
                  verbose=True,
                  **additional_training_options,
              )

              model.fit(
                  train_data,
                  cat_features=cat_features,
                  init_model=starting_model_path,
                  #verbose=False,
                  #plot=True,
              )
              Path(model_path).parent.mkdir(parents=True, exist_ok=True)
              model.save_model(model_path)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Catboost train regression', description="Train a CatBoost classifier model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary CatBoostModel format.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n\n        loss_function: The metric to use in training and also selector of the machine learning\n            problem to solve. Default = 'RMSE'. Possible values:\n            'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'\n        num_iterations: Number of trees to add to the ensemble.\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\n            Default value is selected automatically for binary classification with other parameters set to default.\n            In all other cases default is 0.03.\n        depth: Depth of a tree. All trees are the same depth. Default = 6\n        random_seed: Random number seed. Default = 0\n\n        cat_features: A list of Categorical features (indices or names).\n        additional_training_options: A dictionary with additional options to pass to CatBoostRegressor\n\n    Outputs:\n        model: Trained model in binary CatBoostModel format.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
          _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--loss-function", dest="loss_function", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--depth", dest="depth", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--cat-features", dest="cat_features", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--additional-training-options", dest="additional_training_options", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = catboost_train_regression(**_parsed_args)
        args:
        - --training-data
        - {inputPath: training_data}
        - if:
            cond: {isPresent: starting_model}
            then:
            - --starting-model
            - {inputPath: starting_model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - if:
            cond: {isPresent: loss_function}
            then:
            - --loss-function
            - {inputValue: loss_function}
        - if:
            cond: {isPresent: num_iterations}
            then:
            - --num-iterations
            - {inputValue: num_iterations}
        - if:
            cond: {isPresent: learning_rate}
            then:
            - --learning-rate
            - {inputValue: learning_rate}
        - if:
            cond: {isPresent: depth}
            then:
            - --depth
            - {inputValue: depth}
        - if:
            cond: {isPresent: random_seed}
            then:
            - --random-seed
            - {inputValue: random_seed}
        - if:
            cond: {isPresent: cat_features}
            then:
            - --cat-features
            - {inputValue: cat_features}
        - if:
            cond: {isPresent: additional_training_options}
            then:
            - --additional-training-options
            - {inputValue: additional_training_options}
        - --model
        - {outputPath: model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml',
    digest: 46eff5c8c007ed9f6358c49c86dd335164043cca0c643828b698c875c0b29e11}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Split table into folds
    description: |-
      Splits the data table into the specified number of folds.

          The data is split into the specified number of folds k (default: 5).
          Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
          Each training subsample has (k-1)/k fraction of samples.
          The train_i subsample is produced by excluding test_i subsample form all samples.

          Inputs:
              table: The data to split by rows
              number_of_folds: Number of folds to split data into
              random_seed: Random seed for reproducible splitting

          Outputs:
              train_i: The i-th training subsample
              test_i: The i-th testing subsample

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'
    inputs:
    - {name: table, type: CSV}
    - {name: number_of_folds, type: Integer, default: '5', optional: true}
    - {name: random_seed, type: Integer, default: '0', optional: true}
    outputs:
    - {name: train_1, type: CSV}
    - {name: train_2, type: CSV}
    - {name: train_3, type: CSV}
    - {name: train_4, type: CSV}
    - {name: train_5, type: CSV}
    - {name: test_1, type: CSV}
    - {name: test_2, type: CSV}
    - {name: test_3, type: CSV}
    - {name: test_4, type: CSV}
    - {name: test_5, type: CSV}
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'scikit-learn==0.23.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def split_table_into_folds(
              table_path,

              train_1_path,
              train_2_path,
              train_3_path,
              train_4_path,
              train_5_path,

              test_1_path,
              test_2_path,
              test_3_path,
              test_4_path,
              test_5_path,

              number_of_folds = 5,
              random_seed = 0,
          ):
              """Splits the data table into the specified number of folds.

              The data is split into the specified number of folds k (default: 5).
              Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
              Each training subsample has (k-1)/k fraction of samples.
              The train_i subsample is produced by excluding test_i subsample form all samples.

              Inputs:
                  table: The data to split by rows
                  number_of_folds: Number of folds to split data into
                  random_seed: Random seed for reproducible splitting

              Outputs:
                  train_i: The i-th training subsample
                  test_i: The i-th testing subsample

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              """
              import pandas
              from sklearn import model_selection

              max_number_of_folds = 5

              if number_of_folds < 1 or number_of_folds > max_number_of_folds:
                  raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))

              df = pandas.read_csv(
                  table_path,
              )
              splitter = model_selection.KFold(
                  n_splits=number_of_folds,
                  shuffle=True,
                  random_state=random_seed,
              )
              folds = list(splitter.split(df))

              fold_paths = [
                  (train_1_path, test_1_path),
                  (train_2_path, test_2_path),
                  (train_3_path, test_3_path),
                  (train_4_path, test_4_path),
                  (train_5_path, test_5_path),
              ]

              for i in range(max_number_of_folds):
                  (train_path, test_path) = fold_paths[i]
                  if i < len(folds):
                      (train_indices, test_indices) = folds[i]
                      train_fold = df.iloc[train_indices]
                      test_fold = df.iloc[test_indices]
                  else:
                      train_fold = df.iloc[0:0]
                      test_fold = df.iloc[0:0]
                  train_fold.to_csv(train_path, index=False)
                  test_fold.to_csv(test_path, index=False)

          import argparse
          _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.\n\n    The data is split into the specified number of folds k (default: 5).\n    Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.\n    Each training subsample has (k-1)/k fraction of samples.\n    The train_i subsample is produced by excluding test_i subsample form all samples.\n\n    Inputs:\n        table: The data to split by rows\n        number_of_folds: Number of folds to split data into\n        random_seed: Random seed for reproducible splitting\n\n    Outputs:\n        train_i: The i-th training subsample\n        test_i: The i-th testing subsample\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--number-of-folds", dest="number_of_folds", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--train-1", dest="train_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--train-2", dest="train_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--train-3", dest="train_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--train-4", dest="train_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--train-5", dest="train_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--test-1", dest="test_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--test-2", dest="test_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--test-3", dest="test_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--test-4", dest="test_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--test-5", dest="test_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = split_table_into_folds(**_parsed_args)
        args:
        - --table
        - {inputPath: table}
        - if:
            cond: {isPresent: number_of_folds}
            then:
            - --number-of-folds
            - {inputValue: number_of_folds}
        - if:
            cond: {isPresent: random_seed}
            then:
            - --random-seed
            - {inputValue: random_seed}
        - --train-1
        - {outputPath: train_1}
        - --train-2
        - {outputPath: train_2}
        - --train-3
        - {outputPath: train_3}
        - --train-4
        - {outputPath: train_4}
        - --train-5
        - {outputPath: train_5}
        - --test-1
        - {outputPath: test_1}
        - --test-2
        - {outputPath: test_2}
        - --test-3
        - {outputPath: test_3}
        - --test-4
        - {outputPath: test_4}
        - --test-5
        - {outputPath: test_5}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/Chicago_Taxi_Trips/component.yaml',
    digest: c91392ee01c340e50cd90e4bb402458f022d634bbb7a60eadfa888c9334247e5}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Chicago Taxi Trips dataset
    description: |
      City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

      The input parameters configure the SQL query to the database.
      The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
      Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/component.yaml'
    inputs:
    - {name: Where, type: String, default: 'trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"'}
    - {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}
    - {name: Select, type: String, default: 'trip_id,taxi_id,trip_start_timestamp,trip_end_timestamp,trip_seconds,trip_miles,pickup_census_tract,dropoff_census_tract,pickup_community_area,dropoff_community_area,fare,tips,tolls,extras,trip_total,payment_type,company,pickup_centroid_latitude,pickup_centroid_longitude,pickup_centroid_location,dropoff_centroid_latitude,dropoff_centroid_longitude,dropoff_centroid_location'}
    - {name: Format, type: String, default: 'csv', description: 'Output data format. Suports csv,tsv,cml,rdf,json'}
    outputs:
    - {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}
    implementation:
      container:
        # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
        image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
        command:
        - sh
        - -c
        - |
          set -e -x -o pipefail
          output_path="$0"
          select="$1"
          where="$2"
          limit="$3"
          format="$4"
          mkdir -p "$(dirname "$output_path")"
          curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
              --data-urlencode '$limit='"${limit}" \
              --data-urlencode '$where='"${where}" \
              --data-urlencode '$select='"${select}" \
              | tr -d '"' > "$output_path"  # Removing unneeded quotes around all numbers
        - {outputPath: Table}
        - {inputValue: Select}
        - {inputValue: Where}
        - {inputValue: Limit}
        - {inputValue: Format}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/HuggingFace/Load_dataset/component.yaml',
    digest: 976ecd3d14acb52281694ae5f56da187ad0d2adf3ef4846e59299cb3c1d85851}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Load dataset using huggingface\nmetadata:\n  annotations: \n    author:\
    \ Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/HuggingFace/Load_dataset/component.yaml'\n\
    inputs:\n- {name: dataset_name, type: String}\noutputs:\n- {name: dataset_dict,\
    \ type: HuggingFaceDatasetDict}\n- {name: splits, type: JsonArray}\nimplementation:\n\
    \  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    -\
    \ (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n\
    \      'datasets==1.6.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n\
    \      --quiet --no-warn-script-location 'datasets==1.6.2' --user) && \"$0\" \"\
    $@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf\
    \ \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\
    \n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n    \
    \      import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\
    \          return file_path\n\n      def load_dataset_using_huggingface(\n   \
    \       dataset_name,\n          dataset_dict_path,\n      ):\n          from\
    \ datasets import load_dataset\n\n          dataset_dict = load_dataset(dataset_name)\n\
    \          dataset_dict.save_to_disk(dataset_dict_path)\n          splits = list(dataset_dict.keys())\n\
    \          return (splits,)\n\n      def _serialize_json(obj) -> str:\n      \
    \    if isinstance(obj, str):\n              return obj\n          import json\n\
    \          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n\
    \                  return obj.to_struct()\n              else:\n             \
    \     raise TypeError(\"Object of type '%s' is not JSON serializable and does\
    \ not have .to_struct() method.\" % obj.__class__.__name__)\n          return\
    \ json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import\
    \ argparse\n      _parser = argparse.ArgumentParser(prog='Load dataset using huggingface',\
    \ description='')\n      _parser.add_argument(\"--dataset-name\", dest=\"dataset_name\"\
    , type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"\
    --dataset-dict\", dest=\"dataset_dict_path\", type=_make_parent_dirs_and_return_path,\
    \ required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\"\
    , dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n\
    \      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs\
    \ = load_dataset_using_huggingface(**_parsed_args)\n\n      _output_serializers\
    \ = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx,\
    \ output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n\
    \          except OSError:\n              pass\n          with open(output_file,\
    \ 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n\
    \    args:\n    - --dataset-name\n    - {inputValue: dataset_name}\n    - --dataset-dict\n\
    \    - {outputPath: dataset_dict}\n    - '----output-paths'\n    - {outputPath:\
    \ splits}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/HuggingFace/Split_dataset/component.yaml',
    digest: 0fbe1144561e49b9bfe88c59117f3fdfac1635c642cfc35bf771e9ffa798bc7e}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Split dataset huggingface\nmetadata:\n  annotations: \n    author:\
    \ Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/HuggingFace/Split_dataset/component.yaml'\n\
    inputs:\n- {name: dataset_dict, type: HuggingFaceDatasetDict}\n- {name: split_name,\
    \ type: String, optional: true}\noutputs:\n- {name: dataset_split, type: HuggingFaceDataset}\n\
    - {name: dataset, type: HuggingFaceArrowDataset}\n- {name: dataset_info, type:\
    \ JsonObject}\n- {name: dataset_state, type: JsonObject}\nimplementation:\n  container:\n\
    \    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1\
    \ python3 -m pip install --quiet --no-warn-script-location\n      'datasets==1.6.2'\
    \ || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n      --quiet --no-warn-script-location\
    \ 'datasets==1.6.2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n \
    \     program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n\
    \      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path:\
    \ str):\n          import os\n          os.makedirs(os.path.dirname(file_path),\
    \ exist_ok=True)\n          return file_path\n\n      def split_dataset_huggingface(\n\
    \          dataset_dict_path,\n          dataset_split_path,\n          dataset_path,\n\
    \          # dataset_indices_path: OutputPath('HuggingFaceArrowDataset'),\n  \
    \        dataset_info_path,\n          dataset_state_path,\n          split_name\
    \ = None,\n      ):\n          import os\n          import shutil\n          from\
    \ datasets import config as datasets_config\n\n          print(f'DatasetDict contents:\
    \ {os.listdir(dataset_dict_path)}')\n          shutil.copytree(os.path.join(dataset_dict_path,\
    \ split_name), dataset_split_path)\n          print(f'Dataset contents: {os.listdir(os.path.join(dataset_dict_path,\
    \ split_name))}')\n          shutil.copy(os.path.join(dataset_dict_path, split_name,\
    \ datasets_config.DATASET_ARROW_FILENAME), dataset_path)\n          # shutil.copy(os.path.join(dataset_dict_path,\
    \ split_name, datasets_config.DATASET_INDICES_FILENAME), dataset_indices_path)\n\
    \          shutil.copy(os.path.join(dataset_dict_path, split_name, datasets_config.DATASET_INFO_FILENAME),\
    \ dataset_info_path)\n          shutil.copy(os.path.join(dataset_dict_path, split_name,\
    \ datasets_config.DATASET_STATE_JSON_FILENAME), dataset_state_path)\n\n      import\
    \ argparse\n      _parser = argparse.ArgumentParser(prog='Split dataset huggingface',\
    \ description='')\n      _parser.add_argument(\"--dataset-dict\", dest=\"dataset_dict_path\"\
    , type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"\
    --split-name\", dest=\"split_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
    \      _parser.add_argument(\"--dataset-split\", dest=\"dataset_split_path\",\
    \ type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
    \      _parser.add_argument(\"--dataset\", dest=\"dataset_path\", type=_make_parent_dirs_and_return_path,\
    \ required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--dataset-info\"\
    , dest=\"dataset_info_path\", type=_make_parent_dirs_and_return_path, required=True,\
    \ default=argparse.SUPPRESS)\n      _parser.add_argument(\"--dataset-state\",\
    \ dest=\"dataset_state_path\", type=_make_parent_dirs_and_return_path, required=True,\
    \ default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\
    \n      _outputs = split_dataset_huggingface(**_parsed_args)\n    args:\n    -\
    \ --dataset-dict\n    - {inputPath: dataset_dict}\n    - if:\n        cond: {isPresent:\
    \ split_name}\n        then:\n        - --split-name\n        - {inputValue: split_name}\n\
    \    - --dataset-split\n    - {outputPath: dataset_split}\n    - --dataset\n \
    \   - {outputPath: dataset}\n    - --dataset-info\n    - {outputPath: dataset_info}\n\
    \    - --dataset-state\n    - {outputPath: dataset_state}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Evaluator/component.yaml',
    digest: 7c046e59173ffbafcc845954c606657f8898424140012168b4d53a6394f67d10}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Evaluator
    inputs:
    - {name: examples, type: Examples}
    - {name: model, type: Model, optional: true}
    - {name: baseline_model, type: Model, optional: true}
    - {name: schema, type: Schema, optional: true}
    - name: eval_config
      type:
        JsonObject: {data_type: 'proto:tensorflow_model_analysis.EvalConfig'}
      optional: true
    - name: feature_slicing_spec
      type:
        JsonObject: {data_type: 'proto:tfx.components.evaluator.FeatureSlicingSpec'}
      optional: true
    - {name: fairness_indicator_thresholds, type: JsonArray, optional: true}
    - {name: example_splits, type: String, optional: true}
    - {name: module_file, type: String, optional: true}
    - {name: module_path, type: String, optional: true}
    outputs:
    - {name: evaluation, type: ModelEvaluation}
    - {name: blessing, type: ModelBlessing}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Evaluator/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def Evaluator(
              examples_path,
              evaluation_path,
              blessing_path,
              model_path = None,
              baseline_model_path = None,
              schema_path = None,
              eval_config = None,
              feature_slicing_spec = None,
              fairness_indicator_thresholds = None,
              example_splits = None,
              module_file = None,
              module_path = None,
          ):
              from tfx.components.evaluator.component import Evaluator as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Evaluator', description='')
          _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--baseline-model", dest="baseline_model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--schema", dest="schema_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--eval-config", dest="eval_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--feature-slicing-spec", dest="feature_slicing_spec", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--fairness-indicator-thresholds", dest="fairness_indicator_thresholds", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--example-splits", dest="example_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-path", dest="module_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--evaluation", dest="evaluation_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--blessing", dest="blessing_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = Evaluator(**_parsed_args)
        args:
        - --examples
        - {inputPath: examples}
        - if:
            cond: {isPresent: model}
            then:
            - --model
            - {inputPath: model}
        - if:
            cond: {isPresent: baseline_model}
            then:
            - --baseline-model
            - {inputPath: baseline_model}
        - if:
            cond: {isPresent: schema}
            then:
            - --schema
            - {inputPath: schema}
        - if:
            cond: {isPresent: eval_config}
            then:
            - --eval-config
            - {inputValue: eval_config}
        - if:
            cond: {isPresent: feature_slicing_spec}
            then:
            - --feature-slicing-spec
            - {inputValue: feature_slicing_spec}
        - if:
            cond: {isPresent: fairness_indicator_thresholds}
            then:
            - --fairness-indicator-thresholds
            - {inputValue: fairness_indicator_thresholds}
        - if:
            cond: {isPresent: example_splits}
            then:
            - --example-splits
            - {inputValue: example_splits}
        - if:
            cond: {isPresent: module_file}
            then:
            - --module-file
            - {inputValue: module_file}
        - if:
            cond: {isPresent: module_path}
            then:
            - --module-path
            - {inputValue: module_path}
        - --evaluation
        - {outputPath: evaluation}
        - --blessing
        - {outputPath: blessing}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Evaluator/with_URI_IO/component.yaml',
    digest: 522f305db689c9e7e0af53f3a4f4d64e56ee318d2f992e2c6e173f04253a1905}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Evaluator
    inputs:
    - {name: examples_uri, type: ExamplesUri}
    - {name: output_evaluation_uri, type: ModelEvaluationUri}
    - {name: output_blessing_uri, type: ModelBlessingUri}
    - {name: model_uri, type: ModelUri, optional: true}
    - {name: baseline_model_uri, type: ModelUri, optional: true}
    - {name: schema_uri, type: SchemaUri, optional: true}
    - name: eval_config
      type:
        JsonObject: {data_type: 'proto:tensorflow_model_analysis.EvalConfig'}
      optional: true
    - name: feature_slicing_spec
      type:
        JsonObject: {data_type: 'proto:tfx.components.evaluator.FeatureSlicingSpec'}
      optional: true
    - {name: fairness_indicator_thresholds, type: JsonArray, optional: true}
    - {name: example_splits, type: String, optional: true}
    - {name: module_file, type: String, optional: true}
    - {name: module_path, type: String, optional: true}
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: evaluation_uri, type: ModelEvaluationUri}
    - {name: blessing_uri, type: ModelBlessingUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Evaluator/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def Evaluator(
              examples_uri,
              output_evaluation_uri,
              output_blessing_uri,
              model_uri = None,
              baseline_model_uri = None,
              schema_uri = None,
              eval_config = None,
              feature_slicing_spec = None,
              fairness_indicator_thresholds = None,
              example_splits = None,
              module_file = None,
              module_path = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.evaluator.component import Evaluator as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_evaluation_uri, output_blessing_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Evaluator', description='')
          _parser.add_argument("--examples-uri", dest="examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-evaluation-uri", dest="output_evaluation_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-blessing-uri", dest="output_blessing_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model-uri", dest="model_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--baseline-model-uri", dest="baseline_model_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--schema-uri", dest="schema_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--eval-config", dest="eval_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--feature-slicing-spec", dest="feature_slicing_spec", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--fairness-indicator-thresholds", dest="fairness_indicator_thresholds", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--example-splits", dest="example_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-path", dest="module_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = Evaluator(**_parsed_args)

          _output_serializers = [
              str,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --examples-uri
        - {inputValue: examples_uri}
        - --output-evaluation-uri
        - {inputValue: output_evaluation_uri}
        - --output-blessing-uri
        - {inputValue: output_blessing_uri}
        - if:
            cond: {isPresent: model_uri}
            then:
            - --model-uri
            - {inputValue: model_uri}
        - if:
            cond: {isPresent: baseline_model_uri}
            then:
            - --baseline-model-uri
            - {inputValue: baseline_model_uri}
        - if:
            cond: {isPresent: schema_uri}
            then:
            - --schema-uri
            - {inputValue: schema_uri}
        - if:
            cond: {isPresent: eval_config}
            then:
            - --eval-config
            - {inputValue: eval_config}
        - if:
            cond: {isPresent: feature_slicing_spec}
            then:
            - --feature-slicing-spec
            - {inputValue: feature_slicing_spec}
        - if:
            cond: {isPresent: fairness_indicator_thresholds}
            then:
            - --fairness-indicator-thresholds
            - {inputValue: fairness_indicator_thresholds}
        - if:
            cond: {isPresent: example_splits}
            then:
            - --example-splits
            - {inputValue: example_splits}
        - if:
            cond: {isPresent: module_file}
            then:
            - --module-file
            - {inputValue: module_file}
        - if:
            cond: {isPresent: module_path}
            then:
            - --module-path
            - {inputValue: module_path}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: evaluation_uri}
        - {outputPath: blessing_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/BigQueryExampleGen/component.yaml',
    digest: 9bc427d0a5195fc57fa1bd1524374813cff8e86b207f27c6685a93d0c592721d}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: BigQueryExampleGen
    inputs:
    - name: input_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
    - name: output_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
    outputs:
    - {name: examples, type: Examples}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/BigQueryExampleGen/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def BigQueryExampleGen(
              examples_path,
              input_config,
              output_config,
          ):
              from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='BigQueryExampleGen', description='')
          _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--examples", dest="examples_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = BigQueryExampleGen(**_parsed_args)
        args:
        - --input-config
        - {inputValue: input_config}
        - --output-config
        - {inputValue: output_config}
        - --examples
        - {outputPath: examples}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/BigQueryExampleGen/with_URI_IO/component.yaml',
    digest: fc260822fb92bc9401fdee34b17d21bde244c58e473d4979d8b4daa3113ce521}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: BigQueryExampleGen
    inputs:
    - {name: output_examples_uri, type: ExamplesUri}
    - name: input_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
    - name: output_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: examples_uri, type: ExamplesUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/BigQueryExampleGen/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def BigQueryExampleGen(
              output_examples_uri,
              input_config,
              output_config,
              beam_pipeline_args = None,
          ):
              from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_examples_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='BigQueryExampleGen', description='')
          _parser.add_argument("--output-examples-uri", dest="output_examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = BigQueryExampleGen(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --output-examples-uri
        - {inputValue: output_examples_uri}
        - --input-config
        - {inputValue: input_config}
        - --output-config
        - {inputValue: output_config}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: examples_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/CsvExampleGen/component.yaml',
    digest: 7f2e484b67b89d822941810268a3b53f82c4651f744065a2086d5ffb8a23f56c}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: CsvExampleGen
    inputs:
    - {name: input_base, type: String}
    - name: input_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
    - name: output_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
    - name: range_config
      type:
        JsonObject: {data_type: 'proto:tfx.configs.RangeConfig'}
      optional: true
    outputs:
    - {name: examples, type: Examples}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/CsvExampleGen/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def CsvExampleGen(
              examples_path,
              input_base,
              input_config,
              output_config,
              range_config = None,
          ):
              from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='CsvExampleGen', description='')
          _parser.add_argument("--input-base", dest="input_base", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--range-config", dest="range_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--examples", dest="examples_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = CsvExampleGen(**_parsed_args)
        args:
        - --input-base
        - {inputValue: input_base}
        - --input-config
        - {inputValue: input_config}
        - --output-config
        - {inputValue: output_config}
        - if:
            cond: {isPresent: range_config}
            then:
            - --range-config
            - {inputValue: range_config}
        - --examples
        - {outputPath: examples}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/CsvExampleGen/with_URI_IO/component.yaml',
    digest: e81339eb60d77a505f7f733971a0c26eea3a873edaed86c48b359793a74500b1}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: CsvExampleGen
    inputs:
    - {name: output_examples_uri, type: ExamplesUri}
    - {name: input_base, type: String}
    - name: input_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
    - name: output_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
    - name: range_config
      type:
        JsonObject: {data_type: 'proto:tfx.configs.RangeConfig'}
      optional: true
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: examples_uri, type: ExamplesUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/CsvExampleGen/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def CsvExampleGen(
              output_examples_uri,
              input_base,
              input_config,
              output_config,
              range_config = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_examples_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='CsvExampleGen', description='')
          _parser.add_argument("--output-examples-uri", dest="output_examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-base", dest="input_base", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--range-config", dest="range_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = CsvExampleGen(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --output-examples-uri
        - {inputValue: output_examples_uri}
        - --input-base
        - {inputValue: input_base}
        - --input-config
        - {inputValue: input_config}
        - --output-config
        - {inputValue: output_config}
        - if:
            cond: {isPresent: range_config}
            then:
            - --range-config
            - {inputValue: range_config}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: examples_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/ImportExampleGen/component.yaml',
    digest: 09376d09a0f79468d37b953e7968f7e1b636359148354f107a02569bea62187f}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: ImportExampleGen
    inputs:
    - {name: input_base, type: String}
    - name: input_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
    - name: output_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
    - name: range_config
      type:
        JsonObject: {data_type: 'proto:tfx.configs.RangeConfig'}
      optional: true
    outputs:
    - {name: examples, type: Examples}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/ImportExampleGen/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def ImportExampleGen(
              examples_path,
              input_base,
              input_config,
              output_config,
              range_config = None,
          ):
              from tfx.components.example_gen.import_example_gen.component import ImportExampleGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='ImportExampleGen', description='')
          _parser.add_argument("--input-base", dest="input_base", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--range-config", dest="range_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--examples", dest="examples_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = ImportExampleGen(**_parsed_args)
        args:
        - --input-base
        - {inputValue: input_base}
        - --input-config
        - {inputValue: input_config}
        - --output-config
        - {inputValue: output_config}
        - if:
            cond: {isPresent: range_config}
            then:
            - --range-config
            - {inputValue: range_config}
        - --examples
        - {outputPath: examples}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/ImportExampleGen/with_URI_IO/component.yaml',
    digest: d09ef3d176b1da9b9bd20249f2f0d7f3d290dcb5012aa999a01247a0fc9d9b81}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: ImportExampleGen
    inputs:
    - {name: output_examples_uri, type: ExamplesUri}
    - {name: input_base, type: String}
    - name: input_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}
    - name: output_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}
    - name: range_config
      type:
        JsonObject: {data_type: 'proto:tfx.configs.RangeConfig'}
      optional: true
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: examples_uri, type: ExamplesUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/ImportExampleGen/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def ImportExampleGen(
              output_examples_uri,
              input_base,
              input_config,
              output_config,
              range_config = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.example_gen.import_example_gen.component import ImportExampleGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_examples_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='ImportExampleGen', description='')
          _parser.add_argument("--output-examples-uri", dest="output_examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-base", dest="input_base", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--input-config", dest="input_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-config", dest="output_config", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--range-config", dest="range_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = ImportExampleGen(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --output-examples-uri
        - {inputValue: output_examples_uri}
        - --input-base
        - {inputValue: input_base}
        - --input-config
        - {inputValue: input_config}
        - --output-config
        - {inputValue: output_config}
        - if:
            cond: {isPresent: range_config}
            then:
            - --range-config
            - {inputValue: range_config}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: examples_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleValidator/component.yaml',
    digest: 1485b7118f740fbf00f64366dbd54cc5776842816722afc016ca2dc648611c60}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: ExampleValidator
    inputs:
    - {name: statistics, type: ExampleStatistics}
    - {name: schema, type: Schema}
    - {name: exclude_splits, type: String, optional: true}
    outputs:
    - {name: anomalies, type: ExampleAnomalies}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleValidator/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def ExampleValidator(
              statistics_path,
              schema_path,
              anomalies_path,
              exclude_splits = None,
          ):
              from tfx.components.example_validator.component import ExampleValidator as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='ExampleValidator', description='')
          _parser.add_argument("--statistics", dest="statistics_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--schema", dest="schema_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--anomalies", dest="anomalies_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = ExampleValidator(**_parsed_args)
        args:
        - --statistics
        - {inputPath: statistics}
        - --schema
        - {inputPath: schema}
        - if:
            cond: {isPresent: exclude_splits}
            then:
            - --exclude-splits
            - {inputValue: exclude_splits}
        - --anomalies
        - {outputPath: anomalies}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleValidator/with_URI_IO/component.yaml',
    digest: d7fca2807dc64189bcc9ba53d55528cf3f46e8dc70e5914f297b1595769ce191}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: ExampleValidator
    inputs:
    - {name: statistics_uri, type: ExampleStatisticsUri}
    - {name: schema_uri, type: SchemaUri}
    - {name: output_anomalies_uri, type: ExampleAnomaliesUri}
    - {name: exclude_splits, type: String, optional: true}
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: anomalies_uri, type: ExampleAnomaliesUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleValidator/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def ExampleValidator(
              statistics_uri,
              schema_uri,
              output_anomalies_uri,
              exclude_splits = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.example_validator.component import ExampleValidator as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_anomalies_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='ExampleValidator', description='')
          _parser.add_argument("--statistics-uri", dest="statistics_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--schema-uri", dest="schema_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-anomalies-uri", dest="output_anomalies_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = ExampleValidator(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --statistics-uri
        - {inputValue: statistics_uri}
        - --schema-uri
        - {inputValue: schema_uri}
        - --output-anomalies-uri
        - {inputValue: output_anomalies_uri}
        - if:
            cond: {isPresent: exclude_splits}
            then:
            - --exclude-splits
            - {inputValue: exclude_splits}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: anomalies_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/SchemaGen/component.yaml',
    digest: 83ab7b74d3252b6a82159f898d748857af7a5e6460f9b16109a8132646519eac}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: SchemaGen
    inputs:
    - {name: statistics, type: ExampleStatistics}
    - {name: infer_feature_shape, type: Integer, optional: true}
    - {name: exclude_splits, type: String, optional: true}
    outputs:
    - {name: schema, type: Schema}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/SchemaGen/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def SchemaGen(
              statistics_path,
              schema_path,
              infer_feature_shape = None,
              exclude_splits = None,
          ):
              from tfx.components.schema_gen.component import SchemaGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='SchemaGen', description='')
          _parser.add_argument("--statistics", dest="statistics_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--infer-feature-shape", dest="infer_feature_shape", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--schema", dest="schema_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = SchemaGen(**_parsed_args)
        args:
        - --statistics
        - {inputPath: statistics}
        - if:
            cond: {isPresent: infer_feature_shape}
            then:
            - --infer-feature-shape
            - {inputValue: infer_feature_shape}
        - if:
            cond: {isPresent: exclude_splits}
            then:
            - --exclude-splits
            - {inputValue: exclude_splits}
        - --schema
        - {outputPath: schema}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/SchemaGen/with_URI_IO/component.yaml',
    digest: 2d266af7fe473479a8d601ecb0155e04481ecdaee48048fa72d1b998fece2aaf}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: SchemaGen
    inputs:
    - {name: statistics_uri, type: ExampleStatisticsUri}
    - {name: output_schema_uri, type: SchemaUri}
    - {name: infer_feature_shape, type: Integer, optional: true}
    - {name: exclude_splits, type: String, optional: true}
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: schema_uri, type: SchemaUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/SchemaGen/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def SchemaGen(
              statistics_uri,
              output_schema_uri,
              infer_feature_shape = None,
              exclude_splits = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.schema_gen.component import SchemaGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_schema_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='SchemaGen', description='')
          _parser.add_argument("--statistics-uri", dest="statistics_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-schema-uri", dest="output_schema_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--infer-feature-shape", dest="infer_feature_shape", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = SchemaGen(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --statistics-uri
        - {inputValue: statistics_uri}
        - --output-schema-uri
        - {inputValue: output_schema_uri}
        - if:
            cond: {isPresent: infer_feature_shape}
            then:
            - --infer-feature-shape
            - {inputValue: infer_feature_shape}
        - if:
            cond: {isPresent: exclude_splits}
            then:
            - --exclude-splits
            - {inputValue: exclude_splits}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: schema_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/StatisticsGen/component.yaml',
    digest: 32816822efeb651f196ad25b3d88c4dc2e55613e570537a0a710b4b8ee83562f}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: StatisticsGen
    inputs:
    - {name: examples, type: Examples}
    - {name: schema, type: Schema, optional: true}
    - {name: exclude_splits, type: String, optional: true}
    outputs:
    - {name: statistics, type: ExampleStatistics}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/StatisticsGen/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def StatisticsGen(
              examples_path,
              statistics_path,
              schema_path = None,
              exclude_splits = None,
          ):
              from tfx.components.statistics_gen.component import StatisticsGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='StatisticsGen', description='')
          _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--schema", dest="schema_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--statistics", dest="statistics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = StatisticsGen(**_parsed_args)
        args:
        - --examples
        - {inputPath: examples}
        - if:
            cond: {isPresent: schema}
            then:
            - --schema
            - {inputPath: schema}
        - if:
            cond: {isPresent: exclude_splits}
            then:
            - --exclude-splits
            - {inputValue: exclude_splits}
        - --statistics
        - {outputPath: statistics}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/StatisticsGen/with_URI_IO/component.yaml',
    digest: 21d8745dc1a540997d46dabbbb6be3551a4a213733003f56e1453c11b7a68b32}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: StatisticsGen
    inputs:
    - {name: examples_uri, type: ExamplesUri}
    - {name: output_statistics_uri, type: ExampleStatisticsUri}
    - {name: schema_uri, type: SchemaUri, optional: true}
    - {name: exclude_splits, type: String, optional: true}
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: statistics_uri, type: ExampleStatisticsUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/StatisticsGen/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def StatisticsGen(
              examples_uri,
              output_statistics_uri,
              schema_uri = None,
              exclude_splits = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.statistics_gen.component import StatisticsGen as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_statistics_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='StatisticsGen', description='')
          _parser.add_argument("--examples-uri", dest="examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-statistics-uri", dest="output_statistics_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--schema-uri", dest="schema_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--exclude-splits", dest="exclude_splits", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = StatisticsGen(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --examples-uri
        - {inputValue: examples_uri}
        - --output-statistics-uri
        - {inputValue: output_statistics_uri}
        - if:
            cond: {isPresent: schema_uri}
            then:
            - --schema-uri
            - {inputValue: schema_uri}
        - if:
            cond: {isPresent: exclude_splits}
            then:
            - --exclude-splits
            - {inputValue: exclude_splits}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: statistics_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Trainer/component.yaml',
    digest: 343627821fb4979e40edadf8cd4a40624dd6eeeaac3156aec790e85ecc546303}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Trainer
    inputs:
    - {name: examples, type: Examples}
    - name: train_args
      type:
        JsonObject: {data_type: 'proto:tfx.components.trainer.TrainArgs'}
    - name: eval_args
      type:
        JsonObject: {data_type: 'proto:tfx.components.trainer.EvalArgs'}
    - {name: transform_graph, type: TransformGraph, optional: true}
    - {name: schema, type: Schema, optional: true}
    - {name: base_model, type: Model, optional: true}
    - {name: hyperparameters, type: HyperParameters, optional: true}
    - {name: module_file, type: String, optional: true}
    - {name: run_fn, type: String, optional: true}
    - {name: trainer_fn, type: String, optional: true}
    - {name: custom_config, type: String, optional: true}
    outputs:
    - {name: model, type: Model}
    - {name: model_run, type: ModelRun}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Trainer/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def Trainer(
              examples_path,
              model_path,
              model_run_path,
              train_args,
              eval_args,
              transform_graph_path = None,
              schema_path = None,
              base_model_path = None,
              hyperparameters_path = None,
              module_file = None,
              run_fn = None,
              trainer_fn = None,
              custom_config = None,
          ):
              from tfx.components.trainer.component import Trainer as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='Trainer', description='')
          _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--train-args", dest="train_args", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--eval-args", dest="eval_args", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transform-graph", dest="transform_graph_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--schema", dest="schema_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--base-model", dest="base_model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--hyperparameters", dest="hyperparameters_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--run-fn", dest="run_fn", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--trainer-fn", dest="trainer_fn", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--custom-config", dest="custom_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model-run", dest="model_run_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = Trainer(**_parsed_args)
        args:
        - --examples
        - {inputPath: examples}
        - --train-args
        - {inputValue: train_args}
        - --eval-args
        - {inputValue: eval_args}
        - if:
            cond: {isPresent: transform_graph}
            then:
            - --transform-graph
            - {inputPath: transform_graph}
        - if:
            cond: {isPresent: schema}
            then:
            - --schema
            - {inputPath: schema}
        - if:
            cond: {isPresent: base_model}
            then:
            - --base-model
            - {inputPath: base_model}
        - if:
            cond: {isPresent: hyperparameters}
            then:
            - --hyperparameters
            - {inputPath: hyperparameters}
        - if:
            cond: {isPresent: module_file}
            then:
            - --module-file
            - {inputValue: module_file}
        - if:
            cond: {isPresent: run_fn}
            then:
            - --run-fn
            - {inputValue: run_fn}
        - if:
            cond: {isPresent: trainer_fn}
            then:
            - --trainer-fn
            - {inputValue: trainer_fn}
        - if:
            cond: {isPresent: custom_config}
            then:
            - --custom-config
            - {inputValue: custom_config}
        - --model
        - {outputPath: model}
        - --model-run
        - {outputPath: model_run}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Trainer/with_URI_IO/component.yaml',
    digest: e2941d03178fb50f1344cfb5c9d6cc6b4befafd57a8b9c99dded272c07f1ff67}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Trainer
    inputs:
    - {name: examples_uri, type: ExamplesUri}
    - {name: output_model_uri, type: ModelUri}
    - {name: output_model_run_uri, type: ModelRunUri}
    - name: train_args
      type:
        JsonObject: {data_type: 'proto:tfx.components.trainer.TrainArgs'}
    - name: eval_args
      type:
        JsonObject: {data_type: 'proto:tfx.components.trainer.EvalArgs'}
    - {name: transform_graph_uri, type: TransformGraphUri, optional: true}
    - {name: schema_uri, type: SchemaUri, optional: true}
    - {name: base_model_uri, type: ModelUri, optional: true}
    - {name: hyperparameters_uri, type: HyperParametersUri, optional: true}
    - {name: module_file, type: String, optional: true}
    - {name: run_fn, type: String, optional: true}
    - {name: trainer_fn, type: String, optional: true}
    - {name: custom_config, type: String, optional: true}
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: model_uri, type: ModelUri}
    - {name: model_run_uri, type: ModelRunUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Trainer/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def Trainer(
              examples_uri,
              output_model_uri,
              output_model_run_uri,
              train_args,
              eval_args,
              transform_graph_uri = None,
              schema_uri = None,
              base_model_uri = None,
              hyperparameters_uri = None,
              module_file = None,
              run_fn = None,
              trainer_fn = None,
              custom_config = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.trainer.component import Trainer as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_model_uri, output_model_run_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Trainer', description='')
          _parser.add_argument("--examples-uri", dest="examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-model-uri", dest="output_model_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-model-run-uri", dest="output_model_run_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--train-args", dest="train_args", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--eval-args", dest="eval_args", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transform-graph-uri", dest="transform_graph_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--schema-uri", dest="schema_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--base-model-uri", dest="base_model_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--hyperparameters-uri", dest="hyperparameters_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--run-fn", dest="run_fn", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--trainer-fn", dest="trainer_fn", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--custom-config", dest="custom_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = Trainer(**_parsed_args)

          _output_serializers = [
              str,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --examples-uri
        - {inputValue: examples_uri}
        - --output-model-uri
        - {inputValue: output_model_uri}
        - --output-model-run-uri
        - {inputValue: output_model_run_uri}
        - --train-args
        - {inputValue: train_args}
        - --eval-args
        - {inputValue: eval_args}
        - if:
            cond: {isPresent: transform_graph_uri}
            then:
            - --transform-graph-uri
            - {inputValue: transform_graph_uri}
        - if:
            cond: {isPresent: schema_uri}
            then:
            - --schema-uri
            - {inputValue: schema_uri}
        - if:
            cond: {isPresent: base_model_uri}
            then:
            - --base-model-uri
            - {inputValue: base_model_uri}
        - if:
            cond: {isPresent: hyperparameters_uri}
            then:
            - --hyperparameters-uri
            - {inputValue: hyperparameters_uri}
        - if:
            cond: {isPresent: module_file}
            then:
            - --module-file
            - {inputValue: module_file}
        - if:
            cond: {isPresent: run_fn}
            then:
            - --run-fn
            - {inputValue: run_fn}
        - if:
            cond: {isPresent: trainer_fn}
            then:
            - --trainer-fn
            - {inputValue: trainer_fn}
        - if:
            cond: {isPresent: custom_config}
            then:
            - --custom-config
            - {inputValue: custom_config}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: model_uri}
        - {outputPath: model_run_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Transform/component.yaml',
    digest: 1a3ac504652bbdd592ba640fbc6c6da7c3d877022b17c2b3d759ef8075d16daf}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Transform
    inputs:
    - {name: examples, type: Examples}
    - {name: schema, type: Schema}
    - {name: analyzer_cache, type: TransformCache, optional: true}
    - {name: module_file, type: String, optional: true}
    - {name: preprocessing_fn, type: String, optional: true}
    - {name: force_tf_compat_v1, type: Integer, optional: true}
    - {name: custom_config, type: String, optional: true}
    - name: splits_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.transform.SplitsConfig'}
      optional: true
    outputs:
    - {name: transform_graph, type: TransformGraph}
    - {name: transformed_examples, type: Examples}
    - {name: updated_analyzer_cache, type: TransformCache}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Transform/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def Transform(
              examples_path,
              schema_path,
              transform_graph_path,
              transformed_examples_path,
              updated_analyzer_cache_path,
              analyzer_cache_path = None,
              module_file = None,
              preprocessing_fn = None,
              force_tf_compat_v1 = None,
              custom_config = None,
              splits_config = None,
          ):
              from tfx.components.transform.component import Transform as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='Transform', description='')
          _parser.add_argument("--examples", dest="examples_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--schema", dest="schema_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--analyzer-cache", dest="analyzer_cache_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--preprocessing-fn", dest="preprocessing_fn", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--force-tf-compat-v1", dest="force_tf_compat_v1", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--custom-config", dest="custom_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--splits-config", dest="splits_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--transform-graph", dest="transform_graph_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transformed-examples", dest="transformed_examples_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--updated-analyzer-cache", dest="updated_analyzer_cache_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = Transform(**_parsed_args)
        args:
        - --examples
        - {inputPath: examples}
        - --schema
        - {inputPath: schema}
        - if:
            cond: {isPresent: analyzer_cache}
            then:
            - --analyzer-cache
            - {inputPath: analyzer_cache}
        - if:
            cond: {isPresent: module_file}
            then:
            - --module-file
            - {inputValue: module_file}
        - if:
            cond: {isPresent: preprocessing_fn}
            then:
            - --preprocessing-fn
            - {inputValue: preprocessing_fn}
        - if:
            cond: {isPresent: force_tf_compat_v1}
            then:
            - --force-tf-compat-v1
            - {inputValue: force_tf_compat_v1}
        - if:
            cond: {isPresent: custom_config}
            then:
            - --custom-config
            - {inputValue: custom_config}
        - if:
            cond: {isPresent: splits_config}
            then:
            - --splits-config
            - {inputValue: splits_config}
        - --transform-graph
        - {outputPath: transform_graph}
        - --transformed-examples
        - {outputPath: transformed_examples}
        - --updated-analyzer-cache
        - {outputPath: updated_analyzer_cache}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Transform/with_URI_IO/component.yaml',
    digest: ddde1173d37f2070576d53fee3f796038b978497792ed494c47ffa96a3a74805}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Transform
    inputs:
    - {name: examples_uri, type: ExamplesUri}
    - {name: schema_uri, type: SchemaUri}
    - {name: output_transform_graph_uri, type: TransformGraphUri}
    - {name: output_transformed_examples_uri, type: ExamplesUri}
    - {name: output_updated_analyzer_cache_uri, type: TransformCacheUri}
    - {name: analyzer_cache_uri, type: TransformCacheUri, optional: true}
    - {name: module_file, type: String, optional: true}
    - {name: preprocessing_fn, type: String, optional: true}
    - {name: force_tf_compat_v1, type: Integer, optional: true}
    - {name: custom_config, type: String, optional: true}
    - name: splits_config
      type:
        JsonObject: {data_type: 'proto:tfx.components.transform.SplitsConfig'}
      optional: true
    - {name: beam_pipeline_args, type: JsonArray, optional: true}
    outputs:
    - {name: transform_graph_uri, type: TransformGraphUri}
    - {name: transformed_examples_uri, type: ExamplesUri}
    - {name: updated_analyzer_cache_uri, type: TransformCacheUri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Transform/with_URI_IO/component.yaml'
    implementation:
      container:
        image: tensorflow/tfx:0.29.0
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def Transform(
              examples_uri,
              schema_uri,
              output_transform_graph_uri,
              output_transformed_examples_uri,
              output_updated_analyzer_cache_uri,
              analyzer_cache_uri = None,
              module_file = None,
              preprocessing_fn = None,
              force_tf_compat_v1 = None,
              custom_config = None,
              splits_config = None,
              beam_pipeline_args = None,
          ):
              from tfx.components.transform.component import Transform as component_class

              #Generated code
              import os
              import tempfile
              from tensorflow.io import gfile
              from google.protobuf import json_format, message
              from tfx.types import channel_utils, artifact_utils
              from tfx.components.base import base_executor

              arguments = locals().copy()

              component_class_args = {}

              for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():
                  argument_value = arguments.get(name, None)
                  if argument_value is None:
                      continue
                  parameter_type = execution_parameter.type
                  if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):
                      argument_value_obj = parameter_type()
                      json_format.Parse(argument_value, argument_value_obj)
                  else:
                      argument_value_obj = argument_value
                  component_class_args[name] = argument_value_obj

              for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():
                  artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel_parameter.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:
                          # Recovering splits
                          subdirs = gfile.listdir(artifact_path)
                          # Workaround for https://github.com/tensorflow/tensorflow/issues/39167
                          subdirs = [subdir.rstrip('/') for subdir in subdirs]
                          split_names = [subdir.replace('Split-', '') for subdir in subdirs]
                          artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))
                      component_class_args[name] = channel_utils.as_channel([artifact])

              component_class_instance = component_class(**component_class_args)

              input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())
              output_dict = {}
              exec_properties = component_class_instance.exec_properties

              # Generating paths for output artifacts
              for name, channel in component_class_instance.outputs.items():
                  artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')
                  if artifact_path:
                      artifact = channel.type()
                      artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash
                      artifact_list = [artifact]
                      channel._artifacts = artifact_list
                      output_dict[name] = artifact_list

              print('component instance: ' + str(component_class_instance))

              executor_context = base_executor.BaseExecutor.Context(
                  beam_pipeline_args=arguments.get('beam_pipeline_args'),
                  tmp_dir=tempfile.gettempdir(),
                  unique_id='tfx_component',
              )
              executor = component_class_instance.executor_spec.executor_class(executor_context)
              executor.Do(
                  input_dict=input_dict,
                  output_dict=output_dict,
                  exec_properties=exec_properties,
              )

              return (output_transform_graph_uri, output_transformed_examples_uri, output_updated_analyzer_cache_uri, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Transform', description='')
          _parser.add_argument("--examples-uri", dest="examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--schema-uri", dest="schema_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-transform-graph-uri", dest="output_transform_graph_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-transformed-examples-uri", dest="output_transformed_examples_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-updated-analyzer-cache-uri", dest="output_updated_analyzer_cache_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--analyzer-cache-uri", dest="analyzer_cache_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--module-file", dest="module_file", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--preprocessing-fn", dest="preprocessing_fn", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--force-tf-compat-v1", dest="force_tf_compat_v1", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--custom-config", dest="custom_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--splits-config", dest="splits_config", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--beam-pipeline-args", dest="beam_pipeline_args", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = Transform(**_parsed_args)

          _output_serializers = [
              str,
              str,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --examples-uri
        - {inputValue: examples_uri}
        - --schema-uri
        - {inputValue: schema_uri}
        - --output-transform-graph-uri
        - {inputValue: output_transform_graph_uri}
        - --output-transformed-examples-uri
        - {inputValue: output_transformed_examples_uri}
        - --output-updated-analyzer-cache-uri
        - {inputValue: output_updated_analyzer_cache_uri}
        - if:
            cond: {isPresent: analyzer_cache_uri}
            then:
            - --analyzer-cache-uri
            - {inputValue: analyzer_cache_uri}
        - if:
            cond: {isPresent: module_file}
            then:
            - --module-file
            - {inputValue: module_file}
        - if:
            cond: {isPresent: preprocessing_fn}
            then:
            - --preprocessing-fn
            - {inputValue: preprocessing_fn}
        - if:
            cond: {isPresent: force_tf_compat_v1}
            then:
            - --force-tf-compat-v1
            - {inputValue: force_tf_compat_v1}
        - if:
            cond: {isPresent: custom_config}
            then:
            - --custom-config
            - {inputValue: custom_config}
        - if:
            cond: {isPresent: splits_config}
            then:
            - --splits-config
            - {inputValue: splits_config}
        - if:
            cond: {isPresent: beam_pipeline_args}
            then:
            - --beam-pipeline-args
            - {inputValue: beam_pipeline_args}
        - '----output-paths'
        - {outputPath: transform_graph_uri}
        - {outputPath: transformed_examples_uri}
        - {outputPath: updated_analyzer_cache_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/filesystem/get_file/component.yaml',
    digest: 4415166532fa69baa33378631802fab630f51ea1677ce6408cef946672f4a9d2}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Get file
    description: Get file from directory.
    inputs:
    - {name: Directory, type: Directory}
    - {name: Subpath, type: String}
    outputs:
    - {name: File}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/get_file/component.yaml'
    implementation:
      container:
        image: alpine
        command:
        - sh
        - -ex
        - -c
        - |
          mkdir -p "$(dirname "$2")"
          cp -r "$0/$1" "$2"
        - inputPath: Directory
        - inputValue: Subpath
        - outputPath: File
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/filesystem/get_subdirectory/component.yaml',
    digest: f7fa5446b5e4c978fecb1b0e7c996b4e17c167ad0b7ec9168a59356a4d69719b}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Get subdirectory
    description: Get subdirectory from directory.
    inputs:
    - {name: Directory, type: Directory}
    - {name: Subpath, type: String}
    outputs:
    - {name: Subdir, type: Directory}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/get_subdirectory/component.yaml'
    implementation:
      container:
        image: alpine
        command:
        - sh
        - -ex
        - -c
        - |
          mkdir -p "$(dirname "$2")"
          cp -r "$0/$1" "$2"
        - inputPath: Directory
        - inputValue: Subpath
        - outputPath: Subdir
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/filesystem/list_items/component.yaml',
    digest: 6dd9eeeb094712b72a94ac26d73cc6c3de9e095c042b65dc1f47b373a5337064}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: List items
    description: Recursively list directory contents.
    inputs:
    - {name: Directory, type: Directory}
    outputs:
    - {name: Items}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/filesystem/list_items/component.yaml'
    implementation:
      container:
        image: alpine
        command:
        - sh
        - -ex
        - -c
        - |
          mkdir -p "$(dirname "$1")"
          #ls --almost-all --recursive "$0" > "$1"
          ls -A -R "$0" > "$1"
        - inputPath: Directory
        - outputPath: Items
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/1197613c37312be56b1343fb09d56c1bc702e363/components/gcp/automl/create_dataset_for_tables/component.yaml',
    digest: 87e4ca7fa53ed12ed68ab77f4639af51deca741971214613e6c69534f3d872d6}
  annotations:
    GitHub commit:
      sha: 1197613c37312be56b1343fb09d56c1bc702e363
      html_url: https://github.com/Ark-kun/pipeline_components/commit/1197613c37312be56b1343fb09d56c1bc702e363
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/1197613c37312be56b1343fb09d56c1bc702e363
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T04:35:20Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T04:35:20Z'}
        message: Coogle Cloud - AutoML - Upgraded the Create dataset for tables component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create dataset for google cloud automl tables
    description: Creates an empty Dataset for AutoML tables
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/create_dataset_for_tables/component.yaml'
    inputs:
    - {name: display_name, type: String}
    - {name: description, type: String, optional: true}
    - {name: tables_dataset_metadata, type: JsonObject, default: '{}', optional: true}
    - {name: gcp_project_id, type: String, optional: true}
    - {name: gcp_region, type: String, optional: true}
    - {name: retry_config, type: JsonObject, optional: true}
    - {name: timeout, type: Float, optional: true}
    outputs:
    - {name: dataset_name, type: String}
    - {name: dataset, type: JsonObject}
    - {name: dataset_path, type: String}
    - {name: create_time, type: String}
    - {name: dataset_id, type: String}
    - {name: dataset_url, type: URI}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==2.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location 'google-cloud-automl==2.4.2' --user)
          && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_dataset_for_google_cloud_automl_tables(
              display_name,
              description = None,
              tables_dataset_metadata = {},
              gcp_project_id = None,
              gcp_region = None,
              retry_config = None, # : google.api_core.retry.Retry = google.api_core.gapic_v1.method.DEFAULT,
              timeout = None, #=google.api_core.gapic_v1.method.DEFAULT,
          ):
              '''Creates an empty Dataset for AutoML tables'''
              import logging
              import google
              from google.protobuf import json_format
              from google.cloud import automl_v1beta1 as automl
              client = automl.AutoMlClient()

              if not gcp_project_id:
                  _, gcp_project_id = google.auth.default()

              if not gcp_region:
                  gcp_region = 'us-central1'
              if gcp_region != 'us-central1':
                  logging.warn('AutoML only supports the us-central1 region')

              dataset = client.create_dataset(
                  parent=f"projects/{gcp_project_id}/locations/{gcp_region}",
                  dataset=automl.Dataset(
                      display_name=display_name,
                      description=description,
                      tables_dataset_metadata=tables_dataset_metadata,
                  ),
                  retry=google.api_core.retry.Retry(**retry_config) if retry_config else google.api_core.gapic_v1.method.DEFAULT,
                  timeout=timeout or google.api_core.gapic_v1.method.DEFAULT,
                  # ! metadata was dict before, but now it's a sequence of tuples.
                  #metadata=(metadata or {}).items(),
              )
              dataset_json = json_format.MessageToJson(dataset._pb)
              print(dataset_json)
              dataset_id = dataset.name.rsplit('/', 1)[-1]
              dataset_url = f'https://console.cloud.google.com/automl-tables/locations/{gcp_region}/datasets/{dataset_id}/schemav2?project={gcp_project_id}'
              print(dataset_url)
              return (dataset.name, dataset_json, dataset.name, str(dataset.create_time), dataset_id, dataset_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Create dataset for google cloud automl tables', description='Creates an empty Dataset for AutoML tables')
          _parser.add_argument("--display-name", dest="display_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--description", dest="description", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--tables-dataset-metadata", dest="tables_dataset_metadata", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--retry-config", dest="retry_config", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--timeout", dest="timeout", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_dataset_for_google_cloud_automl_tables(**_parsed_args)

          _output_serializers = [
              _serialize_str,
              _serialize_json,
              _serialize_str,
              _serialize_str,
              _serialize_str,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --display-name
        - {inputValue: display_name}
        - if:
            cond: {isPresent: description}
            then:
            - --description
            - {inputValue: description}
        - if:
            cond: {isPresent: tables_dataset_metadata}
            then:
            - --tables-dataset-metadata
            - {inputValue: tables_dataset_metadata}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - if:
            cond: {isPresent: retry_config}
            then:
            - --retry-config
            - {inputValue: retry_config}
        - if:
            cond: {isPresent: timeout}
            then:
            - --timeout
            - {inputValue: timeout}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset}
        - {outputPath: dataset_path}
        - {outputPath: create_time}
        - {outputPath: dataset_id}
        - {outputPath: dataset_url}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/create_model_for_tables/component.yaml',
    digest: b7334bf39afb2171a1d653dff9f9bc61dd5ab6fa162053a70539ef89b0addbca}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl create model for tables
    inputs:
    - {name: gcp_project_id, type: String}
    - {name: gcp_region, type: String}
    - {name: display_name, type: String}
    - {name: dataset_id, type: String}
    - {name: target_column_path, type: String, optional: true}
    - {name: input_feature_column_paths, type: JsonArray, optional: true}
    - {name: optimization_objective, type: String, default: MAXIMIZE_AU_PRC, optional: true}
    - {name: train_budget_milli_node_hours, type: Integer, default: '1000', optional: true}
    outputs:
    - {name: model_path, type: String}
    - {name: model_id, type: String}
    - {name: model_page_url, type: URI}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/create_model_for_tables/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==0.4.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location 'google-cloud-automl==0.4.0' --user)
          && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def automl_create_model_for_tables(
              gcp_project_id ,
              gcp_region ,
              display_name ,
              dataset_id ,
              target_column_path  = None,
              input_feature_column_paths  = None,
              optimization_objective  = 'MAXIMIZE_AU_PRC',
              train_budget_milli_node_hours  = 1000,
          )        :
              from google.cloud import automl
              client = automl.AutoMlClient()

              location_path = client.location_path(gcp_project_id, gcp_region)
              model_dict = {
                  'display_name': display_name,
                  'dataset_id': dataset_id,
                  'tables_model_metadata': {
                      'target_column_spec': automl.types.ColumnSpec(name=target_column_path),
                      'input_feature_column_specs': [automl.types.ColumnSpec(name=path) for path in input_feature_column_paths] if input_feature_column_paths else None,
                      'optimization_objective': optimization_objective,
                      'train_budget_milli_node_hours': train_budget_milli_node_hours,
                  },
              }

              create_model_response = client.create_model(location_path, model_dict)
              print('Create model operation: {}'.format(create_model_response.operation))
              result = create_model_response.result()
              print(result)
              model_name = result.name
              model_id = model_name.rsplit('/', 1)[-1]
              model_url = 'https://console.cloud.google.com/automl-tables/locations/{region}/datasets/{dataset_id};modelId={model_id};task=basic/train?project={project_id}'.format(
                  project_id=gcp_project_id,
                  region=gcp_region,
                  dataset_id=dataset_id,
                  model_id=model_id,
              )

              return (model_name, model_id, model_url)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Automl create model for tables', description='')
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--display-name", dest="display_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--dataset-id", dest="dataset_id", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--target-column-path", dest="target_column_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--input-feature-column-paths", dest="input_feature_column_paths", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimization-objective", dest="optimization_objective", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--train-budget-milli-node-hours", dest="train_budget_milli_node_hours", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_create_model_for_tables(**_parsed_args)

          _output_serializers = [
              _serialize_str,
              _serialize_str,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --gcp-project-id
        - {inputValue: gcp_project_id}
        - --gcp-region
        - {inputValue: gcp_region}
        - --display-name
        - {inputValue: display_name}
        - --dataset-id
        - {inputValue: dataset_id}
        - if:
            cond: {isPresent: target_column_path}
            then:
            - --target-column-path
            - {inputValue: target_column_path}
        - if:
            cond: {isPresent: input_feature_column_paths}
            then:
            - --input-feature-column-paths
            - {inputValue: input_feature_column_paths}
        - if:
            cond: {isPresent: optimization_objective}
            then:
            - --optimization-objective
            - {inputValue: optimization_objective}
        - if:
            cond: {isPresent: train_budget_milli_node_hours}
            then:
            - --train-budget-milli-node-hours
            - {inputValue: train_budget_milli_node_hours}
        - '----output-paths'
        - {outputPath: model_path}
        - {outputPath: model_id}
        - {outputPath: model_page_url}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/deploy_model/component.yaml',
    digest: 937cbc77c84f7fd8b23937f8070ec4129baaeaeec9faed724190b6b941c32396}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl deploy model
    description: |-
      Deploys a trained model.

          Args:
              model_path: The resource name of the model to export. Format: 'projects/<project>/locations/<location>/models/<model>'

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: model_path, type: String}
    outputs:
    - {name: model_path, type: String}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/deploy_model/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==2.0.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location 'google-cloud-automl==2.0.0' --user)
          && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def automl_deploy_model(
              model_path,
          ):
              """Deploys a trained model.

              Args:
                  model_path: The resource name of the model to export. Format: 'projects/<project>/locations/<location>/models/<model>'

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              from google.cloud import automl
              client = automl.AutoMlClient()
              response = client.deploy_model(
                  name=model_path,
              )
              print('Operation started:')
              print(response.operation)
              result = response.result()
              metadata = response.metadata
              print('Operation finished:')
              print(metadata)
              return (model_path, )

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import argparse
          _parser = argparse.ArgumentParser(prog='Automl deploy model', description="Deploys a trained model.\n\n    Args:\n        model_path: The resource name of the model to export. Format: 'projects/<project>/locations/<location>/models/<model>'\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
          _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_deploy_model(**_parsed_args)

          _output_serializers = [
              _serialize_str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --model-path
        - {inputValue: model_path}
        - '----output-paths'
        - {outputPath: model_path}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/export_data_to_gcs/component.yaml',
    digest: 09bf2c70eb8dd963b1688ae8335aa58dd140b8d7f1ad879db7008aa90e26e571}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl export data to gcs
    description: |
      Exports dataset data to GCS.
    inputs:
    - name: dataset_path
      type: String
    - name: gcs_output_uri_prefix
      optional: true
      type: String
    - name: timeout
      optional: true
      type: Float
    - default: '{}'
      name: metadata
      optional: true
      type: JsonObject
    outputs:
    - name: gcs_output_uri_prefix
      type: String
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/export_data_to_gcs/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - python3
        - -u
        - -c
        - |
          from typing import NamedTuple

          def automl_export_data_to_gcs(
              dataset_path: str,
              gcs_output_uri_prefix: str = None,
              #retry=None, #=google.api_core.gapic_v1.method.DEFAULT,
              timeout: float = None, #=google.api_core.gapic_v1.method.DEFAULT,
              metadata: dict = {},
          ) -> NamedTuple('Outputs', [('gcs_output_uri_prefix', str)]):
              """Exports dataset data to GCS."""
              import sys
              import subprocess
              subprocess.run([sys.executable, "-m", "pip", "install", "google-cloud-automl==0.4.0", "--quiet", "--no-warn-script-location"], env={"PIP_DISABLE_PIP_VERSION_CHECK": "1"}, check=True)

              import google
              from google.cloud import automl
              client = automl.AutoMlClient()

              output_config = {"gcs_destination": {"output_uri_prefix": gcs_output_uri_prefix}}

              response = client.export_data(
                  name=dataset_path,
                  output_config=output_config,
                  #retry=retry or google.api_core.gapic_v1.method.DEFAULT
                  timeout=timeout or google.api_core.gapic_v1.method.DEFAULT,
                  metadata=metadata,
              )
              print('Operation started:')
              print(response.operation)
              result = response.result()
              metadata = response.metadata
              print('Operation finished:')
              print(metadata)
              return (gcs_output_uri_prefix, )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Automl export data to gcs', description='Exports dataset data to GCS.\n')
          _parser.add_argument("--dataset-path", dest="dataset_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--gcs-output-uri-prefix", dest="gcs_output_uri_prefix", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--timeout", dest="timeout", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metadata", dest="metadata", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_export_data_to_gcs(**_parsed_args)

          if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
              _outputs = [_outputs]

          _output_serializers = [
              str
          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --dataset-path
        - inputValue: dataset_path
        - if:
            cond:
              isPresent: gcs_output_uri_prefix
            then:
            - --gcs-output-uri-prefix
            - inputValue: gcs_output_uri_prefix
        - if:
            cond:
              isPresent: timeout
            then:
            - --timeout
            - inputValue: timeout
        - if:
            cond:
              isPresent: metadata
            then:
            - --metadata
            - inputValue: metadata
        - '----output-paths'
        - outputPath: gcs_output_uri_prefix
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/export_model_to_gcs/component.yaml',
    digest: 62158ef7c92a369f58eb0a2415c3179535b8dcd1382602b81e9b7bc7f4b0a9f6}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl export model to gcs
    description: |-
      Exports a trained model to a user specified Google Cloud Storage location.

          Args:
              model_path: The resource name of the model to export. Format: 'projects/<project>/locations/<location>/models/<model>'
              gcs_output_uri_prefix: The Google Cloud Storage directory where the model should be written to. Must be in the same location as AutoML. Required location: us-central1.
              model_format: The format in which the model must be exported. The available, and default, formats depend on the problem and model type. Possible formats: tf_saved_model, tf_js, tflite, core_ml, edgetpu_tflite. See https://cloud.google.com/automl/docs/reference/rest/v1/projects.locations.models/export?hl=en#modelexportoutputconfig

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: model_path, type: String}
    - {name: gcs_output_uri_prefix, type: String}
    - {name: model_format, type: String, default: tf_saved_model, optional: true}
    outputs:
    - {name: model_directory, type: Uri}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/export_model_to_gcs/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==2.0.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location 'google-cloud-automl==2.0.0' --user)
          && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def automl_export_model_to_gcs(
              model_path,
              gcs_output_uri_prefix,
              model_format = 'tf_saved_model',
          ):
              """Exports a trained model to a user specified Google Cloud Storage location.

              Args:
                  model_path: The resource name of the model to export. Format: 'projects/<project>/locations/<location>/models/<model>'
                  gcs_output_uri_prefix: The Google Cloud Storage directory where the model should be written to. Must be in the same location as AutoML. Required location: us-central1.
                  model_format: The format in which the model must be exported. The available, and default, formats depend on the problem and model type. Possible formats: tf_saved_model, tf_js, tflite, core_ml, edgetpu_tflite. See https://cloud.google.com/automl/docs/reference/rest/v1/projects.locations.models/export?hl=en#modelexportoutputconfig

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              from google.cloud import automl

              client = automl.AutoMlClient()
              response = client.export_model(
                  name=model_path,
                  output_config=automl.ModelExportOutputConfig(
                      model_format=model_format,
                      gcs_destination=automl.GcsDestination(
                          output_uri_prefix=gcs_output_uri_prefix,
                      ),
                  ),
              )

              print('Operation started:')
              print(response.operation)
              result = response.result()
              metadata = response.metadata
              print('Operation finished:')
              print(metadata)
              return (metadata.export_model_details.output_info.gcs_output_directory, )

          import argparse
          _parser = argparse.ArgumentParser(prog='Automl export model to gcs', description="Exports a trained model to a user specified Google Cloud Storage location.\n\n    Args:\n        model_path: The resource name of the model to export. Format: 'projects/<project>/locations/<location>/models/<model>'\n        gcs_output_uri_prefix: The Google Cloud Storage directory where the model should be written to. Must be in the same location as AutoML. Required location: us-central1.\n        model_format: The format in which the model must be exported. The available, and default, formats depend on the problem and model type. Possible formats: tf_saved_model, tf_js, tflite, core_ml, edgetpu_tflite. See https://cloud.google.com/automl/docs/reference/rest/v1/projects.locations.models/export?hl=en#modelexportoutputconfig\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>")
          _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--gcs-output-uri-prefix", dest="gcs_output_uri_prefix", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model-format", dest="model_format", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_export_model_to_gcs(**_parsed_args)

          _output_serializers = [
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --model-path
        - {inputValue: model_path}
        - --gcs-output-uri-prefix
        - {inputValue: gcs_output_uri_prefix}
        - if:
            cond: {isPresent: model_format}
            then:
            - --model-format
            - {inputValue: model_format}
        - '----output-paths'
        - {outputPath: model_directory}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/import_data_from_bigquery/component.yaml',
    digest: f826eec2ddad5f547fb0d1aaffa116d2f0770c0ca1cd865bfebd3f9f749ca186}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl import data from bigquery
    inputs:
    - name: dataset_path
    - name: input_uri
      type: String
    - name: retry
      optional: true
    - name: timeout
      optional: true
    - name: metadata
      type: JsonObject
      optional: true
    outputs:
    - name: dataset_path
      type: String
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/import_data_from_bigquery/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - python3
        - -u
        - -c
        - |
          from typing import NamedTuple

          def automl_import_data_from_bigquery(
              dataset_path,
              input_uri: str,
              retry=None, #=google.api_core.gapic_v1.method.DEFAULT,
              timeout=None, #=google.api_core.gapic_v1.method.DEFAULT,
              metadata: dict = None,
          ) -> NamedTuple('Outputs', [('dataset_path', str)]):
              import sys
              import subprocess
              subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

              import google
              from google.cloud import automl
              client = automl.AutoMlClient()
              input_config = {
                  'bigquery_source': {
                      'input_uri': input_uri,
                  },
              }
              response = client.import_data(
                  dataset_path,
                  input_config,
                  retry or google.api_core.gapic_v1.method.DEFAULT,
                  timeout or google.api_core.gapic_v1.method.DEFAULT,
                  metadata,
              )
              result = response.result()
              print(result)
              metadata = response.metadata
              print(metadata)
              return (dataset_path)

          import json
          import argparse
          _missing_arg = object()
          _parser = argparse.ArgumentParser(prog='Automl import data from bigquery', description='')
          _parser.add_argument("--dataset-path", dest="dataset_path", type=str, required=True, default=_missing_arg)
          _parser.add_argument("--input-uri", dest="input_uri", type=str, required=True, default=_missing_arg)
          _parser.add_argument("--retry", dest="retry", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--timeout", dest="timeout", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--metadata", dest="metadata", type=json.loads, required=False, default=_missing_arg)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_import_data_from_bigquery(**_parsed_args)

          if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
              _outputs = [_outputs]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(str(_outputs[idx]))
        args:
        - --dataset-path
        - inputValue: dataset_path
        - --input-uri
        - inputValue: input_uri
        - if:
            cond:
              isPresent: retry
            then:
            - --retry
            - inputValue: retry
        - if:
            cond:
              isPresent: timeout
            then:
            - --timeout
            - inputValue: timeout
        - if:
            cond:
              isPresent: metadata
            then:
            - --metadata
            - inputValue: metadata
        - '----output-paths'
        - outputPath: dataset_path
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/import_data_from_gcs/component.yaml',
    digest: 3034cfe00882c9ab7d8bf2f61cafdad8ed061f045b0a3b752d05f9e00256df9b}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl import data from gcs
    inputs:
    - name: dataset_path
      type: String
    - name: input_uris
      type: JsonArray
    - name: retry
      optional: true
    - name: timeout
      optional: true
    - name: metadata
      type: JsonObject
      optional: true
    outputs:
    - name: dataset_path
      type: String
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/import_data_from_gcs/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - python3
        - -u
        - -c
        - |
          from typing import NamedTuple

          def automl_import_data_from_gcs(
              dataset_path: str,
              input_uris: list,
              retry=None, #=google.api_core.gapic_v1.method.DEFAULT,
              timeout=None, #=google.api_core.gapic_v1.method.DEFAULT,
              metadata: dict = None,
          ) -> NamedTuple('Outputs', [('dataset_path', str)]):
              import sys
              import subprocess
              subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

              import google
              from google.cloud import automl
              client = automl.AutoMlClient()
              input_config = {
                  'gcs_source': {
                      'input_uris': input_uris,
                  },
              }
              response = client.import_data(
                  dataset_path,
                  input_config,
                  retry or google.api_core.gapic_v1.method.DEFAULT,
                  timeout or google.api_core.gapic_v1.method.DEFAULT,
                  metadata,
              )
              result = response.result()
              print(result)
              metadata = response.metadata
              print(metadata)
              return (dataset_path)

          import json
          import argparse
          _missing_arg = object()
          _parser = argparse.ArgumentParser(prog='Automl import data from gcs', description='')
          _parser.add_argument("--dataset-path", dest="dataset_path", type=str, required=True, default=_missing_arg)
          _parser.add_argument("--input-uris", dest="input_uris", type=json.loads, required=True, default=_missing_arg)
          _parser.add_argument("--retry", dest="retry", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--timeout", dest="timeout", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--metadata", dest="metadata", type=json.loads, required=False, default=_missing_arg)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_import_data_from_gcs(**_parsed_args)

          if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
              _outputs = [_outputs]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(str(_outputs[idx]))
        args:
        - --dataset-path
        - inputValue: dataset_path
        - --input-uris
        - inputValue: input_uris
        - if:
            cond:
              isPresent: retry
            then:
            - --retry
            - inputValue: retry
        - if:
            cond:
              isPresent: timeout
            then:
            - --timeout
            - inputValue: timeout
        - if:
            cond:
              isPresent: metadata
            then:
            - --metadata
            - inputValue: metadata
        - '----output-paths'
        - outputPath: dataset_path
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/prediction_service_batch_predict/component.yaml',
    digest: 6fef3ce6231a9a478abbe0a9c9eaf45f1dd202aa4e988b86bcda6d90138f3dd6}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl prediction service batch predict
    inputs:
    - name: model_path
    - name: gcs_input_uris
      type: JsonArray
      optional: true
    - name: gcs_output_uri_prefix
      type: String
      optional: true
    - name: bq_input_uri
      type: String
      optional: true
    - name: bq_output_uri
      type: String
      optional: true
    - name: params
      optional: true
    - name: retry
      optional: true
    - name: timeout
      optional: true
    - name: metadata
      type: JsonObject
      optional: true
    outputs:
    - name: gcs_output_directory
      type: String
    - name: bigquery_output_dataset
      type: String
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/prediction_service_batch_predict/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - python3
        - -u
        - -c
        - |
          from typing import NamedTuple

          def automl_prediction_service_batch_predict(
              model_path,
              gcs_input_uris: str = None,
              gcs_output_uri_prefix: str = None,
              bq_input_uri: str = None,
              bq_output_uri: str = None,
              params=None,
              retry=None, #google.api_core.gapic_v1.method.DEFAULT,
              timeout=None, #google.api_core.gapic_v1.method.DEFAULT,
              metadata: dict = None,
          ) -> NamedTuple('Outputs', [('gcs_output_directory', str), ('bigquery_output_dataset', str)]):
              import sys
              import subprocess
              subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

              input_config = {}
              if gcs_input_uris:
                  input_config['gcs_source'] = {'input_uris': gcs_input_uris}
              if bq_input_uri:
                  input_config['bigquery_source'] = {'input_uri': bq_input_uri}

              output_config = {}
              if gcs_output_uri_prefix:
                  output_config['gcs_destination'] = {'output_uri_prefix': gcs_output_uri_prefix}
              if bq_output_uri:
                  output_config['bigquery_destination'] = {'output_uri': bq_output_uri}

              from google.cloud import automl
              client = automl.PredictionServiceClient()
              response = client.batch_predict(
                  model_path,
                  input_config,
                  output_config,
                  params,
                  retry,
                  timeout,
                  metadata,
              )
              print('Operation started:')
              print(response.operation)
              result = response.result()
              metadata = response.metadata
              print('Operation finished:')
              print(metadata)
              output_info = metadata.batch_predict_details.output_info
              # Workaround for Argo issue - it fails when output is empty: https://github.com/argoproj/argo/pull/1277/files#r326028422
              return (output_info.gcs_output_directory or '-', output_info.bigquery_output_dataset or '-')

          import json
          import argparse
          _missing_arg = object()
          _parser = argparse.ArgumentParser(prog='Automl prediction service batch predict', description='')
          _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=_missing_arg)
          _parser.add_argument("--gcs-input-uris", dest="gcs_input_uris", type=json.loads, required=False, default=_missing_arg)
          _parser.add_argument("--gcs-output-uri-prefix", dest="gcs_output_uri_prefix", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--bq-input-uri", dest="bq_input_uri", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--bq-output-uri", dest="bq_output_uri", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--params", dest="params", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--retry", dest="retry", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--timeout", dest="timeout", type=str, required=False, default=_missing_arg)
          _parser.add_argument("--metadata", dest="metadata", type=json.loads, required=False, default=_missing_arg)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_prediction_service_batch_predict(**_parsed_args)

          if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
              _outputs = [_outputs]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(str(_outputs[idx]))
        args:
        - --model-path
        - inputValue: model_path
        - if:
            cond:
              isPresent: gcs_input_uris
            then:
            - --gcs-input-uris
            - inputValue: gcs_input_uris
        - if:
            cond:
              isPresent: gcs_output_uri_prefix
            then:
            - --gcs-output-uri-prefix
            - inputValue: gcs_output_uri_prefix
        - if:
            cond:
              isPresent: bq_input_uri
            then:
            - --bq-input-uri
            - inputValue: bq_input_uri
        - if:
            cond:
              isPresent: bq_output_uri
            then:
            - --bq-output-uri
            - inputValue: bq_output_uri
        - if:
            cond:
              isPresent: params
            then:
            - --params
            - inputValue: params
        - if:
            cond:
              isPresent: retry
            then:
            - --retry
            - inputValue: retry
        - if:
            cond:
              isPresent: timeout
            then:
            - --timeout
            - inputValue: timeout
        - if:
            cond:
              isPresent: metadata
            then:
            - --metadata
            - inputValue: metadata
        - '----output-paths'
        - outputPath: gcs_output_directory
        - outputPath: bigquery_output_dataset
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/gcp/automl/split_dataset_table_column_names/component.yaml',
    digest: 49cdf855d3269a7139b77c59c1c5fed5b5ae0663d5c5202f3aeb915fb2351b6c}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Automl split dataset table column names
    inputs:
    - name: dataset_path
      type: String
    - name: target_column_name
      type: String
    - name: table_index
      type: Integer
      default: '0'
      optional: true
    outputs:
    - name: target_column_path
      type: String
    - name: feature_column_paths
      type: JsonArray
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/gcp/automl/split_dataset_table_column_names/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - python3
        - -u
        - -c
        - |
          from typing import NamedTuple

          def automl_split_dataset_table_column_names(
              dataset_path: str,
              target_column_name: str,
              table_index: int = 0,
          ) -> NamedTuple('Outputs', [('target_column_path', str), ('feature_column_paths', list)]):
              import sys
              import subprocess
              subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

              from google.cloud import automl
              client = automl.AutoMlClient()
              list_table_specs_response = client.list_table_specs(dataset_path)
              table_specs = [s for s in list_table_specs_response]
              print('table_specs=')
              print(table_specs)
              table_spec_name = table_specs[table_index].name

              list_column_specs_response = client.list_column_specs(table_spec_name)
              column_specs = [s for s in list_column_specs_response]
              print('column_specs=')
              print(column_specs)

              target_column_spec = [s for s in column_specs if s.display_name == target_column_name][0]
              feature_column_specs = [s for s in column_specs if s.display_name != target_column_name]
              feature_column_names = [s.name for s in feature_column_specs]

              import json
              return (target_column_spec.name, json.dumps(feature_column_names))

          import argparse
          _missing_arg = object()
          _parser = argparse.ArgumentParser(prog='Automl split dataset table column names', description='')
          _parser.add_argument("--dataset-path", dest="dataset_path", type=str, required=True, default=_missing_arg)
          _parser.add_argument("--target-column-name", dest="target_column_name", type=str, required=True, default=_missing_arg)
          _parser.add_argument("--table-index", dest="table_index", type=int, required=False, default=_missing_arg)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = automl_split_dataset_table_column_names(**_parsed_args)

          if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
              _outputs = [_outputs]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(str(_outputs[idx]))
        args:
        - --dataset-path
        - inputValue: dataset_path
        - --target-column-name
        - inputValue: target_column_name
        - if:
            cond:
              isPresent: table_index
            then:
            - --table-index
            - inputValue: table_index
        - '----output-paths'
        - outputPath: target_column_path
        - outputPath: feature_column_paths
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/git/clone/component.yaml',
    digest: 2187e588f6a05550736861c5053e40c770c02716d67da9c6be5b96d169261809}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Git clone
    description: Creates a shallow clone of the specified repo branch
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/git/clone/component.yaml'
        volatile_component: "true"
    inputs:
    - {name: Repo URI, type: URI}
    - {name: Branch, type: String, default: master}
    outputs:
    - {name: Repo dir, type: Directory}
    implementation:
      container:
        image: alpine/git
        command:
        - git
        - clone
        - --depth=1
        - --branch
        - inputValue: Branch
        - inputValue: Repo URI
        - outputPath: Repo dir
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/247a0e0cf55c28b978c7f8e44fcedba356c12e93/components/google-cloud/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml',
    digest: 2cf52d5af8e83541dd838da498e3160ce3475d061e75de12260ed0233e1a5b4a}
  annotations:
    GitHub commit:
      sha: 247a0e0cf55c28b978c7f8e44fcedba356c12e93
      html_url: https://github.com/Ark-kun/pipeline_components/commit/247a0e0cf55c28b978c7f8e44fcedba356c12e93
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/247a0e0cf55c28b978c7f8e44fcedba356c12e93
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T05:44:06Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T05:44:06Z'}
        message: Google Cloud - AutoML - Tables - Create_dataset - Added the "from_BigQuery"
          component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create dataset from BigQuery for Google Cloud AutoML Tables
    description: Creates Google Cloud AutoML Tables Dataset from CSV data stored in GCS.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml'}
    inputs:
    - name: data_uri
      type: GoogleCloudBigQueryUri
      description: |-
        Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.
        The bucket must be a regional bucket in the us-central1 region.
        The file name must have a (case-insensitive) '.CSV' file extension.
    - {name: target_column_name, type: String, description: Name of the target column
        for training., optional: true}
    - {name: column_nullability, type: JsonObject, description: Maps column name to boolean
        specifying whether the column should be marked as nullable., default: '{}', optional: true}
    - {name: column_types, type: JsonObject, description: 'Maps column name to column
        type. Supported types: FLOAT64, CATEGORY, STRING.', default: '{}', optional: true}
    - name: dataset_display_name
      type: String
      description: |-
        Display name for the AutoML Dataset.
        Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
      optional: true
    - {name: gcp_project_id, type: String, description: 'Google Cloud project ID. If not
        set, the default one will be used.', optional: true}
    - {name: gcp_region, type: String, description: Google Cloud region. AutoML Tables
        only supports us-central1., default: us-central1, optional: true}
    outputs:
    - {name: dataset_name, type: String}
    - {name: dataset, type: JsonObject}
    - {name: dataset_url, type: URI}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==2.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location 'google-cloud-automl==2.4.2' --user)
          && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_dataset_from_BigQuery_for_Google_Cloud_AutoML_Tables(
              data_uri,
              target_column_name = None,
              column_nullability = {},
              column_types = {},
              dataset_display_name = None,
              gcp_project_id = None,
              gcp_region = 'us-central1',  # Currently "us-central1" is the only region supported by AutoML tables.
          ):
              '''Creates Google Cloud AutoML Tables Dataset from CSV data stored in GCS.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  data_uri: Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.
                      The bucket must be a regional bucket in the us-central1 region.
                      The file name must have a (case-insensitive) '.CSV' file extension.
                  target_column_name: Name of the target column for training.
                  column_nullability: Maps column name to boolean specifying whether the column should be marked as nullable.
                  column_types: Maps column name to column type. Supported types: FLOAT64, CATEGORY, STRING.
                  dataset_display_name: Display name for the AutoML Dataset.
                      Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                  gcp_project_id: Google Cloud project ID. If not set, the default one will be used.
                  gcp_region: Google Cloud region. AutoML Tables only supports us-central1.
              Returns:
                  dataset_name: AutoML dataset name (fully-qualified)
              '''

              import datetime
              import logging

              import google.auth
              from google.cloud import automl_v1beta1 as automl
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              # Validating and inferring the arguments

              if not gcp_project_id:
                  _, gcp_project_id = google.auth.default()

              if not gcp_region:
                  gcp_region = 'us-central1'
              if gcp_region != 'us-central1':
                  logging.warn('AutoML only supports the us-central1 region')
              if not dataset_display_name:
                  dataset_display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              column_nullability = column_nullability or {}
              for name, nullability in column_nullability.items():
                  assert isinstance(name, str)
                  assert isinstance(nullability, bool)

              column_types = column_types or {}
              for name, data_type in column_types.items():
                  assert isinstance(name, str)
                  if not hasattr(automl.TypeCode, data_type):
                      supported_types = [type_name for type_name in dir(automl.TypeCode) if type_name[0] != '_']
                      raise ValueError(f'Unknown column type "{data_type}". Supported types: {supported_types}')

              logging.info(f'Creating AutoML Tables dataset.')
              automl_client = automl.AutoMlClient()

              project_location_path = f'projects/{gcp_project_id}/locations/{gcp_region}'

              dataset = automl.Dataset(
                  display_name=dataset_display_name,
                  tables_dataset_metadata=automl.TablesDatasetMetadata(),
                  # labels={},
              )
              dataset = automl_client.create_dataset(
                  dataset=dataset,
                  parent=project_location_path,
              )
              dataset_id = dataset.name.split('/')[-1]
              dataset_web_url = f'https://console.cloud.google.com/automl-tables/locations/{gcp_region}/datasets/{dataset_id}'
              logging.info(f'Created dataset {dataset.name}.')
              logging.info(f'Link: {dataset_web_url}')

              logging.info(f'Importing data to the dataset: {dataset.name}.')
              import_data_input_config = automl.InputConfig(
                  bigquery_source=automl.BigQuerySource(
                      input_uri=data_uri,
                  )
              )
              import_data_response = automl_client.import_data(
                  name=dataset.name,
                  input_config=import_data_input_config,
              )
              import_data_response.result()
              dataset = automl_client.get_dataset(
                  name=dataset.name,
              )
              logging.info(f'Finished importing data.')

              logging.info('Updating column specs')
              target_column_spec = None
              primary_table_spec_name = dataset.name + '/tableSpecs/' + dataset.tables_dataset_metadata.primary_table_spec_id
              table_specs_list = list(automl_client.list_table_specs(
                  parent=dataset.name,
              ))
              for table_spec in table_specs_list:
                  column_specs_list = list(automl_client.list_column_specs(
                      parent=table_spec.name,
                  ))
                  is_primary_table = table_spec.name == primary_table_spec_name
                  for column_spec in column_specs_list:
                      if column_spec.display_name == target_column_name and is_primary_table:
                          target_column_spec = column_spec
                      column_updated = False
                      if column_spec.display_name in column_nullability:
                          column_spec.data_type.nullable = column_nullability[column_spec.display_name]
                          column_updated = True
                      if column_spec.display_name in column_types:
                          new_column_type = column_types[column_spec.display_name]
                          column_spec.data_type.type_code = getattr(automl.TypeCode, new_column_type)
                          column_updated = True
                      if column_updated:
                          automl_client.update_column_spec(column_spec=column_spec)

              if target_column_name:
                  logging.info('Setting target column')
                  if not target_column_spec:
                      raise ValueError(f'Primary table does not have column "{target_column_name}"')
                  target_column_spec_id = target_column_spec.name.split('/')[-1]
                  dataset.tables_dataset_metadata.target_column_spec_id = target_column_spec_id
                  dataset = automl_client.update_dataset(dataset=dataset)

              dataset_json = json_format.MessageToJson(dataset._pb)
              print(dataset_json)

              return (dataset.name, dataset_json, dataset_web_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Create dataset from BigQuery for Google Cloud AutoML Tables', description='Creates Google Cloud AutoML Tables Dataset from CSV data stored in GCS.')
          _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--target-column-name", dest="target_column_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--column-nullability", dest="column_nullability", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--column-types", dest="column_types", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--dataset-display-name", dest="dataset_display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_dataset_from_BigQuery_for_Google_Cloud_AutoML_Tables(**_parsed_args)

          _output_serializers = [
              _serialize_str,
              _serialize_json,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data-uri
        - {inputValue: data_uri}
        - if:
            cond: {isPresent: target_column_name}
            then:
            - --target-column-name
            - {inputValue: target_column_name}
        - if:
            cond: {isPresent: column_nullability}
            then:
            - --column-nullability
            - {inputValue: column_nullability}
        - if:
            cond: {isPresent: column_types}
            then:
            - --column-types
            - {inputValue: column_types}
        - if:
            cond: {isPresent: dataset_display_name}
            then:
            - --dataset-display-name
            - {inputValue: dataset_display_name}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset}
        - {outputPath: dataset_url}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/3862f752bb825bb8cdcae842f0b294794028376b/components/google-cloud/AutoML/Tables/Create_dataset/from_CSV/component.yaml',
    digest: 30b6e53f9d0b37dee6a9127656fa0f414b4091a5578906c9e419e84863f812f3}
  annotations:
    GitHub commit:
      sha: 3862f752bb825bb8cdcae842f0b294794028376b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/3862f752bb825bb8cdcae842f0b294794028376b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/3862f752bb825bb8cdcae842f0b294794028376b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T05:44:55Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T05:44:55Z'}
        message: Google Cloud - AutoML - Tables - Create_dataset - Updated the "from_CSV"
          component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create dataset from CSV for Google Cloud AutoML Tables
    description: Creates Google Cloud AutoML Tables Dataset from CSV data.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/AutoML/Tables/Create_dataset/from_CSV/component.yaml'}
    inputs:
    - {name: data, type: CSV, description: Data in CSV format that will be imported to
        the dataset.}
    - {name: target_column_name, type: String, description: Name of the target column
        for training., optional: true}
    - {name: column_nullability, type: JsonObject, description: Maps column name to boolean
        specifying whether the column should be marked as nullable., default: '{}', optional: true}
    - {name: column_types, type: JsonObject, description: 'Maps column name to column
        type. Supported types: FLOAT64, CATEGORY, STRING.', default: '{}', optional: true}
    - {name: gcs_staging_uri, type: String, description: 'URI of the data staging location
        in Google Cloud Storage. The bucket must have the us-central1 region. If not specified,
        a new staging bucket will be created.', optional: true}
    - name: dataset_display_name
      type: String
      description: |-
        Display name for the AutoML Dataset.
        Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
      optional: true
    - {name: gcp_project_id, type: String, description: 'Google Cloud project ID. If not
        set, the default one will be used.', optional: true}
    - {name: gcp_region, type: String, description: Google Cloud region. AutoML Tables
        only supports us-central1., default: us-central1, optional: true}
    outputs:
    - {name: dataset_name, type: String}
    - {name: dataset, type: JsonObject}
    - {name: dataset_url, type: URI}
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==2.4.2' 'google-cloud-storage==1.41.1' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location 'google-cloud-automl==2.4.2'
          'google-cloud-storage==1.41.1' --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_dataset_from_CSV_for_Google_Cloud_AutoML_Tables(
              data_path,
              target_column_name = None,
              column_nullability = {},
              column_types = {},
              gcs_staging_uri = None,  # Currently AutoML Tables only supports regional buckets in "us-central1".
              dataset_display_name = None,
              gcp_project_id = None,
              gcp_region = 'us-central1',  # Currently "us-central1" is the only region supported by AutoML tables.
          ):
              '''Creates Google Cloud AutoML Tables Dataset from CSV data.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  data_path: Data in CSV format that will be imported to the dataset.
                  target_column_name: Name of the target column for training.
                  column_nullability: Maps column name to boolean specifying whether the column should be marked as nullable.
                  column_types: Maps column name to column type. Supported types: FLOAT64, CATEGORY, STRING.
                  gcs_staging_uri: URI of the data staging location in Google Cloud Storage. The bucket must have the us-central1 region. If not specified, a new staging bucket will be created.
                  dataset_display_name: Display name for the AutoML Dataset.
                      Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                  gcp_project_id: Google Cloud project ID. If not set, the default one will be used.
                  gcp_region: Google Cloud region. AutoML Tables only supports us-central1.
              Returns:
                  dataset_name: AutoML dataset name (fully-qualified)
              '''

              import datetime
              import logging
              import random

              import google.auth
              from google.cloud import automl_v1beta1 as automl
              from google.cloud import storage
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              # Validating and inferring the arguments

              if not gcp_project_id:
                  _, gcp_project_id = google.auth.default()

              if not gcp_region:
                  gcp_region = 'us-central1'
              if gcp_region != 'us-central1':
                  logging.warn('AutoML only supports the us-central1 region')
              if not dataset_display_name:
                  dataset_display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              column_nullability = column_nullability or {}
              for name, nullability in column_nullability.items():
                  assert isinstance(name, str)
                  assert isinstance(nullability, bool)

              column_types = column_types or {}
              for name, data_type in column_types.items():
                  assert isinstance(name, str)
                  if not hasattr(automl.TypeCode, data_type):
                      supported_types = [type_name for type_name in dir(automl.TypeCode) if type_name[0] != '_']
                      raise ValueError(f'Unknown column type "{data_type}". Supported types: {supported_types}')

              # Generating execution ID for data staging
              random_integer = random.SystemRandom().getrandbits(256)
              execution_id = '{:064x}'.format(random_integer)
              logging.info(f'Execution ID: {execution_id}')

              logging.info('Uploading the data to storage')
              # TODO: Split table into < 100MB chunks as required by AutoML Tables
              storage_client = storage.Client()
              if gcs_staging_uri:
                  if not gcs_staging_uri.startswith('gs://'):
                      raise ValueError(f"Invalid staging storage URI: {gcs_staging_uri}")
                  (bucket_name, blob_prefix) = gcs_staging_uri[5:].split('/', 1)
                  bucket = storage_client.get_bucket(bucket_name)
              else:
                  bucket_name = gcp_project_id + '_staging_' + gcp_region
                  try:
                      bucket = storage_client.get_bucket(bucket_name)
                  except Exception as ex:
                      logging.info(f'Creating Storage bucket {bucket_name}')
                      bucket = storage_client.create_bucket(
                          bucket_or_name=bucket_name,
                          project=gcp_project_id,
                          location=gcp_region,
                      )
                      logging.info(f'Created Storage bucket {bucket.name}')
                  blob_prefix = 'google.cloud.automl_tmp'

              # AutoML Tables import data requires that "the file name must have a (case-insensitive) '.CSV' file extension"
              training_data_blob_name = blob_prefix.rstrip('/') + '/' + execution_id + '/' + 'training_data.csv'
              training_data_blob_uri = f'gs://{bucket.name}/{training_data_blob_name}'
              training_data_blob = bucket.blob(training_data_blob_name)
              logging.info(f'Uploading training data to {training_data_blob_uri}')
              training_data_blob.upload_from_filename(data_path)

              logging.info(f'Creating AutoML Tables dataset.')
              automl_client = automl.AutoMlClient()

              project_location_path = f'projects/{gcp_project_id}/locations/{gcp_region}'

              dataset = automl.Dataset(
                  display_name=dataset_display_name,
                  tables_dataset_metadata=automl.TablesDatasetMetadata(),
                  # labels={},
              )
              dataset = automl_client.create_dataset(
                  dataset=dataset,
                  parent=project_location_path,
              )
              dataset_id = dataset.name.split('/')[-1]
              dataset_web_url = f'https://console.cloud.google.com/automl-tables/locations/{gcp_region}/datasets/{dataset_id}'
              logging.info(f'Created dataset {dataset.name}.')
              logging.info(f'Link: {dataset_web_url}')

              logging.info(f'Importing data to the dataset: {dataset.name}.')
              import_data_input_config = automl.InputConfig(
                  gcs_source=automl.GcsSource(
                      input_uris=[training_data_blob_uri],
                  )
              )
              import_data_response = automl_client.import_data(
                  name=dataset.name,
                  input_config=import_data_input_config,
              )
              import_data_response.result()
              dataset = automl_client.get_dataset(
                  name=dataset.name,
              )
              logging.info(f'Finished importing data.')

              logging.info('Updating column specs')
              target_column_spec = None
              primary_table_spec_name = dataset.name + '/tableSpecs/' + dataset.tables_dataset_metadata.primary_table_spec_id
              table_specs_list = list(automl_client.list_table_specs(
                  parent=dataset.name,
              ))
              for table_spec in table_specs_list:
                  table_spec_id = table_spec.name.split('/')[-1]
                  column_specs_list = list(automl_client.list_column_specs(
                      parent=table_spec.name,
                  ))
                  is_primary_table = table_spec.name == primary_table_spec_name
                  for column_spec in column_specs_list:
                      if column_spec.display_name == target_column_name and is_primary_table:
                          target_column_spec = column_spec
                      column_updated = False
                      if column_spec.display_name in column_nullability:
                          column_spec.data_type.nullable = column_nullability[column_spec.display_name]
                          column_updated = True
                      if column_spec.display_name in column_types:
                          new_column_type = column_types[column_spec.display_name]
                          column_spec.data_type.type_code = getattr(automl.TypeCode, new_column_type)
                          column_updated = True
                      if column_updated:
                          automl_client.update_column_spec(column_spec=column_spec)

              if target_column_name:
                  logging.info('Setting target column')
                  if not target_column_spec:
                      raise ValueError(f'Primary table does not have column "{target_column_name}"')
                  target_column_spec_id = target_column_spec.name.split('/')[-1]
                  dataset.tables_dataset_metadata.target_column_spec_id = target_column_spec_id
                  dataset = automl_client.update_dataset(dataset=dataset)

              dataset_json = json_format.MessageToJson(dataset._pb)
              print(dataset_json)

              return (dataset.name, dataset_json, dataset_web_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Create dataset from CSV for Google Cloud AutoML Tables', description='Creates Google Cloud AutoML Tables Dataset from CSV data.')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--target-column-name", dest="target_column_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--column-nullability", dest="column_nullability", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--column-types", dest="column_types", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcs-staging-uri", dest="gcs_staging_uri", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--dataset-display-name", dest="dataset_display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_dataset_from_CSV_for_Google_Cloud_AutoML_Tables(**_parsed_args)

          _output_serializers = [
              _serialize_str,
              _serialize_json,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data
        - {inputPath: data}
        - if:
            cond: {isPresent: target_column_name}
            then:
            - --target-column-name
            - {inputValue: target_column_name}
        - if:
            cond: {isPresent: column_nullability}
            then:
            - --column-nullability
            - {inputValue: column_nullability}
        - if:
            cond: {isPresent: column_types}
            then:
            - --column-types
            - {inputValue: column_types}
        - if:
            cond: {isPresent: gcs_staging_uri}
            then:
            - --gcs-staging-uri
            - {inputValue: gcs_staging_uri}
        - if:
            cond: {isPresent: dataset_display_name}
            then:
            - --dataset-display-name
            - {inputValue: dataset_display_name}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset}
        - {outputPath: dataset_url}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/46b2a4ef4ac530404958839ae397b9fd533cf154/components/google-cloud/AutoML/Tables/Create_dataset/from_GCS/component.yaml',
    digest: 673ed6e3f1a2a8b097e025c44d7bfa156927cedf0cde764669eccce8f6bedb85}
  annotations:
    GitHub commit:
      sha: 46b2a4ef4ac530404958839ae397b9fd533cf154
      html_url: https://github.com/Ark-kun/pipeline_components/commit/46b2a4ef4ac530404958839ae397b9fd533cf154
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/46b2a4ef4ac530404958839ae397b9fd533cf154
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T05:43:50Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T05:43:50Z'}
        message: Google Cloud - AutoML - Tables - Create_dataset - Added the "from_GCS"
          component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create dataset from GCS for Google Cloud AutoML Tables
    description: Creates Google Cloud AutoML Tables Dataset from CSV data stored in GCS.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}
    inputs:
    - name: data_uri
      type: GoogleCloudStorageUri
      description: |-
        Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
        The bucket must be a regional bucket in the us-central1 region.
        The file name must have a (case-insensitive) '.CSV' file extension.
    - {name: target_column_name, type: String, description: Name of the target column
        for training., optional: true}
    - {name: column_nullability, type: JsonObject, description: Maps column name to boolean
        specifying whether the column should be marked as nullable., default: '{}', optional: true}
    - {name: column_types, type: JsonObject, description: 'Maps column name to column
        type. Supported types: FLOAT64, CATEGORY, STRING.', default: '{}', optional: true}
    - name: dataset_display_name
      type: String
      description: |-
        Display name for the AutoML Dataset.
        Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
      optional: true
    - {name: gcp_project_id, type: String, description: 'Google Cloud project ID. If not
        set, the default one will be used.', optional: true}
    - {name: gcp_region, type: String, description: Google Cloud region. AutoML Tables
        only supports us-central1., default: us-central1, optional: true}
    outputs:
    - {name: dataset_name, type: String}
    - {name: dataset, type: JsonObject}
    - {name: dataset_url, type: URI}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-automl==2.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location 'google-cloud-automl==2.4.2' --user)
          && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_dataset_from_GCS_for_Google_Cloud_AutoML_Tables(
              data_uri,
              target_column_name = None,
              column_nullability = {},
              column_types = {},
              dataset_display_name = None,
              gcp_project_id = None,
              gcp_region = 'us-central1',  # Currently "us-central1" is the only region supported by AutoML tables.
          ):
              '''Creates Google Cloud AutoML Tables Dataset from CSV data stored in GCS.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  data_uri: Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
                      The bucket must be a regional bucket in the us-central1 region.
                      The file name must have a (case-insensitive) '.CSV' file extension.
                  target_column_name: Name of the target column for training.
                  column_nullability: Maps column name to boolean specifying whether the column should be marked as nullable.
                  column_types: Maps column name to column type. Supported types: FLOAT64, CATEGORY, STRING.
                  dataset_display_name: Display name for the AutoML Dataset.
                      Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                  gcp_project_id: Google Cloud project ID. If not set, the default one will be used.
                  gcp_region: Google Cloud region. AutoML Tables only supports us-central1.
              Returns:
                  dataset_name: AutoML dataset name (fully-qualified)
              '''

              import datetime
              import json
              import logging

              import google.auth
              from google.cloud import automl_v1beta1 as automl
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              # Validating and inferring the arguments

              if not gcp_project_id:
                  _, gcp_project_id = google.auth.default()

              if not gcp_region:
                  gcp_region = 'us-central1'
              if gcp_region != 'us-central1':
                  logging.warn('AutoML only supports the us-central1 region')
              if not dataset_display_name:
                  dataset_display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              # Hack to enable passing multiple URIs
              # I could have created another component or added another input, but it seems to be too much hassle for now.
              # An alternative would have been to accept comma-delimited or semicolon-delimited URLs.
              if data_uri.startswith("["):
                  data_uris = json.loads(data_uri)
              else:
                  data_uris = [data_uri]

              column_nullability = column_nullability or {}
              for name, nullability in column_nullability.items():
                  assert isinstance(name, str)
                  assert isinstance(nullability, bool)

              column_types = column_types or {}
              for name, data_type in column_types.items():
                  assert isinstance(name, str)
                  if not hasattr(automl.TypeCode, data_type):
                      supported_types = [type_name for type_name in dir(automl.TypeCode) if type_name[0] != '_']
                      raise ValueError(f'Unknown column type "{data_type}". Supported types: {supported_types}')

              logging.info(f'Creating AutoML Tables dataset.')
              automl_client = automl.AutoMlClient()

              project_location_path = f'projects/{gcp_project_id}/locations/{gcp_region}'

              dataset = automl.Dataset(
                  display_name=dataset_display_name,
                  tables_dataset_metadata=automl.TablesDatasetMetadata(),
                  # labels={},
              )
              dataset = automl_client.create_dataset(
                  dataset=dataset,
                  parent=project_location_path,
              )
              dataset_id = dataset.name.split('/')[-1]
              dataset_web_url = f'https://console.cloud.google.com/automl-tables/locations/{gcp_region}/datasets/{dataset_id}'
              logging.info(f'Created dataset {dataset.name}.')
              logging.info(f'Link: {dataset_web_url}')

              logging.info(f'Importing data to the dataset: {dataset.name}.')
              import_data_input_config = automl.InputConfig(
                  gcs_source=automl.GcsSource(
                      input_uris=data_uris,
                  )
              )
              import_data_response = automl_client.import_data(
                  name=dataset.name,
                  input_config=import_data_input_config,
              )
              import_data_response.result()
              dataset = automl_client.get_dataset(
                  name=dataset.name,
              )
              logging.info(f'Finished importing data.')

              logging.info('Updating column specs')
              target_column_spec = None
              primary_table_spec_name = dataset.name + '/tableSpecs/' + dataset.tables_dataset_metadata.primary_table_spec_id
              table_specs_list = list(automl_client.list_table_specs(
                  parent=dataset.name,
              ))
              for table_spec in table_specs_list:
                  column_specs_list = list(automl_client.list_column_specs(
                      parent=table_spec.name,
                  ))
                  is_primary_table = table_spec.name == primary_table_spec_name
                  for column_spec in column_specs_list:
                      if column_spec.display_name == target_column_name and is_primary_table:
                          target_column_spec = column_spec
                      column_updated = False
                      if column_spec.display_name in column_nullability:
                          column_spec.data_type.nullable = column_nullability[column_spec.display_name]
                          column_updated = True
                      if column_spec.display_name in column_types:
                          new_column_type = column_types[column_spec.display_name]
                          column_spec.data_type.type_code = getattr(automl.TypeCode, new_column_type)
                          column_updated = True
                      if column_updated:
                          automl_client.update_column_spec(column_spec=column_spec)

              if target_column_name:
                  logging.info('Setting target column')
                  if not target_column_spec:
                      raise ValueError(f'Primary table does not have column "{target_column_name}"')
                  target_column_spec_id = target_column_spec.name.split('/')[-1]
                  dataset.tables_dataset_metadata.target_column_spec_id = target_column_spec_id
                  dataset = automl_client.update_dataset(dataset=dataset)

              dataset_json = json_format.MessageToJson(dataset._pb)
              print(dataset_json)

              return (dataset.name, dataset_json, dataset_web_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Create dataset from GCS for Google Cloud AutoML Tables', description='Creates Google Cloud AutoML Tables Dataset from CSV data stored in GCS.')
          _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--target-column-name", dest="target_column_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--column-nullability", dest="column_nullability", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--column-types", dest="column_types", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--dataset-display-name", dest="dataset_display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_dataset_from_GCS_for_Google_Cloud_AutoML_Tables(**_parsed_args)

          _output_serializers = [
              _serialize_str,
              _serialize_json,
              str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data-uri
        - {inputValue: data_uri}
        - if:
            cond: {isPresent: target_column_name}
            then:
            - --target-column-name
            - {inputValue: target_column_name}
        - if:
            cond: {isPresent: column_nullability}
            then:
            - --column-nullability
            - {inputValue: column_nullability}
        - if:
            cond: {isPresent: column_types}
            then:
            - --column-types
            - {inputValue: column_types}
        - if:
            cond: {isPresent: dataset_display_name}
            then:
            - --dataset-display-name
            - {inputValue: dataset_display_name}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset}
        - {outputPath: dataset_url}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/1b87c0bdfde5d7ec039401af8561783432731402/components/google-cloud/Optimizer/Add_measurement_for_trial/component.yaml',
    digest: 488a4a66014bdf09351c02145f3bfb6b07233fff9da114fc25330bfd1578a184}
  annotations:
    GitHub commit:
      sha: 1b87c0bdfde5d7ec039401af8561783432731402
      html_url: https://github.com/Ark-kun/pipeline_components/commit/1b87c0bdfde5d7ec039401af8561783432731402
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/1b87c0bdfde5d7ec039401af8561783432731402
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        message: Components - Google Cloud - Optimizer - Switched to specifying the
          discovery doc using discoveryServiceUrl
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Add measurement for trial in gcp ai platform optimizer
    description: Add measurement for a trial and check whether to continue.
    inputs:
    - {name: trial_name, type: String, description: Full trial resource name.}
    - {name: metric_value, type: Float, description: Result of the trial evaluation.}
    - name: complete_trial
      type: Boolean
      description: Whether the trial should be completed. Only completed trials are used
        to suggest new trials. Default is True.
      default: "True"
      optional: true
    - {name: step_count, type: Float, description: Optional. The number of training steps
        performed with the model. Can be used when checking early stopping., optional: true}
    - {name: gcp_project_id, type: String, optional: true}
    - {name: gcp_region, type: String, default: us-central1, optional: true}
    outputs:
    - {name: trial_name, type: JsonArray}
    - {name: trial, type: JsonObject}
    - {name: stop_trial, type: Boolean}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Add_measurement_for_trial/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def add_measurement_for_trial_in_gcp_ai_platform_optimizer(
              trial_name,
              metric_value,
              complete_trial = True,
              step_count = None,
              gcp_project_id = None,
              gcp_region = "us-central1",
          ):
              """Add measurement for a trial and check whether to continue.
              See https://cloud.google.com/ai-platform/optimizer/docs

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  trial_name: Full trial resource name.
                  metric_value: Result of the trial evaluation.
                  step_count: Optional. The number of training steps performed with the model. Can be used when checking early stopping.
                  complete_trial: Whether the trial should be completed. Only completed trials are used to suggest new trials. Default is True.
              """

              import logging
              import time

              import google.auth
              from googleapiclient import discovery

              logging.getLogger().setLevel(logging.INFO)

              client_id = 'client1'
              metric_name = 'metric'

              credentials, default_project_id = google.auth.default()

              # Validating and inferring the arguments
              if not gcp_project_id:
                  gcp_project_id = default_project_id

              # Building the API client.
              # The main API does not work, so we need to build from the published discovery document.
              def create_caip_optimizer_client(project_id):
                  from googleapiclient import discovery
                  # The discovery is broken. See https://github.com/googleapis/google-api-python-client/issues/1470
                  # return discovery.build("ml", "v1")
                  return discovery.build("ml", "v1", discoveryServiceUrl='https://storage.googleapis.com/caip-optimizer-public/api/ml_public_google_rest_v1.json')

              # Workaround for the Optimizer bug: Optimizer returns resource names that use project number, but only supports resource names with project IDs when making requests
              def get_project_number(project_id):
                  service = discovery.build('cloudresourcemanager', 'v1', credentials=credentials)
                  response = service.projects().get(projectId=project_id).execute()
                  return response['projectNumber']

              gcp_project_number = get_project_number(gcp_project_id)

              def fix_resource_name(name):
                  return name.replace(gcp_project_number, gcp_project_id)

              ml_api = create_caip_optimizer_client(gcp_project_id)
              trials_api = ml_api.projects().locations().studies().trials()
              operations_api = ml_api.projects().locations().operations()

              measurement = {
                  'measurement': {
                      'stepCount': step_count,
                      'metrics': [{
                          'metric': metric_name,
                          'value': metric_value,
                      }],
                  },
              }
              add_measurement_response = trials_api.addMeasurement(
                  name=fix_resource_name(trial_name),
                  body=measurement,
              ).execute()

              if complete_trial:
                  should_stop_trial = True
                  complete_response = trials_api.complete(
                      name=fix_resource_name(trial_name),
                  ).execute()
                  return (trial_name, complete_response, should_stop_trial)
              else:
                  check_early_stopping_response = trials_api.checkEarlyStoppingState(
                      name=fix_resource_name(trial_name),
                  ).execute()
                  operation_name = check_early_stopping_response['name']
                  while True:
                      get_operation_response = operations_api.get(
                          name=fix_resource_name(operation_name),
                      ).execute()
                      if get_operation_response.get('done'):
                          break
                      logging.info('Not finished yet: ' + str(get_operation_response))
                      time.sleep(10)
                  operation_response = get_operation_response['response']
                  should_stop_trial = operation_response['shouldStop']
                  return (trial_name, add_measurement_response, should_stop_trial)

          def _serialize_bool(bool_value: bool) -> str:
              if isinstance(bool_value, str):
                  return bool_value
              if not isinstance(bool_value, bool):
                  raise TypeError('Value "{}" has type "{}" instead of bool.'.format(str(bool_value), str(type(bool_value))))
              return str(bool_value)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _deserialize_bool(s) -> bool:
              from distutils.util import strtobool
              return strtobool(s) == 1

          import argparse
          _parser = argparse.ArgumentParser(prog='Add measurement for trial in gcp ai platform optimizer', description='Add measurement for a trial and check whether to continue.')
          _parser.add_argument("--trial-name", dest="trial_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--metric-value", dest="metric_value", type=float, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--complete-trial", dest="complete_trial", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--step-count", dest="step_count", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = add_measurement_for_trial_in_gcp_ai_platform_optimizer(**_parsed_args)

          _output_serializers = [
              _serialize_json,
              _serialize_json,
              _serialize_bool,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --trial-name
        - {inputValue: trial_name}
        - --metric-value
        - {inputValue: metric_value}
        - if:
            cond: {isPresent: complete_trial}
            then:
            - --complete-trial
            - {inputValue: complete_trial}
        - if:
            cond: {isPresent: step_count}
            then:
            - --step-count
            - {inputValue: step_count}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: trial_name}
        - {outputPath: trial}
        - {outputPath: stop_trial}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/2d7cd0ffd3546165f5d58df6b3fd88db27258568/components/google-cloud/Optimizer/Build_double_parameter_spec/component.yaml',
    digest: cb54bed290d127828fb0c061e39875e121ca76a498f656bf93c0ae74662a301d}
  annotations:
    GitHub commit:
      sha: 2d7cd0ffd3546165f5d58df6b3fd88db27258568
      html_url: https://github.com/Ark-kun/pipeline_components/commit/2d7cd0ffd3546165f5d58df6b3fd88db27258568
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/2d7cd0ffd3546165f5d58df6b3fd88db27258568
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T22:35:46Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T22:35:46Z'}
        message: Google Cloud - Optimizer - Added the "Build_double_parameter_spec"
          and "Build_integer_parameter_spec" components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build double parameter spec for Google Cloud AI Platform Optimizer
    description: Builds an instance of Google Cloud AI Platform Optimizer ParameterSpec.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Build_double_parameter_spec/component.yaml'}
    inputs:
    - {name: parameter, type: String, description: Name of teh parameter. The parameter
        name must be unique amongst all ParameterSpecs.}
    - {name: min_value, type: Float, description: Minimum value of the parameter., default: '0',
      optional: true}
    - {name: max_value, type: Float, description: Maximum value of the parameter., default: '1',
      optional: true}
    - name: scale_type
      type: String
      description: |-
        The type of scaling that should be applied to this parameter.
        SCALE_TYPE_UNSPECIFIED By default, no scaling is applied.
        UNIT_LINEAR_SCALE Scales the feasible space to (0, 1) linearly.
        UNIT_LOG_SCALE Scales the feasible space logarithmically to (0, 1). The entire feasible space must be strictly positive.
        UNIT_REVERSE_LOG_SCALE Scales the feasible space "reverse" logarithmically to (0, 1). The result is that values close to the top of the feasible space are spread out more than points near the bottom. The entire feasible space must be strictly positive.
      optional: true
    outputs:
    - {name: parameter_spec, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def build_double_parameter_spec_for_Google_Cloud_AI_Platform_Optimizer(
              parameter,
              min_value = 0,
              max_value = 1,
              scale_type = None,
          ):
              '''Builds an instance of Google Cloud AI Platform Optimizer ParameterSpec.

              See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec

              Args:
                  parameter: Name of teh parameter. The parameter name must be unique amongst all ParameterSpecs.
                  min_value: Minimum value of the parameter.
                  max_value: Maximum value of the parameter.
                  scale_type: The type of scaling that should be applied to this parameter.
                      SCALE_TYPE_UNSPECIFIED By default, no scaling is applied.
                      UNIT_LINEAR_SCALE Scales the feasible space to (0, 1) linearly.
                      UNIT_LOG_SCALE Scales the feasible space logarithmically to (0, 1). The entire feasible space must be strictly positive.
                      UNIT_REVERSE_LOG_SCALE Scales the feasible space "reverse" logarithmically to (0, 1). The result is that values close to the top of the feasible space are spread out more than points near the bottom. The entire feasible space must be strictly positive.
              '''

              parameter_spec_dict = {
                  "parameter": parameter,
                  "type": "DOUBLE",
                  "doubleValueSpec": {
                      "minValue": min_value,
                      "maxValue": max_value,
                  },
              }
              if scale_type:
                  parameter_spec_dict["scaleType"] = scale_type

              return (parameter_spec_dict, )

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build double parameter spec for Google Cloud AI Platform Optimizer', description='Builds an instance of Google Cloud AI Platform Optimizer ParameterSpec.')
          _parser.add_argument("--parameter", dest="parameter", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--min-value", dest="min_value", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--max-value", dest="max_value", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--scale-type", dest="scale_type", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_double_parameter_spec_for_Google_Cloud_AI_Platform_Optimizer(**_parsed_args)

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --parameter
        - {inputValue: parameter}
        - if:
            cond: {isPresent: min_value}
            then:
            - --min-value
            - {inputValue: min_value}
        - if:
            cond: {isPresent: max_value}
            then:
            - --max-value
            - {inputValue: max_value}
        - if:
            cond: {isPresent: scale_type}
            then:
            - --scale-type
            - {inputValue: scale_type}
        - '----output-paths'
        - {outputPath: parameter_spec}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/2d7cd0ffd3546165f5d58df6b3fd88db27258568/components/google-cloud/Optimizer/Build_integer_parameter_spec/component.yaml',
    digest: ef775a39820b25fd6d30fad6dfa3452de3920b5cd97c8e3273b82dda714ec6b6}
  annotations:
    GitHub commit:
      sha: 2d7cd0ffd3546165f5d58df6b3fd88db27258568
      html_url: https://github.com/Ark-kun/pipeline_components/commit/2d7cd0ffd3546165f5d58df6b3fd88db27258568
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/2d7cd0ffd3546165f5d58df6b3fd88db27258568
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T22:35:46Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T22:35:46Z'}
        message: Google Cloud - Optimizer - Added the "Build_double_parameter_spec"
          and "Build_integer_parameter_spec" components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build integer parameter spec for Google Cloud AI Platform Optimizer
    description: Builds an instance of Google Cloud AI Platform Optimizer ParameterSpec.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Build_integer_parameter_spec/component.yaml'}
    inputs:
    - {name: parameter, type: String, description: Name of teh parameter. The parameter
        name must be unique amongst all ParameterSpecs.}
    - {name: min_value, type: Integer, description: Minimum value of the parameter., default: '0',
      optional: true}
    - {name: max_value, type: Integer, description: Maximum value of the parameter., default: '1',
      optional: true}
    - name: scale_type
      type: String
      description: |-
        The type of scaling that should be applied to this parameter.
        SCALE_TYPE_UNSPECIFIED By default, no scaling is applied.
        UNIT_LINEAR_SCALE Scales the feasible space to (0, 1) linearly.
        UNIT_LOG_SCALE Scales the feasible space logarithmically to (0, 1). The entire feasible space must be strictly positive.
        UNIT_REVERSE_LOG_SCALE Scales the feasible space "reverse" logarithmically to (0, 1). The result is that values close to the top of the feasible space are spread out more than points near the bottom. The entire feasible space must be strictly positive.
      optional: true
    outputs:
    - {name: parameter_spec, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def build_integer_parameter_spec_for_Google_Cloud_AI_Platform_Optimizer(
              parameter,
              min_value = 0,
              max_value = 1,
              scale_type = None,
          ):
              '''Builds an instance of Google Cloud AI Platform Optimizer ParameterSpec.

              See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec

              Args:
                  parameter: Name of teh parameter. The parameter name must be unique amongst all ParameterSpecs.
                  min_value: Minimum value of the parameter.
                  max_value: Maximum value of the parameter.
                  scale_type: The type of scaling that should be applied to this parameter.
                      SCALE_TYPE_UNSPECIFIED By default, no scaling is applied.
                      UNIT_LINEAR_SCALE Scales the feasible space to (0, 1) linearly.
                      UNIT_LOG_SCALE Scales the feasible space logarithmically to (0, 1). The entire feasible space must be strictly positive.
                      UNIT_REVERSE_LOG_SCALE Scales the feasible space "reverse" logarithmically to (0, 1). The result is that values close to the top of the feasible space are spread out more than points near the bottom. The entire feasible space must be strictly positive.
              '''

              parameter_spec_dict = {
                  "parameter": parameter,
                  "type": "INTEGER",
                  "integerValueSpec": {
                      # Representing integers as strings: https://developers.google.com/discovery/v1/type-format
                      "minValue": str(min_value),
                      "maxValue": str(max_value),
                  },
              }
              if scale_type:
                  parameter_spec_dict["scaleType"] = scale_type

              return (parameter_spec_dict, )

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build integer parameter spec for Google Cloud AI Platform Optimizer', description='Builds an instance of Google Cloud AI Platform Optimizer ParameterSpec.')
          _parser.add_argument("--parameter", dest="parameter", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--min-value", dest="min_value", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--max-value", dest="max_value", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--scale-type", dest="scale_type", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_integer_parameter_spec_for_Google_Cloud_AI_Platform_Optimizer(**_parsed_args)

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --parameter
        - {inputValue: parameter}
        - if:
            cond: {isPresent: min_value}
            then:
            - --min-value
            - {inputValue: min_value}
        - if:
            cond: {isPresent: max_value}
            then:
            - --max-value
            - {inputValue: max_value}
        - if:
            cond: {isPresent: scale_type}
            then:
            - --scale-type
            - {inputValue: scale_type}
        - '----output-paths'
        - {outputPath: parameter_spec}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/1b87c0bdfde5d7ec039401af8561783432731402/components/google-cloud/Optimizer/Create_study/component.yaml',
    digest: 4d93c328b9a0bc68a7f9bb70700e8a5e7e822de0aa0ae81160544704d01ef5e8}
  annotations:
    GitHub commit:
      sha: 1b87c0bdfde5d7ec039401af8561783432731402
      html_url: https://github.com/Ark-kun/pipeline_components/commit/1b87c0bdfde5d7ec039401af8561783432731402
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/1b87c0bdfde5d7ec039401af8561783432731402
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        message: Components - Google Cloud - Optimizer - Switched to specifying the
          discovery doc using discoveryServiceUrl
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create study in gcp ai platform optimizer
    description: Creates a Google Cloud AI Plaform Optimizer study.
    inputs:
    - {name: study_id, type: String, description: Name of the study.}
    - {name: parameter_specs, type: JsonArray, description: 'List of parameter specs.
        See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec'}
    - {name: optimization_goal, type: String, description: Optimization goal when optimizing
        a single metric. Can be MAXIMIZE (default) or MINIMIZE. Ignored if metric_specs
        list is provided., default: MAXIMIZE, optional: true}
    - {name: metric_specs, type: JsonArray, description: 'List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec',
      optional: true}
    - {name: gcp_project_id, type: String, optional: true}
    - {name: gcp_region, type: String, default: us-central1, optional: true}
    outputs:
    - {name: study_name, type: String}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Create_study/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def create_study_in_gcp_ai_platform_optimizer(
              study_id,
              parameter_specs,
              optimization_goal = 'MAXIMIZE',
              metric_specs = None,
              gcp_project_id = None,
              gcp_region = "us-central1",
          ):
              """Creates a Google Cloud AI Plaform Optimizer study.
              See https://cloud.google.com/ai-platform/optimizer/docs

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  study_id: Name of the study.
                  parameter_specs: List of parameter specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec
                  optimization_goal: Optimization goal when optimizing a single metric. Can be MAXIMIZE (default) or MINIMIZE. Ignored if metric_specs list is provided.
                  metric_specs: List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec
              """

              import logging
              import google.auth

              logging.getLogger().setLevel(logging.INFO)

              # Validating and inferring the arguments
              if not gcp_project_id:
                  _, gcp_project_id = google.auth.default()

              # Building the API client.
              # The main API does not work, so we need to build from the published discovery document.
              def create_caip_optimizer_client(project_id):
                  from googleapiclient import discovery
                  # The discovery is broken. See https://github.com/googleapis/google-api-python-client/issues/1470
                  # return discovery.build("ml", "v1")
                  return discovery.build("ml", "v1", discoveryServiceUrl='https://storage.googleapis.com/caip-optimizer-public/api/ml_public_google_rest_v1.json')

              ml_api = create_caip_optimizer_client(gcp_project_id)

              if not metric_specs:
                  metric_specs=[{
                      'metric': 'metric',
                      'goal': optimization_goal,
                  }]
              study_config = {
                  'algorithm': 'ALGORITHM_UNSPECIFIED',  # Let the service choose the `default` algorithm.
                  'parameters': parameter_specs,
                  'metrics': metric_specs,
              }
              study = {'study_config': study_config}

              create_study_request = ml_api.projects().locations().studies().create(
                  parent=f'projects/{gcp_project_id}/locations/{gcp_region}',
                  studyId=study_id,
                  body=study,
              )
              create_study_response = create_study_request.execute()
              study_name = create_study_response['name']
              return (study_name,)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Create study in gcp ai platform optimizer', description='Creates a Google Cloud AI Plaform Optimizer study.')
          _parser.add_argument("--study-id", dest="study_id", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--parameter-specs", dest="parameter_specs", type=json.loads, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--optimization-goal", dest="optimization_goal", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metric-specs", dest="metric_specs", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_study_in_gcp_ai_platform_optimizer(**_parsed_args)

          _output_serializers = [
              _serialize_str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --study-id
        - {inputValue: study_id}
        - --parameter-specs
        - {inputValue: parameter_specs}
        - if:
            cond: {isPresent: optimization_goal}
            then:
            - --optimization-goal
            - {inputValue: optimization_goal}
        - if:
            cond: {isPresent: metric_specs}
            then:
            - --metric-specs
            - {inputValue: metric_specs}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: study_name}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/1b87c0bdfde5d7ec039401af8561783432731402/components/google-cloud/Optimizer/Suggest_parameter_sets_based_on_measurements/component.yaml',
    digest: 3c81f810d29ed692b1579064afa15202779789a0648a0b365723adb4c6b78860}
  annotations:
    GitHub commit:
      sha: 1b87c0bdfde5d7ec039401af8561783432731402
      html_url: https://github.com/Ark-kun/pipeline_components/commit/1b87c0bdfde5d7ec039401af8561783432731402
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/1b87c0bdfde5d7ec039401af8561783432731402
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        message: Components - Google Cloud - Optimizer - Switched to specifying the
          discovery doc using discoveryServiceUrl
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Suggest parameter sets from measurements using gcp ai platform optimizer
    description: Suggests trials (parameter sets) to evaluate.
    inputs:
    - {name: parameter_specs, type: JsonArray, description: 'List of parameter specs.
        See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec'}
    - {name: metrics_for_parameter_sets, type: JsonArray, description: 'List of parameter
        sets and evaluation metrics for them. Each list item contains "parameters" dict
        and "metrics" dict. Example: {"parameters": {"p1": 1.1, "p2": 2.2}, "metrics":
        {"metric1": 101, "metric2": 102} }'}
    - {name: suggestion_count, type: Integer, description: Number of suggestions to request.}
    - name: maximize
      type: Boolean
      description: Whether to miaximize or minimize when optimizing a single metric.Default
        is to minimize. Ignored if metric_specs list is provided.
      default: "False"
      optional: true
    - {name: metric_specs, type: JsonArray, description: 'List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec',
      optional: true}
    - {name: gcp_project_id, type: String, optional: true}
    - {name: gcp_region, type: String, default: us-central1, optional: true}
    outputs:
    - {name: suggested_parameter_sets, type: JsonArray}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Suggest_parameter_sets_based_on_measurements/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def suggest_parameter_sets_from_measurements_using_gcp_ai_platform_optimizer(
              parameter_specs,
              metrics_for_parameter_sets,
              suggestion_count,
              maximize = False,
              metric_specs = None,
              gcp_project_id = None,
              gcp_region = "us-central1",
          ):
              """Suggests trials (parameter sets) to evaluate.
              See https://cloud.google.com/ai-platform/optimizer/docs

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  parameter_specs: List of parameter specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec
                  metrics_for_parameter_sets: List of parameter sets and evaluation metrics for them. Each list item contains "parameters" dict and "metrics" dict. Example: {"parameters": {"p1": 1.1, "p2": 2.2}, "metrics": {"metric1": 101, "metric2": 102} }
                  maximize: Whether to miaximize or minimize when optimizing a single metric.Default is to minimize. Ignored if metric_specs list is provided.
                  metric_specs: List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec
                  suggestion_count: Number of suggestions to request.

                  suggested_parameter_sets: List of parameter set dictionaries.
              """

              import logging
              import random
              import time

              import google.auth
              from googleapiclient import discovery

              logging.getLogger().setLevel(logging.INFO)

              client_id = 'client1'

              credentials, default_project_id = google.auth.default()

              # Validating and inferring the arguments
              if not gcp_project_id:
                  gcp_project_id = default_project_id

              # Building the API client.
              # The main API does not work, so we need to build from the published discovery document.
              def create_caip_optimizer_client(project_id):
                  from googleapiclient import discovery
                  # The discovery is broken. See https://github.com/googleapis/google-api-python-client/issues/1470
                  # return discovery.build("ml", "v1")
                  return discovery.build("ml", "v1", discoveryServiceUrl='https://storage.googleapis.com/caip-optimizer-public/api/ml_public_google_rest_v1.json')

              # Workaround for the Optimizer bug: Optimizer returns resource names that use project number, but only supports resource names with project IDs when making requests
              def get_project_number(project_id):
                  service = discovery.build('cloudresourcemanager', 'v1', credentials=credentials)
                  response = service.projects().get(projectId=project_id).execute()
                  return response['projectNumber']

              gcp_project_number = get_project_number(gcp_project_id)

              def fix_resource_name(name):
                  return name.replace(gcp_project_number, gcp_project_id)

              ml_api = create_caip_optimizer_client(gcp_project_id)
              studies_api = ml_api.projects().locations().studies()
              trials_api = ml_api.projects().locations().studies().trials()
              operations_api = ml_api.projects().locations().operations()

              random_integer = random.SystemRandom().getrandbits(256)
              study_id = '{:064x}'.format(random_integer)

              if not metric_specs:
                  metric_specs=[{
                      'metric': 'metric',
                      'goal': 'MAXIMIZE' if maximize else 'MINIMIZE',
                  }]
              study_config = {
                  'algorithm': 'ALGORITHM_UNSPECIFIED',  # Let the service choose the `default` algorithm.
                  'parameters': parameter_specs,
                  'metrics': metric_specs,
              }
              study = {'study_config': study_config}

              logging.info(f'Creating temporary study {study_id}')
              create_study_request = studies_api.create(
                  parent=f'projects/{gcp_project_id}/locations/{gcp_region}',
                  studyId=study_id,
                  body=study,
              )
              create_study_response = create_study_request.execute()
              study_name = create_study_response['name']

              paremeter_type_names = {parameter_spec['parameter']: parameter_spec['type'] for parameter_spec in parameter_specs}
              def parameter_name_and_value_to_dict(parameter_name, parameter_value):
                  result = {'parameter': parameter_name}
                  paremeter_type_name = paremeter_type_names[parameter_name]
                  if paremeter_type_name in ['DOUBLE', 'DISCRETE']:
                      result['floatValue'] = parameter_value
                  elif paremeter_type_name == 'INTEGER':
                      result['intValue'] = parameter_value
                  elif paremeter_type_name == 'CATEGORICAL':
                      result['stringValue'] = parameter_value
                  else:
                      raise TypeError(f'Unsupported parameter type "{paremeter_type_name}"')
                  return result

              try:
                  logging.info(f'Adding {len(metrics_for_parameter_sets)} measurements to the study.')
                  for parameters_and_metrics in metrics_for_parameter_sets:
                      parameter_set = parameters_and_metrics['parameters']
                      metrics_set = parameters_and_metrics['metrics']
                      trial = {
                          'parameters': [
                              parameter_name_and_value_to_dict(parameter_name, parameter_value)
                              for parameter_name, parameter_value in parameter_set.items()
                          ],
                          'finalMeasurement': {
                              'metrics': [
                                  {
                                      'metric': metric_name,
                                      'value': metric_value,
                                  }
                                  for metric_name, metric_value in metrics_set.items()
                              ],
                          },
                          'state': 'COMPLETED',
                      }
                      create_trial_response = trials_api.create(
                          parent=fix_resource_name(study_name),
                          body=trial,
                      ).execute()
                      trial_name = create_trial_response["name"]
                      logging.info(f'Added trial "{trial_name}" to the study.')

                  logging.info(f'Requesting suggestions.')
                  suggest_trials_request = trials_api.suggest(
                      parent=fix_resource_name(study_name),
                      body=dict(
                          suggestionCount=suggestion_count,
                          clientId=client_id,
                      ),
                  )
                  suggest_trials_response = suggest_trials_request.execute()
                  operation_name = suggest_trials_response['name']
                  while True:
                      get_operation_response = operations_api.get(
                          name=fix_resource_name(operation_name),
                      ).execute()
                      # Knowledge: The "done" key is just missing until the result is available
                      if get_operation_response.get('done'):
                          break
                      logging.info('Operation not finished yet: ' + str(get_operation_response))
                      time.sleep(10)
                  operation_response = get_operation_response['response']
                  suggested_trials = operation_response['trials']

                  suggested_parameter_sets = [
                      {
                          parameter['parameter']: parameter.get('floatValue') or parameter.get('intValue') or parameter.get('stringValue') or 0.0
                          for parameter in trial['parameters']
                      }
                      for trial in suggested_trials
                  ]
                  return (suggested_parameter_sets,)
              finally:
                  logging.info(f'Deleting study: "{study_name}"')
                  studies_api.delete(name=fix_resource_name(study_name))

          import json
          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _deserialize_bool(s) -> bool:
              from distutils.util import strtobool
              return strtobool(s) == 1

          import argparse
          _parser = argparse.ArgumentParser(prog='Suggest parameter sets from measurements using gcp ai platform optimizer', description='Suggests trials (parameter sets) to evaluate.')
          _parser.add_argument("--parameter-specs", dest="parameter_specs", type=json.loads, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--metrics-for-parameter-sets", dest="metrics_for_parameter_sets", type=json.loads, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--suggestion-count", dest="suggestion_count", type=int, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--maximize", dest="maximize", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metric-specs", dest="metric_specs", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = suggest_parameter_sets_from_measurements_using_gcp_ai_platform_optimizer(**_parsed_args)

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --parameter-specs
        - {inputValue: parameter_specs}
        - --metrics-for-parameter-sets
        - {inputValue: metrics_for_parameter_sets}
        - --suggestion-count
        - {inputValue: suggestion_count}
        - if:
            cond: {isPresent: maximize}
            then:
            - --maximize
            - {inputValue: maximize}
        - if:
            cond: {isPresent: metric_specs}
            then:
            - --metric-specs
            - {inputValue: metric_specs}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: suggested_parameter_sets}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/1b87c0bdfde5d7ec039401af8561783432731402/components/google-cloud/Optimizer/Suggest_trials/component.yaml',
    digest: 1388f0dcaa1c59bb5c17512524f756f0a9665a8619c283d3dd1dd469e789504f}
  annotations:
    GitHub commit:
      sha: 1b87c0bdfde5d7ec039401af8561783432731402
      html_url: https://github.com/Ark-kun/pipeline_components/commit/1b87c0bdfde5d7ec039401af8561783432731402
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/1b87c0bdfde5d7ec039401af8561783432731402
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T01:12:13Z'}
        message: Components - Google Cloud - Optimizer - Switched to specifying the
          discovery doc using discoveryServiceUrl
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Suggest trials in gcp ai platform optimizer
    description: Suggests trials (parameter sets) to evaluate.
    inputs:
    - {name: study_name, type: String, description: Full resource name of the study.}
    - {name: suggestion_count, type: Integer, description: Number of suggestions to request.}
    - {name: gcp_project_id, type: String, optional: true}
    - {name: gcp_region, type: String, default: us-central1, optional: true}
    outputs:
    - {name: suggested_trials, type: JsonArray}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Suggest_trials/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-api-python-client==1.12.3' 'google-auth==1.21.3'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def suggest_trials_in_gcp_ai_platform_optimizer(
              study_name,
              suggestion_count,
              gcp_project_id = None,
              gcp_region = "us-central1",
          ):
              """Suggests trials (parameter sets) to evaluate.
              See https://cloud.google.com/ai-platform/optimizer/docs

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  study_name: Full resource name of the study.
                  suggestion_count: Number of suggestions to request.
              """

              import logging
              import time

              import google.auth
              from googleapiclient import discovery

              logging.getLogger().setLevel(logging.INFO)

              client_id = 'client1'

              credentials, default_project_id = google.auth.default()

              # Validating and inferring the arguments
              if not gcp_project_id:
                  gcp_project_id = default_project_id

              # Building the API client.
              # The main API does not work, so we need to build from the published discovery document.
              def create_caip_optimizer_client(project_id):
                  from googleapiclient import discovery
                  # The discovery is broken. See https://github.com/googleapis/google-api-python-client/issues/1470
                  # return discovery.build("ml", "v1")
                  return discovery.build("ml", "v1", discoveryServiceUrl='https://storage.googleapis.com/caip-optimizer-public/api/ml_public_google_rest_v1.json')

              # Workaround for the Optimizer bug: Optimizer returns resource names that use project number, but only supports resource names with project IDs when making requests
              def get_project_number(project_id):
                  service = discovery.build('cloudresourcemanager', 'v1', credentials=credentials)
                  response = service.projects().get(projectId=project_id).execute()
                  return response['projectNumber']

              gcp_project_number = get_project_number(gcp_project_id)

              def fix_resource_name(name):
                  return name.replace(gcp_project_number, gcp_project_id)

              ml_api = create_caip_optimizer_client(gcp_project_id)
              trials_api = ml_api.projects().locations().studies().trials()
              operations_api = ml_api.projects().locations().operations()

              suggest_trials_request = trials_api.suggest(
                  parent=fix_resource_name(study_name),
                  body=dict(
                      suggestionCount=suggestion_count,
                      clientId=client_id,
                  ),
              )
              suggest_trials_response = suggest_trials_request.execute()
              operation_name = suggest_trials_response['name']
              while True:
                  get_operation_response = operations_api.get(
                      name=fix_resource_name(operation_name),
                  ).execute()
                  # Knowledge: The "done" key is just missing until the result is available
                  if get_operation_response.get('done'):
                      break
                  logging.info('Not finished yet: ' + str(get_operation_response))
                  time.sleep(10)
              operation_response = get_operation_response['response']
              suggested_trials = operation_response['trials']
              return (suggested_trials,)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Suggest trials in gcp ai platform optimizer', description='Suggests trials (parameter sets) to evaluate.')
          _parser.add_argument("--study-name", dest="study_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--suggestion-count", dest="suggestion_count", type=int, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = suggest_trials_in_gcp_ai_platform_optimizer(**_parsed_args)

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --study-name
        - {inputValue: study_name}
        - --suggestion-count
        - {inputValue: suggestion_count}
        - if:
            cond: {isPresent: gcp_project_id}
            then:
            - --gcp-project-id
            - {inputValue: gcp_project_id}
        - if:
            cond: {isPresent: gcp_region}
            then:
            - --gcp-region
            - {inputValue: gcp_region}
        - '----output-paths'
        - {outputPath: suggested_trials}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml',
    digest: 30c424ac6156c478aa0c3027b470baf9cb7dbbf90aebcabde7469bfbd02a512e}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Download from GCS
    inputs:
    - {name: GCS path, type: URI}
    outputs:
    - {name: Data}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download/component.yaml'
    implementation:
        container:
            image: google/cloud-sdk
            command:
            - bash # Pattern comparison only works in Bash
            - -ex
            - -c
            - |
                if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                    gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                fi

                uri="$0"
                output_path="$1"

                # Checking whether the URI points to a single blob, a directory or a URI pattern
                # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
                if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
                    mkdir -p "$(dirname "$output_path")"
                    gsutil -m cp -r "$uri" "$output_path"
                else
                    mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
                    gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
                fi
            - inputValue: GCS path
            - outputPath: Data
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download_blob/component.yaml',
    digest: 61aa717a3eeda5854d69a2823a264dd1d85f347ea2c6335a7e803ec09d4a6f03}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Download from GCS
    inputs:
    - {name: GCS path, type: URI}
    outputs:
    - {name: Data}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download_blob/component.yaml'
    implementation:
        container:
            image: google/cloud-sdk
            command:
            - sh
            - -ex
            - -c
            - |
                if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                    gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                fi
                mkdir -p "$(dirname "$1")"
                gsutil -m cp -r "$0" "$1"
            - inputValue: GCS path
            - outputPath: Data
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download_dir/component.yaml',
    digest: d2e825d903a6c47d5af8613fa1c910331db662eb1665a6c54d13fa49d7af0ba4}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Download from GCS
    inputs:
    - {name: GCS path, type: URI}
    outputs:
    - {name: Data}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download_dir/component.yaml'
    implementation:
        container:
            image: google/cloud-sdk
            command:
            - sh
            - -ex
            - -c
            - |
                if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                    gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                fi
                mkdir -p "$1"
                gsutil -m cp -r "$0" "$1"
            - inputValue: GCS path
            - outputPath: Data
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/list/component.yaml',
    digest: 8283b7ce8e2ffafe453833f8e1695f43243ae19547b1c84c71f4f32b2f4c780b}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: List blobs
    inputs:
    - {name: GCS path, type: URI, description: 'GCS path for listing. For recursive listing use the "gs://bucket/path/**" syntax".'}
    outputs:
    - {name: Paths}
    metadata:
        annotations:
            author: Alexey Volkov <alexey.volkov@ark-kun.com>
            canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/list/component.yaml'
            volatile_component: 'true'
    implementation:
        container:
            image: google/cloud-sdk
            command:
            - sh
            - -ex
            - -c
            - |
                if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                    gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                fi
                mkdir -p "$(dirname "$1")"
                gsutil ls "$0" > "$1"
            - inputValue: GCS path
            - outputPath: Paths
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml',
    digest: e3cfa607ab9e2e0312ef4268dad157edf30c3d4ba5059b2e295fe047258aa31d}
  annotations:
    GitHub commit:
      sha: 6210648f30b2b3a8c01cc10be338da98300efb6b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/6210648f30b2b3a8c01cc10be338da98300efb6b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/6210648f30b2b3a8c01cc10be338da98300efb6b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        message: Switched from echo to printf to avoid trailing newlines and possible
          data mangling
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Upload to GCS
    inputs:
    - {name: Data}
    - {name: GCS path, type: URI}
    outputs:
    - {name: GCS path, type: String}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'
    implementation:
        container:
            image: google/cloud-sdk
            command:
            - sh
            - -ex
            - -c
            - |
                if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                    gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                fi
                gsutil cp -r "$0" "$1"
                mkdir -p "$(dirname "$2")"
                printf "%s" "$1" > "$2"
            - inputPath: Data
            - inputValue: GCS path
            - outputPath: GCS path
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml',
    digest: 074b5c68679117f5aeb41e36ee75b8c349ec88631ba552cbf6991bd5942d9192}
  annotations:
    GitHub commit:
      sha: 6210648f30b2b3a8c01cc10be338da98300efb6b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/6210648f30b2b3a8c01cc10be338da98300efb6b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/6210648f30b2b3a8c01cc10be338da98300efb6b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        message: Switched from echo to printf to avoid trailing newlines and possible
          data mangling
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |-
    name: Upload to GCS with unique name
    description: Upload to GCS with unique URI suffix
    inputs:
    - {name: Data}
    - {name: GCS path prefix, type: URI}
    outputs:
    - {name: GCS path, type: String}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_unique_uri/component.yaml'
    implementation:
        container:
            image: google/cloud-sdk
            command:
            - sh
            - -ex
            - -c
            - |
                data_path="$0"
                url_prefix="$1"
                output_path="$2"
                random_string=$(< dev/urandom tr -dc A-Za-z0-9 | head -c 64)
                uri="${url_prefix}${random_string}"
                if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
                    gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
                fi
                gsutil cp -r "$data_path" "$uri"
                mkdir -p "$(dirname "$output_path")"
                printf "%s" "$uri" > "$output_path"
            - inputPath: Data
            - {inputValue: GCS path prefix}
            - outputPath: GCS path
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/44b0543525ab6149ce995a411f88997e7131a53d/components/google-cloud/Vertex_AI/AutoML/Tables/_samples/VertexAI.AutoML.Tables.pipeline.component.yaml',
    digest: 491a7b708f83380188a0e3bc9ca8c49fe66831a5981b87dbf434eef4b0606abe}
  annotations:
    GitHub commit:
      sha: 44b0543525ab6149ce995a411f88997e7131a53d
      html_url: https://github.com/Ark-kun/pipeline_components/commit/44b0543525ab6149ce995a411f88997e7131a53d
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/44b0543525ab6149ce995a411f88997e7131a53d
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-15T11:42:41Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-15T11:42:41Z'}
        message: Google Cloud - Vertex AI - AutoML - Tables - Added sample pipeline
          (in component.yaml form)
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |-
    name: Vertex AI AutoML Tables pipeline
    metadata:
      annotations:
        sdk: 'https://cloud-pipelines.net/pipeline-editor/'
    inputs: []
    implementation:
      graph:
        tasks:
          Chicago Taxi Trips dataset:
            componentRef:
              url: 'https://raw.githubusercontent.com/Ark-kun/pipelines/2463ecda532517462590d75e6e14a8af6b55869a/components/datasets/Chicago_Taxi_Trips/component.yaml'
            annotations:
              editor.position: '{"x":60,"y":100,"width":180,"height":40}'
            arguments:
              Select: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras'
          Create tabular dataset from CSV for Google Cloud Vertex AI:
            componentRef:
              url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_CSV/component.yaml'
            annotations:
              editor.position: '{"x":60,"y":220,"width":180,"height":70}'
            arguments:
              data:
                taskOutput:
                  taskId: Chicago Taxi Trips dataset
                  outputName: Table
          Train tabular model using Google Cloud Vertex AI AutoML:
            componentRef:
              url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml'
            annotations:
              editor.position: '{"x":60,"y":380,"width":180,"height":70}'
            arguments:
              target_column: tips
              optimization_prediction_type: regression
              dataset_name:
                taskOutput:
                  taskId: Create tabular dataset from CSV for Google Cloud Vertex AI
                  outputName: dataset_name
          Deploy model to endpoint for Google Cloud Vertex AI Model:
            componentRef:
              url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/b2cdd60fe93d609111729ef64e79a8b8a2713435/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml'
            annotations:
              editor.position: '{"x":60,"y":530,"width":180,"height":70}'
            arguments:
              model_name:
                taskOutput:
                  taskId: Train tabular model using Google Cloud Vertex AI AutoML
                  outputName: model_name
          Get model tuning trials for Google Cloud Vertex AI AutoML Tables:
            componentRef:
              url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml'
            annotations:
              editor.position: '{"x":270,"y":530,"width":180,"height":70}'
            arguments:
              model_name:
                taskOutput:
                  taskId: Train tabular model using Google Cloud Vertex AI AutoML
                  outputName: model_name
        outputValues: {}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml',
    digest: d2a9d89dbdd18292df8bcea8b9a8dcf570be3fd984f13d70727bbf2463d83592}
  annotations:
    GitHub commit:
      sha: 00d020c29a144cee7fd35f2d05053addb942f536
      html_url: https://github.com/Ark-kun/pipeline_components/commit/00d020c29a144cee7fd35f2d05053addb942f536
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/00d020c29a144cee7fd35f2d05053addb942f536
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:38:19Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:38:19Z'}
        message: |-
          fix: Google Cloud - Vertex AI - AutoML - Tables - Create_dataset - Added project inference for the from_GCS and from_BigQuery components

          Added project inference that works when running under Vertex AI Pipelines.
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create tabular dataset from BigQuery for Google Cloud Vertex AI
    description: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in
      GCS.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml'}
    inputs:
    - name: data_uri
      type: GoogleCloudBigQueryUri
      description: |-
        Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.
        The bucket must be a regional bucket in the us-central1 region.
        The file name must have a (case-insensitive) '.CSV' file extension.
    - name: display_name
      type: String
      description: |-
        Display name for the AutoML Dataset.
        Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
      optional: true
    - name: encryption_spec_key_name
      type: String
      description: |-
        Optional. The Cloud KMS resource identifier of the customer
        managed encryption key used to protect a resource. Has the
        form:
        ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
        The key needs to be in the same region as where the compute
        resource is created.
      optional: true
    - {name: project, type: String, description: 'Google Cloud project ID. If not set,
        the default one will be used.', optional: true}
    - {name: location, type: String, description: Google Cloud region. AutoML Tables only
        supports us-central1., default: us-central1, optional: true}
    outputs:
    - {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}
    - {name: dataset_dict, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'
          --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_tabular_dataset_from_BigQuery_for_Google_Cloud_Vertex_AI(
              data_uri,
              display_name = None,
              encryption_spec_key_name = None,
              project = None,
              location = 'us-central1',
          ):
              '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  data_uri: Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.
                      The bucket must be a regional bucket in the us-central1 region.
                      The file name must have a (case-insensitive) '.CSV' file extension.
                  display_name: Display name for the AutoML Dataset.
                      Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                  encryption_spec_key_name (Optional[str]):
                      Optional. The Cloud KMS resource identifier of the customer
                      managed encryption key used to protect a resource. Has the
                      form:
                      ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
                      The key needs to be in the same region as where the compute
                      resource is created.
                  project: Google Cloud project ID. If not set, the default one will be used.
                  location: Google Cloud region. AutoML Tables only supports us-central1.
              Returns:
                  dataset_name: Dataset name (fully-qualified)
                  dataset_dict: Dataset object in JSON format
              '''

              import datetime
              import json
              import logging
              import os

              from google.cloud import aiplatform
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              if not display_name:
                  display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
              # This leads to failure when trying to create any resource in the project.
              # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
              # We can try and get the GCP project ID/number from the environment variables.
              if not project:
                  project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                  if project_number:
                      print(f"Inferred project number: {project_number}")
                      project = project_number

              if not location:
                  location = os.environ.get("CLOUD_ML_REGION")

              aiplatform.init(
                  project=project,
                  location=location,
                  encryption_spec_key_name=encryption_spec_key_name,
              )
              dataset = aiplatform.TabularDataset.create(
                  display_name=display_name,
                  bq_source=data_uri,
              )
              (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')
              dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'
              logging.info(f'Created dataset {dataset.name}.')
              logging.info(f'Link: {dataset_web_url}')
              dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)
              print(dataset_json)
              return (dataset.resource_name, dataset_json, dataset_web_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Create tabular dataset from BigQuery for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')
          _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_tabular_dataset_from_BigQuery_for_Google_Cloud_Vertex_AI(**_parsed_args)

          _output_serializers = [
              str,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data-uri
        - {inputValue: data_uri}
        - if:
            cond: {isPresent: display_name}
            then:
            - --display-name
            - {inputValue: display_name}
        - if:
            cond: {isPresent: encryption_spec_key_name}
            then:
            - --encryption-spec-key-name
            - {inputValue: encryption_spec_key_name}
        - if:
            cond: {isPresent: project}
            then:
            - --project
            - {inputValue: project}
        - if:
            cond: {isPresent: location}
            then:
            - --location
            - {inputValue: location}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset_dict}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml',
    digest: 1dd993468cad985c6bcaa40e8f8ccd685e579d4ed0d6d900e43cce2b4f762c9f}
  annotations:
    GitHub commit:
      sha: 00d020c29a144cee7fd35f2d05053addb942f536
      html_url: https://github.com/Ark-kun/pipeline_components/commit/00d020c29a144cee7fd35f2d05053addb942f536
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/00d020c29a144cee7fd35f2d05053addb942f536
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:38:19Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:38:19Z'}
        message: |-
          fix: Google Cloud - Vertex AI - AutoML - Tables - Create_dataset - Added project inference for the from_GCS and from_BigQuery components

          Added project inference that works when running under Vertex AI Pipelines.
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create tabular dataset from GCS for Google Cloud Vertex AI
    description: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in
      GCS.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}
    inputs:
    - name: data_uri
      type: GoogleCloudStorageUri
      description: |-
        Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
        The bucket must be a regional bucket in the us-central1 region.
        The file name must have a (case-insensitive) '.CSV' file extension.
    - name: display_name
      type: String
      description: |-
        Display name for the AutoML Dataset.
        Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
      optional: true
    - name: encryption_spec_key_name
      type: String
      description: |-
        Optional. The Cloud KMS resource identifier of the customer
        managed encryption key used to protect a resource. Has the
        form:
        ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
        The key needs to be in the same region as where the compute
        resource is created.
      optional: true
    - {name: project, type: String, description: 'Google Cloud project ID. If not set,
        the default one will be used.', optional: true}
    - {name: location, type: String, description: Google Cloud region. AutoML Tables only
        supports us-central1., default: us-central1, optional: true}
    outputs:
    - {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}
    - {name: dataset_dict, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'
          --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(
              data_uri,  # data_type: "CSV"
              display_name = None,
              encryption_spec_key_name = None,
              project = None,
              location = 'us-central1',
          ):
              '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  data_uri: Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
                      The bucket must be a regional bucket in the us-central1 region.
                      The file name must have a (case-insensitive) '.CSV' file extension.
                  display_name: Display name for the AutoML Dataset.
                      Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                  encryption_spec_key_name (Optional[str]):
                      Optional. The Cloud KMS resource identifier of the customer
                      managed encryption key used to protect a resource. Has the
                      form:
                      ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
                      The key needs to be in the same region as where the compute
                      resource is created.
                  project: Google Cloud project ID. If not set, the default one will be used.
                  location: Google Cloud region. AutoML Tables only supports us-central1.
              Returns:
                  dataset_name: Dataset name (fully-qualified)
                  dataset_dict: Dataset object in JSON format
              '''

              import datetime
              import json
              import logging
              import os

              from google.cloud import aiplatform
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              if not display_name:
                  display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              # Hack to enable passing multiple URIs
              # I could have created another component or added another input, but it seems to be too much hassle for now.
              # An alternative would have been to accept comma-delimited or semicolon-delimited URLs.
              if data_uri.startswith("["):
                  data_uris = json.loads(data_uri)
              else:
                  data_uris = [data_uri]

              # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
              # This leads to failure when trying to create any resource in the project.
              # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
              # We can try and get the GCP project ID/number from the environment variables.
              if not project:
                  project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                  if project_number:
                      print(f"Inferred project number: {project_number}")
                      project = project_number

              if not location:
                  location = os.environ.get("CLOUD_ML_REGION")

              aiplatform.init(
                  project=project,
                  location=location,
                  encryption_spec_key_name=encryption_spec_key_name,
              )
              dataset = aiplatform.TabularDataset.create(
                  display_name=display_name,
                  gcs_source=data_uris,
              )
              (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')
              dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'
              logging.info(f'Created dataset {dataset.name}.')
              logging.info(f'Link: {dataset_web_url}')
              dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)
              print(dataset_json)
              return (dataset.resource_name, dataset_json, dataset_web_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Create tabular dataset from GCS for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')
          _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(**_parsed_args)

          _output_serializers = [
              str,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data-uri
        - {inputValue: data_uri}
        - if:
            cond: {isPresent: display_name}
            then:
            - --display-name
            - {inputValue: display_name}
        - if:
            cond: {isPresent: encryption_spec_key_name}
            then:
            - --encryption-spec-key-name
            - {inputValue: encryption_spec_key_name}
        - if:
            cond: {isPresent: project}
            then:
            - --project
            - {inputValue: project}
        - if:
            cond: {isPresent: location}
            then:
            - --location
            - {inputValue: location}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset_dict}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml',
    digest: 1dd993468cad985c6bcaa40e8f8ccd685e579d4ed0d6d900e43cce2b4f762c9f}
  annotations:
    GitHub commit:
      sha: 00d020c29a144cee7fd35f2d05053addb942f536
      html_url: https://github.com/Ark-kun/pipeline_components/commit/00d020c29a144cee7fd35f2d05053addb942f536
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/00d020c29a144cee7fd35f2d05053addb942f536
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:38:19Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:38:19Z'}
        message: |-
          fix: Google Cloud - Vertex AI - AutoML - Tables - Create_dataset - Added project inference for the from_GCS and from_BigQuery components

          Added project inference that works when running under Vertex AI Pipelines.
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create tabular dataset from GCS for Google Cloud Vertex AI
    description: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in
      GCS.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}
    inputs:
    - name: data_uri
      type: GoogleCloudStorageUri
      description: |-
        Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
        The bucket must be a regional bucket in the us-central1 region.
        The file name must have a (case-insensitive) '.CSV' file extension.
    - name: display_name
      type: String
      description: |-
        Display name for the AutoML Dataset.
        Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
      optional: true
    - name: encryption_spec_key_name
      type: String
      description: |-
        Optional. The Cloud KMS resource identifier of the customer
        managed encryption key used to protect a resource. Has the
        form:
        ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
        The key needs to be in the same region as where the compute
        resource is created.
      optional: true
    - {name: project, type: String, description: 'Google Cloud project ID. If not set,
        the default one will be used.', optional: true}
    - {name: location, type: String, description: Google Cloud region. AutoML Tables only
        supports us-central1., default: us-central1, optional: true}
    outputs:
    - {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}
    - {name: dataset_dict, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'
          --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(
              data_uri,  # data_type: "CSV"
              display_name = None,
              encryption_spec_key_name = None,
              project = None,
              location = 'us-central1',
          ):
              '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  data_uri: Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.
                      The bucket must be a regional bucket in the us-central1 region.
                      The file name must have a (case-insensitive) '.CSV' file extension.
                  display_name: Display name for the AutoML Dataset.
                      Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.
                  encryption_spec_key_name (Optional[str]):
                      Optional. The Cloud KMS resource identifier of the customer
                      managed encryption key used to protect a resource. Has the
                      form:
                      ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.
                      The key needs to be in the same region as where the compute
                      resource is created.
                  project: Google Cloud project ID. If not set, the default one will be used.
                  location: Google Cloud region. AutoML Tables only supports us-central1.
              Returns:
                  dataset_name: Dataset name (fully-qualified)
                  dataset_dict: Dataset object in JSON format
              '''

              import datetime
              import json
              import logging
              import os

              from google.cloud import aiplatform
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              if not display_name:
                  display_name = 'Dataset_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              # Hack to enable passing multiple URIs
              # I could have created another component or added another input, but it seems to be too much hassle for now.
              # An alternative would have been to accept comma-delimited or semicolon-delimited URLs.
              if data_uri.startswith("["):
                  data_uris = json.loads(data_uri)
              else:
                  data_uris = [data_uri]

              # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
              # This leads to failure when trying to create any resource in the project.
              # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
              # We can try and get the GCP project ID/number from the environment variables.
              if not project:
                  project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                  if project_number:
                      print(f"Inferred project number: {project_number}")
                      project = project_number

              if not location:
                  location = os.environ.get("CLOUD_ML_REGION")

              aiplatform.init(
                  project=project,
                  location=location,
                  encryption_spec_key_name=encryption_spec_key_name,
              )
              dataset = aiplatform.TabularDataset.create(
                  display_name=display_name,
                  gcs_source=data_uris,
              )
              (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')
              dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'
              logging.info(f'Created dataset {dataset.name}.')
              logging.info(f'Link: {dataset_web_url}')
              dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)
              print(dataset_json)
              return (dataset.resource_name, dataset_json, dataset_web_url)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Create tabular dataset from GCS for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')
          _parser.add_argument("--data-uri", dest="data_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(**_parsed_args)

          _output_serializers = [
              str,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --data-uri
        - {inputValue: data_uri}
        - if:
            cond: {isPresent: display_name}
            then:
            - --display-name
            - {inputValue: display_name}
        - if:
            cond: {isPresent: encryption_spec_key_name}
            then:
            - --encryption-spec-key-name
            - {inputValue: encryption_spec_key_name}
        - if:
            cond: {isPresent: project}
            then:
            - --project
            - {inputValue: project}
        - if:
            cond: {isPresent: location}
            then:
            - --location
            - {inputValue: location}
        - '----output-paths'
        - {outputPath: dataset_name}
        - {outputPath: dataset_dict}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/a31b7c9652646f2cd035a0b3a23e0723c632521b/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml',
    digest: ae740e7eee8da0a5ef27268370d03baaf86d2d10ef8d3640fbfda6604cf309f1}
  annotations:
    GitHub commit:
      sha: a31b7c9652646f2cd035a0b3a23e0723c632521b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/a31b7c9652646f2cd035a0b3a23e0723c632521b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/a31b7c9652646f2cd035a0b3a23e0723c632521b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T09:35:54Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T09:35:54Z'}
        message: Google Cloud - Vertex AI - AutoML - Tables - Get_model_tuning_trials
          - Added component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Get model tuning trials for Google Cloud Vertex AI AutoML Tables
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml'}
    inputs:
    - {name: model_name, type: GoogleCloudVertexAiModelName}
    outputs:
    - {name: tuning_trials, type: JsonArray}
    - {name: model_structures, type: JsonArray}
    - {name: extra_entries, type: JsonArray}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-logging==2.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
          pip install --quiet --no-warn-script-location 'google-cloud-logging==2.7.0'
          --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def get_model_tuning_trials_for_Google_Cloud_Vertex_AI_AutoML_Tables(
              model_name,
          ):
              import json
              from google.cloud import logging as cloud_logging

              (_, project, _, location, _, model_id) = model_name.split("/")

              # Need to specify project when initializing client.
              # Otherwise we'll get error when running on Vertex AI Pipelines:
              # google.api_core.exceptions.PermissionDenied: 403 The caller does not have permission
              cloud_logging_client = cloud_logging.Client(project=project)

              # Full filter:
              # resource.type="cloudml_job" resource.labels.job_id="{job_id}" resource.labels.project_id="{project_id}" labels.log_type="automl_tables" jsonPayload."@type"="type.googleapis.com/google.cloud.automl.master.TuningTrial"
              log_filter=f'resource.labels.job_id="{model_id}"'
              log_entry_list = list(cloud_logging_client.list_entries(filter_=log_filter))

              tuning_trials = []
              model_structures = []
              extra_entries = []
              for entry in log_entry_list:
                  if entry.payload.get("@type") == "type.googleapis.com/google.cloud.automl.master.TuningTrial":
                      tuning_trials.append(entry.payload)
                  elif entry.payload.get("@type") == "type.googleapis.com/google.cloud.automl.master.TablesModelStructure":
                      model_structures.append(entry.payload)
                  else:
                      extra_entries.append(entry.payload)

              # Manually serializing the results for pretty and stable output
              print("Tuning trials:")
              tuning_trials_json = json.dumps(tuning_trials, sort_keys=True, indent=2)
              print(tuning_trials_json)

              print("Model structures:")
              model_structures_json = json.dumps(model_structures, sort_keys=True, indent=2)
              print(model_structures_json)

              print("Extra entries:")
              extra_entries_json = json.dumps(extra_entries, sort_keys=True, indent=2)
              print(extra_entries_json)

              return (tuning_trials_json, model_structures_json, extra_entries_json)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Get model tuning trials for Google Cloud Vertex AI AutoML Tables', description='')
          _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = get_model_tuning_trials_for_Google_Cloud_Vertex_AI_AutoML_Tables(**_parsed_args)

          _output_serializers = [
              _serialize_json,
              _serialize_json,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --model-name
        - {inputValue: model_name}
        - '----output-paths'
        - {outputPath: tuning_trials}
        - {outputPath: model_structures}
        - {outputPath: extra_entries}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/ab85ecc9c30d4d68a2993ca87861f5e531a4f41b/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml',
    digest: d55cf5568f1ed1ddbbf4a3837e77c555e46fb247f11bd21a30c5d2418366b36e}
  annotations:
    GitHub commit:
      sha: ab85ecc9c30d4d68a2993ca87861f5e531a4f41b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/ab85ecc9c30d4d68a2993ca87861f5e531a4f41b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/ab85ecc9c30d4d68a2993ca87861f5e531a4f41b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T01:23:58Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T01:23:58Z'}
        message: |-
          fix: Google Cloud - Vertex AI - AutoML - Train model - Added project inference

          Added project inference that works when running under Vertex AI Pipelines.
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Train tabular model using Google Cloud Vertex AI AutoML
    description: Trains model using Google Cloud Vertex AI AutoML.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml'}
    inputs:
    - name: dataset_name
      type: GoogleCloudVertexAiTabularDatasetName
      description: |-
        Required. The full name of dataset  (datasets.TabularDataset) within the same Project from which data will be used to train the Model. The
        Dataset must use schema compatible with Model being trained,
        and what is compatible should be described in the used
        TrainingPipeline's [training_task_definition]
        [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
        For tabular Datasets, all their data is exported to
        training, to pick and choose from.
    - {name: target_column, type: String, description: Required. The name of the column
        values of which the Model is to predict.}
    - name: optimization_prediction_type
      type: String
      description: |-
        The type of prediction the Model is to produce.
        "classification" - Predict one out of multiple target values is
        picked for each row.
        "regression" - Predict a value based on its relation to other values.
        This type is available only to columns that contain
        semantically numeric values, i.e. integers or floating
        point number, even if stored as e.g. strings.
    - name: training_fraction_split
      type: Float
      description: |-
        Required. The fraction of the input data that is to be
        used to train the Model. This is ignored if Dataset is not provided.
      default: '0.8'
      optional: true
    - name: validation_fraction_split
      type: Float
      description: |-
        Required. The fraction of the input data that is to be
        used to validate the Model. This is ignored if Dataset is not provided.
      default: '0.1'
      optional: true
    - name: test_fraction_split
      type: Float
      description: |-
        Required. The fraction of the input data that is to be
        used to evaluate the Model. This is ignored if Dataset is not provided.
      default: '0.1'
      optional: true
    - name: predefined_split_column_name
      type: String
      description: |-
        Optional. The key is a name of one of the Dataset's data
        columns. The value of the key (either the label's value or
        value in the column) must be one of {``training``,
        ``validation``, ``test``}, and it defines to which set the
        given piece of data is assigned. If for a piece of data the
        key is not present or has an invalid value, that piece is
        ignored by the pipeline.

        Supported only for tabular and time series Datasets.
      optional: true
    - name: weight_column
      type: String
      description: |-
        Optional. Name of the column that should be used as the weight column.
        Higher values in this column give more importance to the row
        during Model training. The column must have numeric values between 0 and
        10000 inclusively, and 0 value means that the row is ignored.
        If the weight column field is not set, then all rows are assumed to have
        equal weight of 1.
      optional: true
    - name: budget_milli_node_hours
      type: Integer
      description: |-
        Optional. The train budget of creating this Model, expressed in milli node
        hours i.e. 1,000 value in this field means 1 node hour.
        The training cost of the model will not exceed this budget. The final
        cost will be attempted to be close to the budget, though may end up
        being (even) noticeably smaller - at the backend's discretion. This
        especially may happen when further model training ceases to provide
        any improvements.
        If the budget is set to a value known to be insufficient to train a
        Model for the given training set, the training won't be attempted and
        will error.
        The minimum value is 1000 and the maximum is 72000.
      default: '1000'
      optional: true
    - name: model_display_name
      type: String
      description: |-
        Optional. If the script produces a managed Vertex AI Model. The display name of
        the Model. The name can be up to 128 characters long and can be consist
        of any UTF-8 characters.

        If not provided upon creation, the job's display_name is used.
      optional: true
    - name: disable_early_stopping
      type: Boolean
      description: |-
        Required. If true, the entire budget is used. This disables the early stopping
        feature. By default, the early stopping feature is enabled, which means
        that training might stop before the entire training budget has been
        used, if further training does no longer brings significant improvement
        to the model.
      default: "False"
      optional: true
    - name: optimization_objective
      type: String
      description: |-
        Optional. Objective function the Model is to be optimized towards. The training
        task creates a Model that maximizes/minimizes the value of the objective
        function over the validation set.

        The supported optimization objectives depend on the prediction type, and
        in the case of classification also the number of distinct values in the
        target column (two distint values -> binary, 3 or more distinct values
        -> multi class).
        If the field is not set, the default objective function is used.

        Classification (binary):
        "maximize-au-roc" (default) - Maximize the area under the receiver
                                    operating characteristic (ROC) curve.
        "minimize-log-loss" - Minimize log loss.
        "maximize-au-prc" - Maximize the area under the precision-recall curve.
        "maximize-precision-at-recall" - Maximize precision for a specified
                                        recall value.
        "maximize-recall-at-precision" - Maximize recall for a specified
                                        precision value.

        Classification (multi class):
        "minimize-log-loss" (default) - Minimize log loss.

        Regression:
        "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE).
        "minimize-mae" - Minimize mean-absolute error (MAE).
        "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
      optional: true
    - name: optimization_objective_recall_value
      type: Float
      description: |-
        Optional. Required when maximize-precision-at-recall optimizationObjective was
        picked, represents the recall value at which the optimization is done.

        The minimum value is 0 and the maximum is 1.0.
      optional: true
    - name: optimization_objective_precision_value
      type: Float
      description: |-
        Optional. Required when maximize-recall-at-precision optimizationObjective was
        picked, represents the precision value at which the optimization is
        done.

        The minimum value is 0 and the maximum is 1.0.
      optional: true
    - {name: project, type: String, optional: true}
    - {name: location, type: String, default: us-central1, optional: true}
    - {name: encryption_spec_key_name, type: String, optional: true}
    outputs:
    - {name: model_name, type: GoogleCloudVertexAiModelName}
    - {name: model_dict, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-aiplatform==1.6.2' 'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.6.2'
          'google-api-python-client==2.29.0' --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def train_tabular_model_using_Google_Cloud_Vertex_AI_AutoML(
              # AutoMLTabularTrainingJob.run required parameters
              dataset_name,
              target_column,

              # AutoMLTabularTrainingJob.__init__ required parameters
              # display_name: str,
              optimization_prediction_type,

              # AutoMLTabularTrainingJob.run parameters
              training_fraction_split = 0.8,
              validation_fraction_split = 0.1,
              test_fraction_split = 0.1,
              predefined_split_column_name = None,
              weight_column = None,
              budget_milli_node_hours = 1000,
              model_display_name = None,
              disable_early_stopping = False,

              # AutoMLTabularTrainingJob.__init__ parameters
              optimization_objective = None,
              #column_transformations: Union[Dict, List[Dict], NoneType] = None,
              optimization_objective_recall_value = None,
              optimization_objective_precision_value = None,

              project = None,
              location = 'us-central1',
              #training_encryption_spec_key_name: str = None,
              #model_encryption_spec_key_name: str = None,
              encryption_spec_key_name = None,
          ):
              '''Trains model using Google Cloud Vertex AI AutoML.

              Data fraction splits:
              Any of ``training_fraction_split``, ``validation_fraction_split`` and
              ``test_fraction_split`` may optionally be provided, they must sum to up to 1. If
              the provided ones sum to less than 1, the remainder is assigned to sets as
              decided by Vertex AI. If none of the fractions are set, by default roughly 80%
              of data will be used for training, 10% for validation, and 10% for test.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>

              Args:
                  dataset_name:
                      Required. The full name of dataset  (datasets.TabularDataset) within the same Project from which data will be used to train the Model. The
                      Dataset must use schema compatible with Model being trained,
                      and what is compatible should be described in the used
                      TrainingPipeline's [training_task_definition]
                      [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
                      For tabular Datasets, all their data is exported to
                      training, to pick and choose from.
                  target_column (str):
                      Required. The name of the column values of which the Model is to predict.
                  training_fraction_split (float):
                      Required. The fraction of the input data that is to be
                      used to train the Model. This is ignored if Dataset is not provided.
                  validation_fraction_split (float):
                      Required. The fraction of the input data that is to be
                      used to validate the Model. This is ignored if Dataset is not provided.
                  test_fraction_split (float):
                      Required. The fraction of the input data that is to be
                      used to evaluate the Model. This is ignored if Dataset is not provided.
                  predefined_split_column_name (str):
                      Optional. The key is a name of one of the Dataset's data
                      columns. The value of the key (either the label's value or
                      value in the column) must be one of {``training``,
                      ``validation``, ``test``}, and it defines to which set the
                      given piece of data is assigned. If for a piece of data the
                      key is not present or has an invalid value, that piece is
                      ignored by the pipeline.

                      Supported only for tabular and time series Datasets.
                  weight_column (str):
                      Optional. Name of the column that should be used as the weight column.
                      Higher values in this column give more importance to the row
                      during Model training. The column must have numeric values between 0 and
                      10000 inclusively, and 0 value means that the row is ignored.
                      If the weight column field is not set, then all rows are assumed to have
                      equal weight of 1.
                  budget_milli_node_hours (int):
                      Optional. The train budget of creating this Model, expressed in milli node
                      hours i.e. 1,000 value in this field means 1 node hour.
                      The training cost of the model will not exceed this budget. The final
                      cost will be attempted to be close to the budget, though may end up
                      being (even) noticeably smaller - at the backend's discretion. This
                      especially may happen when further model training ceases to provide
                      any improvements.
                      If the budget is set to a value known to be insufficient to train a
                      Model for the given training set, the training won't be attempted and
                      will error.
                      The minimum value is 1000 and the maximum is 72000.
                  model_display_name (str):
                      Optional. If the script produces a managed Vertex AI Model. The display name of
                      the Model. The name can be up to 128 characters long and can be consist
                      of any UTF-8 characters.

                      If not provided upon creation, the job's display_name is used.
                  disable_early_stopping (bool):
                      Required. If true, the entire budget is used. This disables the early stopping
                      feature. By default, the early stopping feature is enabled, which means
                      that training might stop before the entire training budget has been
                      used, if further training does no longer brings significant improvement
                      to the model.

                  optimization_prediction_type (str):
                      The type of prediction the Model is to produce.
                      "classification" - Predict one out of multiple target values is
                      picked for each row.
                      "regression" - Predict a value based on its relation to other values.
                      This type is available only to columns that contain
                      semantically numeric values, i.e. integers or floating
                      point number, even if stored as e.g. strings.
                  optimization_objective (str):
                      Optional. Objective function the Model is to be optimized towards. The training
                      task creates a Model that maximizes/minimizes the value of the objective
                      function over the validation set.

                      The supported optimization objectives depend on the prediction type, and
                      in the case of classification also the number of distinct values in the
                      target column (two distint values -> binary, 3 or more distinct values
                      -> multi class).
                      If the field is not set, the default objective function is used.

                      Classification (binary):
                      "maximize-au-roc" (default) - Maximize the area under the receiver
                                                  operating characteristic (ROC) curve.
                      "minimize-log-loss" - Minimize log loss.
                      "maximize-au-prc" - Maximize the area under the precision-recall curve.
                      "maximize-precision-at-recall" - Maximize precision for a specified
                                                      recall value.
                      "maximize-recall-at-precision" - Maximize recall for a specified
                                                      precision value.

                      Classification (multi class):
                      "minimize-log-loss" (default) - Minimize log loss.

                      Regression:
                      "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE).
                      "minimize-mae" - Minimize mean-absolute error (MAE).
                      "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
                  column_transformations (Optional[Union[Dict, List[Dict]]]):
                      Optional. Transformations to apply to the input columns (i.e. columns other
                      than the targetColumn). Each transformation may produce multiple
                      result values from the column's value, and all are used for training.
                      When creating transformation for BigQuery Struct column, the column
                      should be flattened using "." as the delimiter.
                      If an input column has no transformations on it, such a column is
                      ignored by the training, except for the targetColumn, which should have
                      no transformations defined on.
                  optimization_objective_recall_value (float):
                      Optional. Required when maximize-precision-at-recall optimizationObjective was
                      picked, represents the recall value at which the optimization is done.

                      The minimum value is 0 and the maximum is 1.0.
                  optimization_objective_precision_value (float):
                      Optional. Required when maximize-recall-at-precision optimizationObjective was
                      picked, represents the precision value at which the optimization is
                      done.

                      The minimum value is 0 and the maximum is 1.0.

              Returns:
                  model_name: Model name (fully-qualified)
                  model_dict: Model metadata in JSON format
              '''

              import datetime
              import logging
              import os

              from google.cloud import aiplatform
              from google.protobuf import json_format

              logging.getLogger().setLevel(logging.INFO)

              if not model_display_name:
                  model_display_name = 'TablesModel_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S")

              # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
              # This leads to failure when trying to create any resource in the project.
              # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
              # We can try and get the GCP project ID/number from the environment variables.
              if not project:
                  project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                  if project_number:
                      print(f"Inferred project number: {project_number}")
                      project = project_number
                      # To improve the naming we try to convert the project number into the user project ID.
                      try:
                          from googleapiclient import discovery

                          cloud_resource_manager_service = discovery.build(
                              "cloudresourcemanager", "v3"
                          )
                          project_id = (
                              cloud_resource_manager_service.projects()
                              .get(name=f"projects/{project_number}")
                              .execute()["projectId"]
                          )
                          if project_id:
                              print(f"Inferred project ID: {project_id}")
                              project = project_id
                      except Exception as e:
                          print(e)

              aiplatform.init(
                  project=project,
                  location=location,
                  encryption_spec_key_name=encryption_spec_key_name,
              )

              model = aiplatform.AutoMLTabularTrainingJob(
                  display_name='AutoMLTabularTrainingJob_' + datetime.datetime.utcnow().strftime("%Y_%m_%d_%H_%M_%S"),
                  optimization_prediction_type=optimization_prediction_type,
                  optimization_objective=optimization_objective,
                  #column_transformations=column_transformations,
                  optimization_objective_recall_value=optimization_objective_recall_value,
                  optimization_objective_precision_value=optimization_objective_precision_value,
              ).run(
                  dataset=aiplatform.TabularDataset(dataset_name=dataset_name),
                  target_column=target_column,
                  training_fraction_split=training_fraction_split,
                  validation_fraction_split=validation_fraction_split,
                  test_fraction_split=test_fraction_split,
                  predefined_split_column_name=predefined_split_column_name,
                  weight_column=weight_column,
                  budget_milli_node_hours=budget_milli_node_hours,
                  model_display_name=model_display_name,
                  disable_early_stopping=disable_early_stopping,
              )

              (_, model_project, _, model_location, _, model_id) = model.resource_name.split('/')
              model_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{model_location}/models/{model_id}/evaluate?project={model_project}'
              logging.info(f'Created model {model.name}.')
              logging.info(f'Link: {model_web_url}')
              model_json = json_format.MessageToJson(model._gca_resource._pb)
              print(model_json)
              return (model.resource_name, model_json, model_web_url)

          def _deserialize_bool(s) -> bool:
              from distutils.util import strtobool
              return strtobool(s) == 1

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Train tabular model using Google Cloud Vertex AI AutoML', description='Trains model using Google Cloud Vertex AI AutoML.')
          _parser.add_argument("--dataset-name", dest="dataset_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--target-column", dest="target_column", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--optimization-prediction-type", dest="optimization_prediction_type", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--training-fraction-split", dest="training_fraction_split", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--validation-fraction-split", dest="validation_fraction_split", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--test-fraction-split", dest="test_fraction_split", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--predefined-split-column-name", dest="predefined_split_column_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--weight-column", dest="weight_column", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--budget-milli-node-hours", dest="budget_milli_node_hours", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model-display-name", dest="model_display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--disable-early-stopping", dest="disable_early_stopping", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimization-objective", dest="optimization_objective", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimization-objective-recall-value", dest="optimization_objective_recall_value", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimization-objective-precision-value", dest="optimization_objective_precision_value", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--encryption-spec-key-name", dest="encryption_spec_key_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = train_tabular_model_using_Google_Cloud_Vertex_AI_AutoML(**_parsed_args)

          _output_serializers = [
              str,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --dataset-name
        - {inputValue: dataset_name}
        - --target-column
        - {inputValue: target_column}
        - --optimization-prediction-type
        - {inputValue: optimization_prediction_type}
        - if:
            cond: {isPresent: training_fraction_split}
            then:
            - --training-fraction-split
            - {inputValue: training_fraction_split}
        - if:
            cond: {isPresent: validation_fraction_split}
            then:
            - --validation-fraction-split
            - {inputValue: validation_fraction_split}
        - if:
            cond: {isPresent: test_fraction_split}
            then:
            - --test-fraction-split
            - {inputValue: test_fraction_split}
        - if:
            cond: {isPresent: predefined_split_column_name}
            then:
            - --predefined-split-column-name
            - {inputValue: predefined_split_column_name}
        - if:
            cond: {isPresent: weight_column}
            then:
            - --weight-column
            - {inputValue: weight_column}
        - if:
            cond: {isPresent: budget_milli_node_hours}
            then:
            - --budget-milli-node-hours
            - {inputValue: budget_milli_node_hours}
        - if:
            cond: {isPresent: model_display_name}
            then:
            - --model-display-name
            - {inputValue: model_display_name}
        - if:
            cond: {isPresent: disable_early_stopping}
            then:
            - --disable-early-stopping
            - {inputValue: disable_early_stopping}
        - if:
            cond: {isPresent: optimization_objective}
            then:
            - --optimization-objective
            - {inputValue: optimization_objective}
        - if:
            cond: {isPresent: optimization_objective_recall_value}
            then:
            - --optimization-objective-recall-value
            - {inputValue: optimization_objective_recall_value}
        - if:
            cond: {isPresent: optimization_objective_precision_value}
            then:
            - --optimization-objective-precision-value
            - {inputValue: optimization_objective_precision_value}
        - if:
            cond: {isPresent: project}
            then:
            - --project
            - {inputValue: project}
        - if:
            cond: {isPresent: location}
            then:
            - --location
            - {inputValue: location}
        - if:
            cond: {isPresent: encryption_spec_key_name}
            then:
            - --encryption-spec-key-name
            - {inputValue: encryption_spec_key_name}
        - '----output-paths'
        - {outputPath: model_name}
        - {outputPath: model_dict}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/b2cdd60fe93d609111729ef64e79a8b8a2713435/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml',
    digest: 3d9ecac802bc46de2614d3eee883cfdae58acc3d1be34f867aad2f81e519db12}
  annotations:
    GitHub commit:
      sha: b2cdd60fe93d609111729ef64e79a8b8a2713435
      html_url: https://github.com/Ark-kun/pipeline_components/commit/b2cdd60fe93d609111729ef64e79a8b8a2713435
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/b2cdd60fe93d609111729ef64e79a8b8a2713435
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-15T11:10:53Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-15T11:10:53Z'}
        message: Google Cloud - Vertex AI - Models - Deploy_to_endpoint - Added component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Deploy model to endpoint for Google Cloud Vertex AI Model
    description: Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml'}
    inputs:
    - {name: model_name, type: GoogleCloudVertexAiModelName, description: Full resource
        name of a Google Cloud Vertex AI Model}
    - name: endpoint_name
      type: GoogleCloudVertexAiEndpointName
      description: |-
        Optional. Full name of Google Cloud Vertex Endpoint. A new
        endpoint is created if the name is not passed.
      optional: true
    - name: machine_type
      type: String
      description: |-
        The type of the machine. See the [list of machine types
        supported for prediction
        ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
        Defaults to "n1-standard-2"
      default: n1-standard-2
      optional: true
    - name: min_replica_count
      type: Integer
      description: |-
        Optional. The minimum number of machine replicas this deployed
        model will be always deployed on. If traffic against it increases,
        it may dynamically be deployed onto more replicas, and as traffic
        decreases, some of these extra replicas may be freed.
      default: '1'
      optional: true
    - name: max_replica_count
      type: Integer
      description: |-
        Optional. The maximum number of replicas this deployed model may
        be deployed on when the traffic against it increases. If requested
        value is too large, the deployment will error, but if deployment
        succeeds then the ability to scale the model to that many replicas
        is guaranteed (barring service outages). If traffic against the
        deployed model increases beyond what its replicas at maximum may
        handle, a portion of the traffic will be dropped. If this value
        is not provided, the smaller value of min_replica_count or 1 will
        be used.
      default: '1'
      optional: true
    - name: accelerator_type
      type: String
      description: |-
        Optional. Hardware accelerator type. Must also set accelerator_count if used.
        One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,
        NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
      optional: true
    - {name: accelerator_count, type: Integer, description: Optional. The number of accelerators
        to attach to a worker replica., optional: true}
    outputs:
    - {name: endpoint_name, type: GoogleCloudVertexAiEndpointName}
    - {name: endpoint_dict, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-aiplatform==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.7.0'
          --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(
              model_name,
              endpoint_name = None,
              machine_type = "n1-standard-2",
              min_replica_count = 1,
              max_replica_count = 1,
              accelerator_type = None,
              accelerator_count = None,
              #
              # Uncomment when anyone requests these:
              # deployed_model_display_name: str = None,
              # traffic_percentage: int = 0,
              # traffic_split: dict = None,
              # service_account: str = None,
              # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
              # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,
              #
              # encryption_spec_key_name: str = None,
          ):
              """Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.

              Args:
                  model_name: Full resource name of a Google Cloud Vertex AI Model
                  endpoint_name: Optional. Full name of Google Cloud Vertex Endpoint. A new
                      endpoint is created if the name is not passed.
                  machine_type: The type of the machine. See the [list of machine types
                      supported for prediction
                      ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
                      Defaults to "n1-standard-2"
                  min_replica_count (int):
                      Optional. The minimum number of machine replicas this deployed
                      model will be always deployed on. If traffic against it increases,
                      it may dynamically be deployed onto more replicas, and as traffic
                      decreases, some of these extra replicas may be freed.
                  max_replica_count (int):
                      Optional. The maximum number of replicas this deployed model may
                      be deployed on when the traffic against it increases. If requested
                      value is too large, the deployment will error, but if deployment
                      succeeds then the ability to scale the model to that many replicas
                      is guaranteed (barring service outages). If traffic against the
                      deployed model increases beyond what its replicas at maximum may
                      handle, a portion of the traffic will be dropped. If this value
                      is not provided, the smaller value of min_replica_count or 1 will
                      be used.
                  accelerator_type (str):
                      Optional. Hardware accelerator type. Must also set accelerator_count if used.
                      One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,
                      NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
                  accelerator_count (int):
                      Optional. The number of accelerators to attach to a worker replica.
              """
              import json
              from google.cloud import aiplatform

              model = aiplatform.Model(model_name=model_name)

              if endpoint_name:
                  endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)
              else:
                  endpoint_display_name = model.display_name[:118] + "_endpoint"
                  endpoint = aiplatform.Endpoint.create(
                      display_name=endpoint_display_name,
                      project=model.project,
                      location=model.location,
                      # encryption_spec_key_name=encryption_spec_key_name,
                      labels={"component-source": "github-com-ark-kun-pipeline-components"},
                  )

              endpoint = model.deploy(
                  endpoint=endpoint,
                  # deployed_model_display_name=deployed_model_display_name,
                  machine_type=machine_type,
                  min_replica_count=min_replica_count,
                  max_replica_count=max_replica_count,
                  accelerator_type=accelerator_type,
                  accelerator_count=accelerator_count,
                  # service_account=service_account,
                  # explanation_metadata=explanation_metadata,
                  # explanation_parameters=explanation_parameters,
                  # encryption_spec_key_name=encryption_spec_key_name,
              )

              endpoint_json = json.dumps(endpoint.to_dict(), indent=2)
              print(endpoint_json)
              return (endpoint.resource_name, endpoint_json)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Deploy model to endpoint for Google Cloud Vertex AI Model', description='Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.')
          _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--endpoint-name", dest="endpoint_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--machine-type", dest="machine_type", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--min-replica-count", dest="min_replica_count", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--max-replica-count", dest="max_replica_count", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--accelerator-type", dest="accelerator_type", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--accelerator-count", dest="accelerator_count", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)

          _output_serializers = [
              str,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --model-name
        - {inputValue: model_name}
        - if:
            cond: {isPresent: endpoint_name}
            then:
            - --endpoint-name
            - {inputValue: endpoint_name}
        - if:
            cond: {isPresent: machine_type}
            then:
            - --machine-type
            - {inputValue: machine_type}
        - if:
            cond: {isPresent: min_replica_count}
            then:
            - --min-replica-count
            - {inputValue: min_replica_count}
        - if:
            cond: {isPresent: max_replica_count}
            then:
            - --max-replica-count
            - {inputValue: max_replica_count}
        - if:
            cond: {isPresent: accelerator_type}
            then:
            - --accelerator-type
            - {inputValue: accelerator_type}
        - if:
            cond: {isPresent: accelerator_count}
            then:
            - --accelerator-count
            - {inputValue: accelerator_count}
        - '----output-paths'
        - {outputPath: endpoint_name}
        - {outputPath: endpoint_dict}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d45e011ad8b62b4fe36c12289a624e5e1573c68d/components/google-cloud/Vertex_AI/Models/Export/to_GCS/component.yaml',
    digest: 7525be19088b075bf2f404f4ca8173ccb18220250198699a323b3a836ae814f9}
  annotations:
    GitHub commit:
      sha: d45e011ad8b62b4fe36c12289a624e5e1573c68d
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d45e011ad8b62b4fe36c12289a624e5e1573c68d
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d45e011ad8b62b4fe36c12289a624e5e1573c68d
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:55:42Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-08T10:55:42Z'}
        message: Google Cloud - Vertex AI - Models - Export - to_GCS - Added component
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Export model to GCS for Google Cloud Vertex AI Model
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Export/to_GCS/component.yaml'}
    inputs:
    - {name: model_name, type: GoogleCloudVertexAiModelName}
    - {name: output_prefix_gcs_uri, type: String}
    - {name: export_format, type: String, optional: true}
    outputs:
    - {name: model_dir_uri, type: String}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'google-cloud-aiplatform==1.6.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.6.2'
          --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def export_model_to_GCS_for_Google_Cloud_Vertex_AI_Model(
              model_name,
              output_prefix_gcs_uri,  # GoogleCloudStorageURI
              export_format = None,
          ):
              # Choose output_prefix_gcs_uri properly to avoid the following error:
              # google.api_core.exceptions.FailedPrecondition: 400 The Cloud Storage bucket of `gs://<output_prefix_gcs_uri>/model-7972079425934065664/tf-saved-model/2021-11-08T10:44:57.671790Z` is in location `us`.
              # It must be in the same regional location as the service location `us-central1`.
              from google.cloud import aiplatform

              model = aiplatform.Model(model_name=model_name)

              print("Available export formats:")
              print(model.supported_export_formats)
              if not export_format:
                  export_format = list(model.supported_export_formats.keys())[0]
                  print(f"Auto-selected export formats: {export_format}")

              result = model.export_model(
                  export_format_id=export_format,
                  artifact_destination=output_prefix_gcs_uri,
              )

              # == "gs://<artifact_destination>/model-7972079425934065664/tf-saved-model/2021-11-08T00:54:18.367871Z"
              artifact_output_uri = result["artifactOutputUri"]

              return (artifact_output_uri,)

          def _serialize_str(str_value: str) -> str:
              if not isinstance(str_value, str):
                  raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
              return str_value

          import argparse
          _parser = argparse.ArgumentParser(prog='Export model to GCS for Google Cloud Vertex AI Model', description='')
          _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--output-prefix-gcs-uri", dest="output_prefix_gcs_uri", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--export-format", dest="export_format", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = export_model_to_GCS_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)

          _output_serializers = [
              _serialize_str,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --model-name
        - {inputValue: model_name}
        - --output-prefix-gcs-uri
        - {inputValue: output_prefix_gcs_uri}
        - if:
            cond: {isPresent: export_format}
            then:
            - --export-format
            - {inputValue: export_format}
        - '----output-paths'
        - {outputPath: model_dir_uri}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/2c24c0c0730c818b89f676c4dc5c9d6cb90ab01d/components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml',
    digest: 28cee22ce598009bdc4b54c7ba604a43f69d1de2f5eef0070544d10742b9502a}
  annotations:
    GitHub commit:
      sha: 2c24c0c0730c818b89f676c4dc5c9d6cb90ab01d
      html_url: https://github.com/Ark-kun/pipeline_components/commit/2c24c0c0730c818b89f676c4dc5c9d6cb90ab01d
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/2c24c0c0730c818b89f676c4dc5c9d6cb90ab01d
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-15T10:40:22Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-11-15T10:40:22Z'}
        message: 'fix: Google Cloud - Vertex AI - Model - Upload_XGBoost_model - Returning
          fully-qualified model name'
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Upload XGBoost model to Google Cloud Vertex AI
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml'}
    inputs:
    - {name: model, type: XGBoostModel}
    - {name: xgboost_version, type: String, optional: true}
    - {name: display_name, type: String, optional: true}
    - {name: description, type: String, optional: true}
    - {name: project, type: String, optional: true}
    - {name: location, type: String, default: us-central1, optional: true}
    - {name: labels, type: JsonObject, optional: true}
    - {name: staging_bucket, type: String, optional: true}
    outputs:
    - {name: model_name, type: GoogleCloudVertexAiModelName}
    - {name: model_dict, type: JsonObject}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'
          'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'
          'google-api-python-client==2.29.0' --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def upload_XGBoost_model_to_Google_Cloud_Vertex_AI(
              model_path,
              xgboost_version = None,

              display_name = None,
              description = None,

              # Uncomment when anyone requests these:
              # instance_schema_uri: str = None,
              # parameters_schema_uri: str = None,
              # prediction_schema_uri: str = None,
              # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
              # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,

              project = None,
              location = "us-central1",
              labels = None,
              # encryption_spec_key_name: str = None,
              staging_bucket = None,
          ):
              kwargs = locals()
              kwargs.pop("model_path")

              import json
              import os
              from google.cloud import aiplatform

              # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.
              # This leads to failure when trying to create any resource in the project.
              # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).
              # We can try and get the GCP project ID/number from the environment variables.
              if not project:
                  project_number = os.environ.get("CLOUD_ML_PROJECT_ID")
                  if project_number:
                      print(f"Inferred project number: {project_number}")
                      kwargs["project"] = project_number
                      # To improve the naming we try to convert the project number into the user project ID.
                      try:
                          from googleapiclient import discovery

                          cloud_resource_manager_service = discovery.build(
                              "cloudresourcemanager", "v3"
                          )
                          project_id = (
                              cloud_resource_manager_service.projects()
                              .get(name=f"projects/{project_number}")
                              .execute()["projectId"]
                          )
                          if project_id:
                              print(f"Inferred project ID: {project_id}")
                              kwargs["project"] = project_id
                      except Exception as e:
                          print(e)

              if not location:
                  kwargs["location"] = os.environ.get("CLOUD_ML_REGION")

              if not labels:
                  kwargs["labels"] = {}
              kwargs["labels"]["component-source"] = "github-com-ark-kun-pipeline-components"

              model = aiplatform.Model.upload_xgboost_model_file(
                  model_file_path=model_path,
                  **kwargs,
              )
              model_json = json.dumps(model.to_dict(), indent=2)
              print(model_json)
              return (model.resource_name, model_json)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Upload XGBoost model to Google Cloud Vertex AI', description='')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--xgboost-version", dest="xgboost_version", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--display-name", dest="display_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--description", dest="description", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--project", dest="project", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--location", dest="location", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--labels", dest="labels", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--staging-bucket", dest="staging_bucket", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = upload_XGBoost_model_to_Google_Cloud_Vertex_AI(**_parsed_args)

          _output_serializers = [
              str,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --model
        - {inputPath: model}
        - if:
            cond: {isPresent: xgboost_version}
            then:
            - --xgboost-version
            - {inputValue: xgboost_version}
        - if:
            cond: {isPresent: display_name}
            then:
            - --display-name
            - {inputValue: display_name}
        - if:
            cond: {isPresent: description}
            then:
            - --description
            - {inputValue: description}
        - if:
            cond: {isPresent: project}
            then:
            - --project
            - {inputValue: project}
        - if:
            cond: {isPresent: location}
            then:
            - --location
            - {inputValue: location}
        - if:
            cond: {isPresent: labels}
            then:
            - --labels
            - {inputValue: labels}
        - if:
            cond: {isPresent: staging_bucket}
            then:
            - --staging-bucket
            - {inputValue: staging_bucket}
        - '----output-paths'
        - {outputPath: model_name}
        - {outputPath: model_dict}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_dict/component.yaml',
    digest: a3d12264c5d5cd52900d2a8d16c96630954de1762c0000103c999a0d48396a7a}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build dict
    description: Creates a JSON object from multiple key and value pairs.
    inputs:
    - {name: key_1, type: String, optional: true}
    - {name: value_1, type: JsonObject, optional: true}
    - {name: key_2, type: String, optional: true}
    - {name: value_2, type: JsonObject, optional: true}
    - {name: key_3, type: String, optional: true}
    - {name: value_3, type: JsonObject, optional: true}
    - {name: key_4, type: String, optional: true}
    - {name: value_4, type: JsonObject, optional: true}
    - {name: key_5, type: String, optional: true}
    - {name: value_5, type: JsonObject, optional: true}
    outputs:
    - {name: Output, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_dict/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - python3
        - -u
        - -c
        - |
          def build_dict(
              key_1 = None,
              value_1 = None,
              key_2 = None,
              value_2 = None,
              key_3 = None,
              value_3 = None,
              key_4 = None,
              value_4 = None,
              key_5 = None,
              value_5 = None,
          ):
              """Creates a JSON object from multiple key and value pairs.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              result = dict([
                  (key_1, value_1),
                  (key_2, value_2),
                  (key_3, value_3),
                  (key_4, value_4),
                  (key_5, value_5),
              ])
              if None in result:
                  del result[None]
              return result

          import json
          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build dict', description='Creates a JSON object from multiple key and value pairs.')
          _parser.add_argument("--key-1", dest="key_1", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--value-1", dest="value_1", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--key-2", dest="key_2", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--value-2", dest="value_2", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--key-3", dest="key_3", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--value-3", dest="value_3", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--key-4", dest="key_4", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--value-4", dest="value_4", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--key-5", dest="key_5", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--value-5", dest="value_5", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_dict(**_parsed_args)

          _outputs = [_outputs]

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - if:
            cond: {isPresent: key_1}
            then:
            - --key-1
            - {inputValue: key_1}
        - if:
            cond: {isPresent: value_1}
            then:
            - --value-1
            - {inputValue: value_1}
        - if:
            cond: {isPresent: key_2}
            then:
            - --key-2
            - {inputValue: key_2}
        - if:
            cond: {isPresent: value_2}
            then:
            - --value-2
            - {inputValue: value_2}
        - if:
            cond: {isPresent: key_3}
            then:
            - --key-3
            - {inputValue: key_3}
        - if:
            cond: {isPresent: value_3}
            then:
            - --value-3
            - {inputValue: value_3}
        - if:
            cond: {isPresent: key_4}
            then:
            - --key-4
            - {inputValue: key_4}
        - if:
            cond: {isPresent: value_4}
            then:
            - --value-4
            - {inputValue: value_4}
        - if:
            cond: {isPresent: key_5}
            then:
            - --key-5
            - {inputValue: key_5}
        - if:
            cond: {isPresent: value_5}
            then:
            - --value-5
            - {inputValue: value_5}
        - '----output-paths'
        - {outputPath: Output}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_list/component.yaml',
    digest: 24b33123e1cb13712aec208e81e6c1dc2d6d643c8a1fe323581e8858c3c8993a}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build list
    description: Creates a JSON array from multiple items.
    inputs:
    - {name: item_1, type: JsonObject, optional: true}
    - {name: item_2, type: JsonObject, optional: true}
    - {name: item_3, type: JsonObject, optional: true}
    - {name: item_4, type: JsonObject, optional: true}
    - {name: item_5, type: JsonObject, optional: true}
    outputs:
    - {name: Output, type: JsonArray}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - python3
        - -u
        - -c
        - |
          def build_list(
              item_1 = None,
              item_2 = None,
              item_3 = None,
              item_4 = None,
              item_5 = None,
          ):
              """Creates a JSON array from multiple items.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              result = []
              for item in [item_1, item_2, item_3, item_4, item_5]:
                  if item is not None:
                      result.append(item)
              return result

          import json
          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build list', description='Creates a JSON array from multiple items.')
          _parser.add_argument("--item-1", dest="item_1", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-2", dest="item_2", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-3", dest="item_3", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-4", dest="item_4", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-5", dest="item_5", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_list(**_parsed_args)

          _outputs = [_outputs]

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - if:
            cond: {isPresent: item_1}
            then:
            - --item-1
            - {inputValue: item_1}
        - if:
            cond: {isPresent: item_2}
            then:
            - --item-2
            - {inputValue: item_2}
        - if:
            cond: {isPresent: item_3}
            then:
            - --item-3
            - {inputValue: item_3}
        - if:
            cond: {isPresent: item_4}
            then:
            - --item-4
            - {inputValue: item_4}
        - if:
            cond: {isPresent: item_5}
            then:
            - --item-5
            - {inputValue: item_5}
        - '----output-paths'
        - {outputPath: Output}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_floats/component.yaml',
    digest: b3ad1eeb79066b286809fe4a1b078aee08bbadd07d977b5327880e854bec1ff2}
  annotations:
    GitHub commit:
      sha: bb9d7518b3a23e945c8cc1663942063c6b92c20f
      html_url: https://github.com/Ark-kun/pipeline_components/commit/bb9d7518b3a23e945c8cc1663942063c6b92c20f
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/bb9d7518b3a23e945c8cc1663942063c6b92c20f
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T23:20:43Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T23:20:43Z'}
        message: JSON - Replaced the "Build_list_of_numbers" component with "Build_list_of_integers"
          and "Build_list_of_floats" components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build list of floats
    description: Creates a JSON array from multiple floating-point numbers.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_floats/component.yaml'}
    inputs:
    - {name: item_1, type: Float, optional: true}
    - {name: item_2, type: Float, optional: true}
    - {name: item_3, type: Float, optional: true}
    - {name: item_4, type: Float, optional: true}
    - {name: item_5, type: Float, optional: true}
    outputs:
    - {name: Output, type: JsonArray}
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def build_list_of_floats(
              item_1 = None,
              item_2 = None,
              item_3 = None,
              item_4 = None,
              item_5 = None,
          ):
              """Creates a JSON array from multiple floating-point numbers.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              result = []
              for item in [item_1, item_2, item_3, item_4, item_5]:
                  if item is not None:
                      result.append(item)
              return result

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build list of floats', description='Creates a JSON array from multiple floating-point numbers.')
          _parser.add_argument("--item-1", dest="item_1", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-2", dest="item_2", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-3", dest="item_3", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-4", dest="item_4", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-5", dest="item_5", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_list_of_floats(**_parsed_args)

          _outputs = [_outputs]

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - if:
            cond: {isPresent: item_1}
            then:
            - --item-1
            - {inputValue: item_1}
        - if:
            cond: {isPresent: item_2}
            then:
            - --item-2
            - {inputValue: item_2}
        - if:
            cond: {isPresent: item_3}
            then:
            - --item-3
            - {inputValue: item_3}
        - if:
            cond: {isPresent: item_4}
            then:
            - --item-4
            - {inputValue: item_4}
        - if:
            cond: {isPresent: item_5}
            then:
            - --item-5
            - {inputValue: item_5}
        - '----output-paths'
        - {outputPath: Output}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_integers/component.yaml',
    digest: 965f5a4f2f0a34f07419ab5d60cc3c3505ab2d7579baa0dd36d9a32ea97a7ffa}
  annotations:
    GitHub commit:
      sha: bb9d7518b3a23e945c8cc1663942063c6b92c20f
      html_url: https://github.com/Ark-kun/pipeline_components/commit/bb9d7518b3a23e945c8cc1663942063c6b92c20f
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/bb9d7518b3a23e945c8cc1663942063c6b92c20f
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T23:20:43Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T23:20:43Z'}
        message: JSON - Replaced the "Build_list_of_numbers" component with "Build_list_of_integers"
          and "Build_list_of_floats" components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build list of integers
    description: Creates a JSON array from multiple integer numbers.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_integers/component.yaml'}
    inputs:
    - {name: item_1, type: Integer, optional: true}
    - {name: item_2, type: Integer, optional: true}
    - {name: item_3, type: Integer, optional: true}
    - {name: item_4, type: Integer, optional: true}
    - {name: item_5, type: Integer, optional: true}
    outputs:
    - {name: Output, type: JsonArray}
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def build_list_of_integers(
              item_1 = None,
              item_2 = None,
              item_3 = None,
              item_4 = None,
              item_5 = None,
          ):
              """Creates a JSON array from multiple integer numbers.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              result = []
              for item in [item_1, item_2, item_3, item_4, item_5]:
                  if item is not None:
                      result.append(item)
              return result

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build list of integers', description='Creates a JSON array from multiple integer numbers.')
          _parser.add_argument("--item-1", dest="item_1", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-2", dest="item_2", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-3", dest="item_3", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-4", dest="item_4", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-5", dest="item_5", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_list_of_integers(**_parsed_args)

          _outputs = [_outputs]

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - if:
            cond: {isPresent: item_1}
            then:
            - --item-1
            - {inputValue: item_1}
        - if:
            cond: {isPresent: item_2}
            then:
            - --item-2
            - {inputValue: item_2}
        - if:
            cond: {isPresent: item_3}
            then:
            - --item-3
            - {inputValue: item_3}
        - if:
            cond: {isPresent: item_4}
            then:
            - --item-4
            - {inputValue: item_4}
        - if:
            cond: {isPresent: item_5}
            then:
            - --item-5
            - {inputValue: item_5}
        - '----output-paths'
        - {outputPath: Output}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/aecac18d4023c73c561d7f21192253e9593b9932/components/json/Build_list_of_strings/component.yaml',
    digest: 76963afe607da688f1334e8cff2017da891f92609855d6103ccbab627c6eb2ef}
  annotations:
    GitHub commit:
      sha: aecac18d4023c73c561d7f21192253e9593b9932
      html_url: https://github.com/Ark-kun/pipeline_components/commit/aecac18d4023c73c561d7f21192253e9593b9932
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/aecac18d4023c73c561d7f21192253e9593b9932
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T22:40:04Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-02T22:40:04Z'}
        message: JSON - Added the "Build_list_of_strings" and "Build_list_of_numbers"
          components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Build list of strings
    description: Creates a JSON array from multiple strings.
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_strings/component.yaml'}
    inputs:
    - {name: item_1, type: String, optional: true}
    - {name: item_2, type: String, optional: true}
    - {name: item_3, type: String, optional: true}
    - {name: item_4, type: String, optional: true}
    - {name: item_5, type: String, optional: true}
    outputs:
    - {name: Output, type: JsonArray}
    implementation:
      container:
        image: python:3.9
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def build_list_of_strings(
              item_1 = None,
              item_2 = None,
              item_3 = None,
              item_4 = None,
              item_5 = None,
          ):
              """Creates a JSON array from multiple strings.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              result = []
              for item in [item_1, item_2, item_3, item_4, item_5]:
                  if item is not None:
                      result.append(item)
              return result

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Build list of strings', description='Creates a JSON array from multiple strings.')
          _parser.add_argument("--item-1", dest="item_1", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-2", dest="item_2", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-3", dest="item_3", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-4", dest="item_4", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--item-5", dest="item_5", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = build_list_of_strings(**_parsed_args)

          _outputs = [_outputs]

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - if:
            cond: {isPresent: item_1}
            then:
            - --item-1
            - {inputValue: item_1}
        - if:
            cond: {isPresent: item_2}
            then:
            - --item-2
            - {inputValue: item_2}
        - if:
            cond: {isPresent: item_3}
            then:
            - --item-3
            - {inputValue: item_3}
        - if:
            cond: {isPresent: item_4}
            then:
            - --item-4
            - {inputValue: item_4}
        - if:
            cond: {isPresent: item_5}
            then:
            - --item-5
            - {inputValue: item_5}
        - '----output-paths'
        - {outputPath: Output}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Combine_lists/component.yaml',
    digest: f2b024570631dd504413a7836297d02067423ff9d64b62ebc76be1a943488f4a}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Combine lists
    description: Combines multiple JSON arrays into one.
    inputs:
    - {name: list_1, type: JsonArray, optional: true}
    - {name: list_2, type: JsonArray, optional: true}
    - {name: list_3, type: JsonArray, optional: true}
    - {name: list_4, type: JsonArray, optional: true}
    - {name: list_5, type: JsonArray, optional: true}
    outputs:
    - {name: Output, type: JsonArray}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Combine_lists/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - python3
        - -u
        - -c
        - |
          def combine_lists(
              list_1 = None,
              list_2 = None,
              list_3 = None,
              list_4 = None,
              list_5 = None,
          ):
              """Combines multiple JSON arrays into one.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              """
              result = []
              for list in [list_1, list_2, list_3, list_4, list_5]:
                  if list is not None:
                      result.extend(list)
              return result

          import json
          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Combine lists', description='Combines multiple JSON arrays into one.')
          _parser.add_argument("--list-1", dest="list_1", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--list-2", dest="list_2", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--list-3", dest="list_3", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--list-4", dest="list_4", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--list-5", dest="list_5", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = combine_lists(**_parsed_args)

          _outputs = [_outputs]

          _output_serializers = [
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - if:
            cond: {isPresent: list_1}
            then:
            - --list-1
            - {inputValue: list_1}
        - if:
            cond: {isPresent: list_2}
            then:
            - --list-2
            - {inputValue: list_2}
        - if:
            cond: {isPresent: list_3}
            then:
            - --list-3
            - {inputValue: list_3}
        - if:
            cond: {isPresent: list_4}
            then:
            - --list-4
            - {inputValue: list_4}
        - if:
            cond: {isPresent: list_5}
            then:
            - --list-5
            - {inputValue: list_5}
        - '----output-paths'
        - {outputPath: Output}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_index/component.yaml',
    digest: c7b09bd3bf9cf9e42e22bedcc4822c32c31f81f92e45c019f9cca626961cf2a2}
  annotations:
    GitHub commit:
      sha: dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a
      html_url: https://github.com/Ark-kun/pipeline_components/commit/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T03:18:24Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T03:18:24Z'}
        message: JSON - Fixed the trailing newline in the "Get_element_by_index" "Get_element_by_key"
          components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Get element by index from JSON
    inputs:
    - {name: Json}
    - {name: Index, type: Integer}
    outputs:
    - {name: Output}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_index/component.yaml'
    implementation:
      container:
        image: stedolan/jq:latest
        command:
        - sh
        - -exc
        - |
          input_path=$0
          output_path=$1
          index=$2
          mkdir -p "$(dirname "$output_path")"
          < "$input_path" jq --raw-output --join-output .["$index"] > "$output_path"
        - {inputPath: Json}
        - {outputPath: Output}
        - {inputValue: Index}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_key/component.yaml',
    digest: 2790d7a0b9983e3b9ddca1fb02b243a7b3e0280198cd859438d622ad1ca8d3b0}
  annotations:
    GitHub commit:
      sha: dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a
      html_url: https://github.com/Ark-kun/pipeline_components/commit/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T03:18:24Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T03:18:24Z'}
        message: JSON - Fixed the trailing newline in the "Get_element_by_index" "Get_element_by_key"
          components
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Get element by key from JSON
    inputs:
    - {name: Json}
    - {name: Key, type: String}
    outputs:
    - {name: Output}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_key/component.yaml'
    implementation:
      container:
        image: stedolan/jq:latest
        command:
        - sh
        - -exc
        - |
          input_path=$0
          output_path=$1
          key=$2
          mkdir -p "$(dirname "$output_path")"
          < "$input_path" jq --raw-output --join-output '.["'"$key"'"]' > "$output_path"
        - {inputPath: Json}
        - {outputPath: Output}
        - {inputValue: Key}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Query/component.yaml',
    digest: 879a8ddf73f5bec05643901f88b024f82c83cf1443536b98a624331a89d8a9f5}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Query JSON using JQ
    inputs:
    - {name: Json}
    - {name: Query, type: String}
    - {name: Options, type: String, default: '--raw-output'}
    outputs:
    - {name: Output}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Query/component.yaml'
    implementation:
      container:
        image: stedolan/jq:latest
        command:
        - sh
        - -exc
        - |
          input_path=$0
          output_path=$1
          query=$2
          options=$3
          mkdir -p "$(dirname "$output_path")"
          < "$input_path" jq $options "$query" > "$output_path"
        - {inputPath: Json}
        - {outputPath: Output}
        - {inputValue: Query}
        - {inputValue: Options}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/keras/Train_classifier/from_CSV/component.yaml',
    digest: f35b7e3ca33263f74a1495214afe00d7ea628f063259e456eb3686dfd71ac1b9}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Keras train classifier from csv
    description: |-
      Trains classifier model using Keras.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: training_features, type: CSV}
    - {name: training_labels, type: CSV}
    - {name: network_json, type: KerasModelJson}
    - {name: loss_name, type: String, default: categorical_crossentropy, optional: true}
    - {name: num_classes, type: Integer, optional: true}
    - {name: optimizer, type: String, default: rmsprop, optional: true}
    - {name: optimizer_config, type: JsonObject, optional: true}
    - {name: learning_rate, type: Float, default: '0.01', optional: true}
    - {name: num_epochs, type: Integer, default: '100', optional: true}
    - {name: batch_size, type: Integer, default: '32', optional: true}
    - {name: metrics, type: JsonArray, default: '["accuracy"]', optional: true}
    - {name: random_seed, type: Integer, default: '0', optional: true}
    outputs:
    - {name: model, type: KerasModelHdf5}
    - {name: final_loss, type: Float}
    - {name: final_metrics, type: JsonObject}
    - {name: metrics_history, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/keras/Train_classifier/from_CSV/component.yaml'
    implementation:
      container:
        image: tensorflow/tensorflow:2.2.0
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'keras==2.3.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
          pip install --quiet --no-warn-script-location 'keras==2.3.1' 'pandas==1.0.5'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def keras_train_classifier_from_csv(
              training_features_path,
              training_labels_path,
              network_json_path,
              model_path,
              loss_name = 'categorical_crossentropy',
              num_classes = None,
              optimizer = 'rmsprop',
              optimizer_config = None,
              learning_rate = 0.01,
              num_epochs = 100,
              batch_size = 32,
              metrics = ['accuracy'],
              random_seed = 0,
          ):
              '''Trains classifier model using Keras.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pathlib import Path

              import keras
              import numpy
              import pandas
              import tensorflow

              tensorflow.random.set_seed(random_seed)
              numpy.random.seed(random_seed)

              training_features_df = pandas.read_csv(training_features_path)
              training_labels_df = pandas.read_csv(training_labels_path)

              x_train = training_features_df.to_numpy()
              y_train_labels = training_labels_df.to_numpy()
              print('Training features shape:', x_train.shape)
              print('Numer of training samples:', x_train.shape[0])

              # Convert class vectors to binary class matrices.
              y_train_one_hot = keras.utils.to_categorical(y_train_labels, num_classes)

              model_json_str = Path(network_json_path).read_text()
              model = keras.models.model_from_json(model_json_str)

              model.add(keras.layers.Activation('softmax'))

              # Initializing the optimizer
              optimizer_config = optimizer_config or {}
              optimizer_config['learning_rate'] = learning_rate
              optimizer = keras.optimizers.deserialize({
                  'class_name': optimizer,
                  'config': optimizer_config,
              })

              model.compile(
                  loss=loss_name,
                  optimizer=optimizer,
                  metrics=metrics,
              )

              history = model.fit(
                  x_train,
                  y_train_one_hot,
                  batch_size=batch_size,
                  epochs=num_epochs,
                  shuffle=True
              )

              model.save(model_path)

              metrics_history = {name: [float(value) for value in values] for name, values in history.history.items()}
              final_metrics = {name: values[-1] for name, values in metrics_history.items()}
              final_loss = final_metrics['loss']
              return (final_loss, final_metrics, metrics_history)

          import json
          def _serialize_float(float_value: float) -> str:
              if isinstance(float_value, str):
                  return float_value
              if not isinstance(float_value, (float, int)):
                  raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
              return str(float_value)

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          import argparse
          _parser = argparse.ArgumentParser(prog='Keras train classifier from csv', description='Trains classifier model using Keras.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--training-features", dest="training_features_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--training-labels", dest="training_labels_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--network-json", dest="network_json_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--loss-name", dest="loss_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--num-classes", dest="num_classes", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimizer", dest="optimizer", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimizer-config", dest="optimizer_config", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--num-epochs", dest="num_epochs", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metrics", dest="metrics", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = keras_train_classifier_from_csv(**_parsed_args)

          _output_serializers = [
              _serialize_float,
              _serialize_json,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --training-features
        - {inputPath: training_features}
        - --training-labels
        - {inputPath: training_labels}
        - --network-json
        - {inputPath: network_json}
        - if:
            cond: {isPresent: loss_name}
            then:
            - --loss-name
            - {inputValue: loss_name}
        - if:
            cond: {isPresent: num_classes}
            then:
            - --num-classes
            - {inputValue: num_classes}
        - if:
            cond: {isPresent: optimizer}
            then:
            - --optimizer
            - {inputValue: optimizer}
        - if:
            cond: {isPresent: optimizer_config}
            then:
            - --optimizer-config
            - {inputValue: optimizer_config}
        - if:
            cond: {isPresent: learning_rate}
            then:
            - --learning-rate
            - {inputValue: learning_rate}
        - if:
            cond: {isPresent: num_epochs}
            then:
            - --num-epochs
            - {inputValue: num_epochs}
        - if:
            cond: {isPresent: batch_size}
            then:
            - --batch-size
            - {inputValue: batch_size}
        - if:
            cond: {isPresent: metrics}
            then:
            - --metrics
            - {inputValue: metrics}
        - if:
            cond: {isPresent: random_seed}
            then:
            - --random-seed
            - {inputValue: random_seed}
        - --model
        - {outputPath: model}
        - '----output-paths'
        - {outputPath: final_loss}
        - {outputPath: final_metrics}
        - {outputPath: metrics_history}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/kfp/Run_component/component.yaml',
    digest: e5462a0027fad59dc9c59ae542f6186118f6147e2b3f65d75d9cb05b553ef402}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Run component or pipeline\nmetadata:\n  annotations: \n    author:\
    \ Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/kfp/Run_component/component.yaml'\n\
    inputs:\n- {name: component_url, type: Url}\n- {name: arguments, type: JsonObject}\n\
    - {name: endpoint, type: String, optional: true}\n- {name: wait_timeout_seconds,\
    \ type: Float, optional: true}\noutputs:\n- {name: run_id, type: String}\n- {name:\
    \ run_object, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n\
    \    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3\
    \ -m pip install --quiet --no-warn-script-location\n      'kfp==1.4.0' || PIP_DISABLE_PIP_VERSION_CHECK=1\
    \ python3 -m pip install --quiet\n      --no-warn-script-location 'kfp==1.4.0'\
    \ --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n\
    \      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\"\
    \ \"$@\"\n    - |\n      def run_component_or_pipeline(\n          component_url,\n\
    \          arguments,\n          endpoint = None,\n          wait_timeout_seconds\
    \ = None,\n      ):\n          import json\n          import os\n          import\
    \ kfp\n          from kfp_server_api import ApiClient\n          print('Loading\
    \ component...')\n          op = kfp.components.load_component_from_url(component_url)\n\
    \          print('Loading component done.')\n          print('Submitting run...')\n\
    \          if not endpoint:\n              endpoint = 'http://' + os.environ['ML_PIPELINE_SERVICE_HOST']\
    \ + ':' + os.environ['ML_PIPELINE_SERVICE_PORT']\n          create_run_result\
    \ = kfp.Client(host=endpoint).create_run_from_pipeline_func(op, arguments=arguments)\n\
    \          run_id = str(create_run_result.run_id)\n          print('Submitted\
    \ run: ' + run_id)\n          run_url = f'{endpoint.rstrip(\"/\")}/#/runs/details/{run_id}'\n\
    \          print(run_url)\n          print('Waiting for the run to finish...')\n\
    \          run_object = create_run_result.wait_for_run_completion(wait_timeout_seconds)\n\
    \          print('Run has finished.')\n          # sanitize_for_serialization\
    \ uses correct field names and properly converts datetime values\n          run_dict\
    \ = ApiClient().sanitize_for_serialization(run_object)\n          return (\n \
    \             run_id,\n              json.dumps(run_dict, indent=4),\n       \
    \   )\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj,\
    \ str):\n              return obj\n          import json\n          def default_serializer(obj):\n\
    \              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n\
    \              else:\n                  raise TypeError(\"Object of type '%s'\
    \ is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n\
    \          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\
    \n      def _serialize_str(str_value: str) -> str:\n          if not isinstance(str_value,\
    \ str):\n              raise TypeError('Value \"{}\" has type \"{}\" instead of\
    \ str.'.format(str(str_value), str(type(str_value))))\n          return str_value\n\
    \n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Run\
    \ component or pipeline', description='')\n      _parser.add_argument(\"--component-url\"\
    , dest=\"component_url\", type=str, required=True, default=argparse.SUPPRESS)\n\
    \      _parser.add_argument(\"--arguments\", dest=\"arguments\", type=json.loads,\
    \ required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--endpoint\"\
    , dest=\"endpoint\", type=str, required=False, default=argparse.SUPPRESS)\n  \
    \    _parser.add_argument(\"--wait-timeout-seconds\", dest=\"wait_timeout_seconds\"\
    , type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"\
    ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args\
    \ = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\"\
    , [])\n\n      _outputs = run_component_or_pipeline(**_parsed_args)\n\n      _output_serializers\
    \ = [\n          _serialize_str,\n          _serialize_json,\n\n      ]\n\n  \
    \    import os\n      for idx, output_file in enumerate(_output_files):\n    \
    \      try:\n              os.makedirs(os.path.dirname(output_file))\n       \
    \   except OSError:\n              pass\n          with open(output_file, 'w')\
    \ as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n\
    \    - --component-url\n    - {inputValue: component_url}\n    - --arguments\n\
    \    - {inputValue: arguments}\n    - if:\n        cond: {isPresent: endpoint}\n\
    \        then:\n        - --endpoint\n        - {inputValue: endpoint}\n    -\
    \ if:\n        cond: {isPresent: wait_timeout_seconds}\n        then:\n      \
    \  - --wait-timeout-seconds\n        - {inputValue: wait_timeout_seconds}\n  \
    \  - '----output-paths'\n    - {outputPath: run_id}\n    - {outputPath: run_object}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/kubernetes/Apply_object/component.yaml',
    digest: 0530c0eabc2357ed27d1a7f5c2c5952322fb1003947518b7847fa6b9dbcf7f52}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Apply Kubernetes object\ninputs:\n- {name: Object, type: JsonObject}\n\
    outputs:\n- {name: Name, type: String}\n- {name: Kind, type: String}\n- {name:\
    \ Object, type: JsonObject}\nmetadata:\n  annotations:\n    author: Alexey Volkov\
    \ <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/kubernetes/Apply_object/component.yaml'\n\
    implementation:\n  container:\n    image: bitnami/kubectl:1.17.17\n    command:\n\
    \    - bash\n    - -exc\n    - |\n      object_path=$0\n      output_name_path=$1\n\
    \      output_kind_path=$2\n      output_object_path=$3\n      mkdir -p \"$(dirname\
    \ \"$output_name_path\")\"\n      mkdir -p \"$(dirname \"$output_kind_path\")\"\
    \n      mkdir -p \"$(dirname \"$output_object_path\")\"\n\n      kubectl apply\
    \ -f \"$object_path\" --output=json > \"$output_object_path\"\n      \n      <\
    \ \"$output_object_path\" jq '.metadata.name' --raw-output > \"$output_name_path\"\
    \n      < \"$output_object_path\" jq '.kind' --raw-output > \"$output_kind_path\"\
    \n\n    - {inputPath: Object}\n    - {outputPath: Name}\n    - {outputPath: Kind}\n\
    \    - {outputPath: Object}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/kubernetes/Create_object/component.yaml',
    digest: 4ea0d5af3cd59b64476e1347e5c502c6b4c3278608e37ae29536b1efd9484cba}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Create Kubernetes object\ninputs:\n- {name: Object, type: JsonObject}\n\
    outputs:\n- {name: Name, type: String}\n- {name: Kind, type: String}\n- {name:\
    \ Object, type: JsonObject}\nmetadata:\n  annotations:\n    author: Alexey Volkov\
    \ <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/kubernetes/Create_object/component.yaml'\n\
    implementation:\n  container:\n    image: bitnami/kubectl:1.17.17\n    command:\n\
    \    - bash\n    - -exc\n    - |\n      object_path=$0\n      output_name_path=$1\n\
    \      output_kind_path=$2\n      output_object_path=$3\n      mkdir -p \"$(dirname\
    \ \"$output_name_path\")\"\n      mkdir -p \"$(dirname \"$output_kind_path\")\"\
    \n      mkdir -p \"$(dirname \"$output_object_path\")\"\n\n      kubectl create\
    \ -f \"$object_path\" --output=json > \"$output_object_path\"\n      \n      <\
    \ \"$output_object_path\" jq '.metadata.name' --raw-output > \"$output_name_path\"\
    \n      < \"$output_object_path\" jq '.kind' --raw-output > \"$output_kind_path\"\
    \n\n    - {inputPath: Object}\n    - {outputPath: Name}\n    - {outputPath: Kind}\n\
    \    - {outputPath: Object}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/kubernetes/Create_PersistentVolumeClaim/component.yaml',
    digest: 3ce35f89e110abc72ea1e8214be59b51e043f32a5bc48997adba856211b6b40d}
  annotations:
    GitHub commit:
      sha: 6210648f30b2b3a8c01cc10be338da98300efb6b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/6210648f30b2b3a8c01cc10be338da98300efb6b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/6210648f30b2b3a8c01cc10be338da98300efb6b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        message: Switched from echo to printf to avoid trailing newlines and possible
          data mangling
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create PersistentVolumeClaim in Kubernetes
    inputs:
    - {name: Name,         type: String}
    - {name: Storage size, type: String, default: 1Gi}
    outputs:
    - {name: Name,         type: String}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/kubernetes/Create_PersistentVolumeClaim/component.yaml'
    implementation:
      container:
        image: bitnami/kubectl:1.17.17
        command:
        - bash
        - -exc
        - |
          name=$0
          storage_size=$1
          output_name_path=$2
          mkdir -p "$(dirname "$output_name_path")"
          object_path=$(mktemp)

          cat <<EOF >"$object_path"
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: $name
          spec:
            #storageClassName: standard
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: $storage_size
          EOF
          object_name=$(kubectl apply -f "$object_path" --namespace default --output=name)
          object_name=${object_name##persistentvolumeclaim/}
          printf "%s" "$object_name" >"$output_name_path"

        - {inputValue: Name}
        - {inputValue: Storage size}
        - {outputPath: Name}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/kubernetes/Delete_object/component.yaml',
    digest: 1194e690a450231ef293497415a72ac80856e62bcb97983601808fd651d1f581}
  annotations:
    GitHub commit:
      sha: 6210648f30b2b3a8c01cc10be338da98300efb6b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/6210648f30b2b3a8c01cc10be338da98300efb6b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/6210648f30b2b3a8c01cc10be338da98300efb6b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        message: Switched from echo to printf to avoid trailing newlines and possible
          data mangling
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Delete Kubernetes object
    inputs:
    - {name: Name, type: String}
    - {name: Kind, type: String}
    outputs:
    - {name: Name, type: String}
    - {name: Kind, type: String}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/kubernetes/Delete_object/component.yaml'
    implementation:
      container:
        image: bitnami/kubectl:1.17.17
        command:
        - bash
        - -exc
        - |
          object_name=$0
          object_type=$1
          output_name_path=$2
          output_kind_path=$3
          mkdir -p "$(dirname "$output_name_path")"
          mkdir -p "$(dirname "$output_kind_path")"

          typed_object_name=$(kubectl delete "$object_type" "$object_name" --output=name)
          printf "%s" "${typed_object_name##*/}" >"$output_name_path"
          printf "%s" "${typed_object_name%/*}" >"$output_kind_path"

        - {inputValue: Name}
        - {inputValue: Kind}
        - {outputPath: Name}
        - {outputPath: Kind}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/kubernetes/Get_object/component.yaml',
    digest: 19f7987edfcf3354c6d6462f463fe654896d21845ef04d25f020199044466c4c}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Get Kubernetes object
    inputs:
    - {name: Name, type: String}
    - {name: Kind, type: String}
    outputs:
    - {name: Object, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/kubernetes/Get_object/component.yaml'
    implementation:
      container:
        image: bitnami/kubectl:1.17.17
        command:
        - bash
        - -exc
        - |
          object_name=$0
          object_type=$1
          output_object_path=$2
          mkdir -p "$(dirname "$output_object_path")"

          kubectl get "$object_type" "$object_name" --output=json >"$output_object_path"

        - {inputValue: Name}
        - {inputValue: Kind}
        - {outputPath: Object}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Aggregate_regression_metrics/component.yaml',
    digest: 1291e9c03fab7205acb9bb6b7b41920eed3b07ff3270f1dd57173e421d21f12e}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Aggregate regression metrics
    description: |-
      Calculates regression metrics.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: metrics_1, type: JsonObject}
    - {name: metrics_2, type: JsonObject, optional: true}
    - {name: metrics_3, type: JsonObject, optional: true}
    - {name: metrics_4, type: JsonObject, optional: true}
    - {name: metrics_5, type: JsonObject, optional: true}
    outputs:
    - {name: number_of_items, type: Integer}
    - {name: max_absolute_error, type: Float}
    - {name: mean_absolute_error, type: Float}
    - {name: mean_squared_error, type: Float}
    - {name: root_mean_squared_error, type: Float}
    - {name: metrics, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ml_metrics/Aggregate_regression_metrics/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - python3
        - -u
        - -c
        - |
          def aggregate_regression_metrics(
              metrics_1,
              metrics_2 = None,
              metrics_3 = None,
              metrics_4 = None,
              metrics_5 = None,
          ):
              '''Calculates regression metrics.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import math

              metrics_dicts = [d for d in [metrics_1, metrics_2, metrics_3, metrics_4, metrics_5] if d is not None]
              number_of_items = sum(metrics['number_of_items'] for metrics in metrics_dicts)
              max_absolute_error = max(metrics['max_absolute_error'] for metrics in metrics_dicts)
              mean_absolute_error = sum(metrics['mean_absolute_error'] * metrics['number_of_items'] for metrics in metrics_dicts) / number_of_items
              mean_squared_error = sum(metrics['mean_squared_error'] * metrics['number_of_items'] for metrics in metrics_dicts) / number_of_items
              root_mean_squared_error = math.sqrt(mean_squared_error)
              metrics = dict(
                  number_of_items=number_of_items,
                  max_absolute_error=max_absolute_error,
                  mean_absolute_error=mean_absolute_error,
                  mean_squared_error=mean_squared_error,
                  root_mean_squared_error=root_mean_squared_error,
              )

              return (
                  number_of_items,
                  max_absolute_error,
                  mean_absolute_error,
                  mean_squared_error,
                  root_mean_squared_error,
                  metrics,
              )

          def _serialize_json(obj) -> str:
              if isinstance(obj, str):
                  return obj
              import json
              def default_serializer(obj):
                  if hasattr(obj, 'to_struct'):
                      return obj.to_struct()
                  else:
                      raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
              return json.dumps(obj, default=default_serializer, sort_keys=True)

          def _serialize_float(float_value: float) -> str:
              if isinstance(float_value, str):
                  return float_value
              if not isinstance(float_value, (float, int)):
                  raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
              return str(float_value)

          def _serialize_int(int_value: int) -> str:
              if isinstance(int_value, str):
                  return int_value
              if not isinstance(int_value, int):
                  raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
              return str(int_value)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Aggregate regression metrics', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--metrics-1", dest="metrics_1", type=json.loads, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--metrics-2", dest="metrics_2", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metrics-3", dest="metrics_3", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metrics-4", dest="metrics_4", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--metrics-5", dest="metrics_5", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
          _parsed_args = vars(_parser.parse_args())
          _output_files = _parsed_args.pop("_output_paths", [])

          _outputs = aggregate_regression_metrics(**_parsed_args)

          _output_serializers = [
              _serialize_int,
              _serialize_float,
              _serialize_float,
              _serialize_float,
              _serialize_float,
              _serialize_json,

          ]

          import os
          for idx, output_file in enumerate(_output_files):
              try:
                  os.makedirs(os.path.dirname(output_file))
              except OSError:
                  pass
              with open(output_file, 'w') as f:
                  f.write(_output_serializers[idx](_outputs[idx]))
        args:
        - --metrics-1
        - {inputValue: metrics_1}
        - if:
            cond: {isPresent: metrics_2}
            then:
            - --metrics-2
            - {inputValue: metrics_2}
        - if:
            cond: {isPresent: metrics_3}
            then:
            - --metrics-3
            - {inputValue: metrics_3}
        - if:
            cond: {isPresent: metrics_4}
            then:
            - --metrics-4
            - {inputValue: metrics_4}
        - if:
            cond: {isPresent: metrics_5}
            then:
            - --metrics-5
            - {inputValue: metrics_5}
        - '----output-paths'
        - {outputPath: number_of_items}
        - {outputPath: max_absolute_error}
        - {outputPath: mean_absolute_error}
        - {outputPath: mean_squared_error}
        - {outputPath: root_mean_squared_error}
        - {outputPath: metrics}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml',
    digest: 944def5d4b82aaeee38ac5c171f5e4007d639cfa00e065443dba684a38f7cf7e}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Calculate regression metrics from csv\ndescription: |-\n  Calculates\
    \ regression metrics.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\
    metadata:\n  annotations: \n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\
    \    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'\n\
    inputs:\n- {name: true_values}\n- {name: predicted_values}\noutputs:\n- {name:\
    \ number_of_items, type: Integer}\n- {name: max_absolute_error, type: Float}\n\
    - {name: mean_absolute_error, type: Float}\n- {name: mean_squared_error, type:\
    \ Float}\n- {name: root_mean_squared_error, type: Float}\n- {name: metrics, type:\
    \ JsonObject}\nimplementation:\n  container:\n    image: python:3.7\n    command:\n\
    \    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\
    \ --quiet --no-warn-script-location\n      'numpy==1.19.0' || PIP_DISABLE_PIP_VERSION_CHECK=1\
    \ python3 -m pip install --quiet\n      --no-warn-script-location 'numpy==1.19.0'\
    \ --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n     \
    \ def calculate_regression_metrics_from_csv(\n          true_values_path,\n  \
    \        predicted_values_path,\n      ):\n          '''Calculates regression\
    \ metrics.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\
    \          '''\n          import math\n          import numpy\n\n          true_values\
    \ = numpy.loadtxt(true_values_path, dtype=numpy.float64)\n          predicted_values\
    \ = numpy.loadtxt(predicted_values_path, dtype=numpy.float64)\n\n          if\
    \ len(predicted_values.shape) != 1:\n              raise NotImplemented('Only\
    \ single prediction values are supported.')\n          if len(true_values.shape)\
    \ != 1:\n              raise NotImplemented('Only single true values are supported.')\n\
    \n          if predicted_values.shape != true_values.shape:\n              raise\
    \ ValueError('Input shapes are different: {} != {}'.format(predicted_values.shape,\
    \ true_values.shape))\n\n          number_of_items = true_values.size\n      \
    \    errors = (true_values - predicted_values)\n          abs_errors = numpy.abs(errors)\n\
    \          squared_errors = errors ** 2\n          max_absolute_error = numpy.max(abs_errors)\n\
    \          mean_absolute_error = numpy.average(abs_errors)\n          mean_squared_error\
    \ = numpy.average(squared_errors)\n          root_mean_squared_error = math.sqrt(mean_squared_error)\n\
    \          metrics = dict(\n              number_of_items=number_of_items,\n \
    \             max_absolute_error=max_absolute_error,\n              mean_absolute_error=mean_absolute_error,\n\
    \              mean_squared_error=mean_squared_error,\n              root_mean_squared_error=root_mean_squared_error,\n\
    \          )\n\n          return (\n              number_of_items,\n         \
    \     max_absolute_error,\n              mean_absolute_error,\n              mean_squared_error,\n\
    \              root_mean_squared_error,\n              metrics,\n          )\n\
    \n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n\
    \              return obj\n          import json\n          def default_serializer(obj):\n\
    \              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n\
    \              else:\n                  raise TypeError(\"Object of type '%s'\
    \ is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n\
    \          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\
    \n      def _serialize_float(float_value: float) -> str:\n          if isinstance(float_value,\
    \ str):\n              return float_value\n          if not isinstance(float_value,\
    \ (float, int)):\n              raise TypeError('Value \"{}\" has type \"{}\"\
    \ instead of float.'.format(str(float_value), str(type(float_value))))\n     \
    \     return str(float_value)\n\n      def _serialize_int(int_value: int) -> str:\n\
    \          if isinstance(int_value, str):\n              return int_value\n  \
    \        if not isinstance(int_value, int):\n              raise TypeError('Value\
    \ \"{}\" has type \"{}\" instead of int.'.format(str(int_value), str(type(int_value))))\n\
    \          return str(int_value)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Calculate\
    \ regression metrics from csv', description='Calculates regression metrics.\\\
    n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n\
    \      _parser.add_argument(\"--true-values\", dest=\"true_values_path\", type=str,\
    \ required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predicted-values\"\
    , dest=\"predicted_values_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
    \      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
    \ nargs=6)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files\
    \ = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = calculate_regression_metrics_from_csv(**_parsed_args)\n\
    \n      _output_serializers = [\n          _serialize_int,\n          _serialize_float,\n\
    \          _serialize_float,\n          _serialize_float,\n          _serialize_float,\n\
    \          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file\
    \ in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n\
    \          except OSError:\n              pass\n          with open(output_file,\
    \ 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n\
    \    args:\n    - --true-values\n    - {inputPath: true_values}\n    - --predicted-values\n\
    \    - {inputPath: predicted_values}\n    - '----output-paths'\n    - {outputPath:\
    \ number_of_items}\n    - {outputPath: max_absolute_error}\n    - {outputPath:\
    \ mean_absolute_error}\n    - {outputPath: mean_squared_error}\n    - {outputPath:\
    \ root_mean_squared_error}\n    - {outputPath: metrics}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/2b16a9f1331ef05da18f8b2b53707b7aa0f26662/components/notebooks/Run_notebook_using_papermill/component.yaml',
    digest: 35814889779cab9450d13a2133a20190982e117b4f5ea677b9839cbe40fbb368}
  annotations:
    GitHub commit:
      sha: 2b16a9f1331ef05da18f8b2b53707b7aa0f26662
      html_url: https://github.com/Ark-kun/pipeline_components/commit/2b16a9f1331ef05da18f8b2b53707b7aa0f26662
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/2b16a9f1331ef05da18f8b2b53707b7aa0f26662
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T19:07:09Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T19:07:09Z'}
        message: |-
          Notebooks - Fixed the component.yaml syntax in the "Run_notebook_using_papermill" component

          Also fixed the default value.
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Run notebook using papermill\ndescription: |\n  Run Jupyter notebook\
    \ using papermill.\n  The notebook will receive the parameter values passed to\
    \ it as well as the INPUT_DATA_PATH and OUTPUT_DATA_PATH variables that will be\
    \ set to the input data path (if provided) and directory for the optional output\
    \ data.\ninputs:\n- {name: Notebook, type: JupyterNotebook, description: 'Notebook\
    \ to execute.'}\n- {name: Parameters, type: JsonObject, default: '{}', description:\
    \ 'Map with notebook paramater values.'}\n- {name: Packages to install, type:\
    \ JsonArray, default: '', description: 'Python packages to install'}\n- {name:\
    \ Input data, optional: true, description: 'Optional data that can be passed to\
    \ notebook. In notebook, the INPUT_DATA_PATH variable will point to the data (if\
    \ passed).'}\noutputs:\n- {name: Notebook, type: JupyterNotebook, description:\
    \ 'Executed notebook.'}\n- {name: Output data, description: 'Directory with any\
    \ output data. In notebook, the OUTPUT_DATA_PATH variable will point to this directory,\
    \ so that the notebook can write output data there.'}\nmetadata:\n  annotations:\n\
    \    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location:\
    \ 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/notebooks/Run_notebook_using_papermill/component.yaml'\n\
    implementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n\
    \    - -exc\n    - |\n      input_notebook_path=\"$0\"\n      output_notebook_path=\"\
    $1\"\n      arguments=\"$2\"\n      packages_to_install=\"$3\"\n      input_data_path=\"\
    $4\"\n      output_data_path=\"$5\"\n      mkdir -p \"$(dirname \"$output_notebook_path\"\
    )\"\n      mkdir -p \"$output_data_path\"\n\n      # Converting packages_to_install\
    \ from JSON to command-line arguments\n      packages_to_install=$(echo \"$packages_to_install\"\
    \ | sed -E -e 's/^\\[//' -e 's/]$//' -e 's/\",/\" /g' -e \"s/\\\"/'/g\")\n   \
    \   # Installing packages\n      sh -c \"python3 -m pip install --upgrade --quiet\
    \ jupyter papermill==2.2.0 ${packages_to_install}\"\n      # Running the notebook\
    \ using papermill\n      papermill --parameters_yaml \"$arguments\" --parameters\
    \ INPUT_DATA_PATH \"$input_data_path\" --parameters OUTPUT_DATA_PATH \"$output_data_path\"\
    \ \"$input_notebook_path\" \"$output_notebook_path\"\n      \n    - {inputPath:\
    \ Notebook}\n    - {outputPath: Notebook}\n    - {inputValue: Parameters}\n  \
    \  - if:\n        cond: {isPresent: Packages to install}\n        then: [{inputValue:\
    \ Packages to install}]\n        else: [\"[]\"]\n    - if:\n        cond: {isPresent:\
    \ Input data}\n        then: [{inputPath: Input data}]\n        else: [\"\"]\n\
    \    - {outputPath: Output data}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml',
    digest: 48505cdb67ebfb9fd91db372226ef31abd8c5d4654ba6a31593a41f31b9f8b9c}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Pandas Transform DataFrame in ApacheParquet format
    description: |-
      Transform DataFrame loaded from an ApacheParquet file.

          Inputs:
              table: DataFrame to transform.
              transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                  The DataFrame variable is called "df".
                  Examples:
                  - `df['prod'] = df['X'] * df['Y']`
                  - `df = df[['X', 'prod']]`
                  - `df.insert(0, "is_positive", df["X"] > 0)`

          Outputs:
              transformed_table: Transformed DataFrame.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: table, type: ApacheParquet}
    - {name: transform_code, type: PythonCode}
    outputs:
    - {name: transformed_table, type: ApacheParquet}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pandas==1.0.4' 'pyarrow==0.14.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' 'pyarrow==0.14.1'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def Pandas_Transform_DataFrame_in_ApacheParquet_format(
              table_path,
              transformed_table_path,
              transform_code,
          ):
              '''Transform DataFrame loaded from an ApacheParquet file.

              Inputs:
                  table: DataFrame to transform.
                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                      The DataFrame variable is called "df".
                      Examples:
                      - `df['prod'] = df['X'] * df['Y']`
                      - `df = df[['X', 'prod']]`
                      - `df.insert(0, "is_positive", df["X"] > 0)`

              Outputs:
                  transformed_table: Transformed DataFrame.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import pandas

              df = pandas.read_parquet(table_path)
              # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
              namespace = locals()
              exec(transform_code, namespace)
              namespace['df'].to_parquet(transformed_table_path)

          import argparse
          _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in ApacheParquet format', description='Transform DataFrame loaded from an ApacheParquet file.\n\n    Inputs:\n        table: DataFrame to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed DataFrame.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = Pandas_Transform_DataFrame_in_ApacheParquet_format(**_parsed_args)
        args:
        - --table
        - {inputPath: table}
        - --transform-code
        - {inputValue: transform_code}
        - --transformed-table
        - {outputPath: transformed_table}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml',
    digest: 777017bf588eba84bc1c685198931d3c295036ba492f28bdb0fc5f5bcad5d439}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Pandas Transform DataFrame in CSV format
    description: |-
      Transform DataFrame loaded from a CSV file.

          Inputs:
              table: Table to transform.
              transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                  The DataFrame variable is called "df".
                  Examples:
                  - `df['prod'] = df['X'] * df['Y']`
                  - `df = df[['X', 'prod']]`
                  - `df.insert(0, "is_positive", df["X"] > 0)`

          Outputs:
              transformed_table: Transformed table.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: table, type: CSV}
    - {name: transform_code, type: PythonCode}
    outputs:
    - {name: transformed_table, type: CSV}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pandas==1.0.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'pandas==1.0.4' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def Pandas_Transform_DataFrame_in_CSV_format(
              table_path,
              transformed_table_path,
              transform_code,
          ):
              '''Transform DataFrame loaded from a CSV file.

              Inputs:
                  table: Table to transform.
                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                      The DataFrame variable is called "df".
                      Examples:
                      - `df['prod'] = df['X'] * df['Y']`
                      - `df = df[['X', 'prod']]`
                      - `df.insert(0, "is_positive", df["X"] > 0)`

              Outputs:
                  transformed_table: Transformed table.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import pandas

              df = pandas.read_csv(
                  table_path,
              )
              # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
              namespace = locals()
              exec(transform_code, namespace)
              namespace['df'].to_csv(
                  transformed_table_path,
                  index=False,
              )

          import argparse
          _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
        args:
        - --table
        - {inputPath: table}
        - --transform-code
        - {inputValue: transform_code}
        - --transformed-table
        - {outputPath: transformed_table}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml',
    digest: 7edca2cd8212000a52dedf6c60aeb808fd022818ccb0ee64ddc85259b84e6186}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Convert to onnx from pytorch script module
    description: Creates fully-connected network in PyTorch ScriptModule format
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml'
    inputs:
    - {name: model, type: PyTorchScriptModule}
    - {name: list_of_input_shapes, type: JsonArray}
    outputs:
    - {name: converted_model, type: OnnxModel}
    implementation:
      container:
        image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def convert_to_onnx_from_pytorch_script_module(
              model_path,
              converted_model_path,
              list_of_input_shapes,
          ):
              '''Creates fully-connected network in PyTorch ScriptModule format'''
              import torch
              model = torch.jit.load(model_path)
              example_inputs = [
                  torch.ones(*input_shape)
                  for input_shape in list_of_input_shapes
              ]
              example_outputs = model.forward(*example_inputs)
              torch.onnx.export(
                  model=model,
                  args=example_inputs,
                  f=converted_model_path,
                  verbose=True,
                  training=torch.onnx.TrainingMode.EVAL,
                  example_outputs=example_outputs,
              )

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Convert to onnx from pytorch script module', description='Creates fully-connected network in PyTorch ScriptModule format')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--list-of-input-shapes", dest="list_of_input_shapes", type=json.loads, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--converted-model", dest="converted_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = convert_to_onnx_from_pytorch_script_module(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --list-of-input-shapes
        - {inputValue: list_of_input_shapes}
        - --converted-model
        - {outputPath: converted_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Create_fully_connected_network/component.yaml',
    digest: 6074655c1dddbafcd7a4d618ba1d2489fe3f53444399f1450a23c4acc69280c5}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create fully connected pytorch network
    description: Creates fully-connected network in PyTorch ScriptModule format
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_fully_connected_network/component.yaml'
    inputs:
    - {name: layer_sizes, type: JsonArray}
    - {name: activation_name, type: String, default: relu, optional: true}
    - {name: random_seed, type: Integer, default: '0', optional: true}
    outputs:
    - {name: network, type: PyTorchScriptModule}
    implementation:
      container:
        image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
        command:
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def create_fully_connected_pytorch_network(
              layer_sizes,
              network_path,
              activation_name = 'relu',
              random_seed = 0,
          ):
              '''Creates fully-connected network in PyTorch ScriptModule format'''
              import torch
              torch.manual_seed(random_seed)

              activation = getattr(torch, activation_name, None) or getattr(torch.nn.functional, activation_name, None)
              if not activation:
                  raise ValueError(f'Activation "{activation_name}" was not found.')

              class ActivationLayer(torch.nn.Module):
                  def forward(self, input):
                      return activation(input)

              layers = []
              for layer_idx in range(len(layer_sizes) - 1):
                  layer = torch.nn.Linear(layer_sizes[layer_idx], layer_sizes[layer_idx + 1])
                  layers.append(layer)
                  if layer_idx < len(layer_sizes) - 2:
                      layers.append(ActivationLayer())

              network = torch.nn.Sequential(*layers)
              script_module = torch.jit.script(network)
              print(script_module)
              script_module.save(network_path)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Create fully connected pytorch network', description='Creates fully-connected network in PyTorch ScriptModule format')
          _parser.add_argument("--layer-sizes", dest="layer_sizes", type=json.loads, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--activation-name", dest="activation_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--network", dest="network_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = create_fully_connected_pytorch_network(**_parsed_args)
        args:
        - --layer-sizes
        - {inputValue: layer_sizes}
        - if:
            cond: {isPresent: activation_name}
            then:
            - --activation-name
            - {inputValue: activation_name}
        - if:
            cond: {isPresent: random_seed}
            then:
            - --random-seed
            - {inputValue: random_seed}
        - --network
        - {outputPath: network}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Create_PyTorch_Model_Archive/component.yaml',
    digest: 5a436743e390813aba8f740173a9717edd03cebb90328677d181bbfea3544108}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Create PyTorch Model Archive\ninputs:\n- {name: Model, type: PyTorchScriptModule}\n\
    - {name: Model name, type: String, default: model}\n- {name: Model version, type:\
    \ String, default: \"1.0\"}\n- {name: Handler, type: PythonCode, description:\
    \ \"See https://github.com/pytorch/serve/blob/master/docs/custom_service.md\"\
    }\noutputs:\n- {name: Model archive, type: PyTorchModelArchive}\nmetadata:\n \
    \ annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location:\
    \ 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_PyTorch_Model_Archive/component.yaml'\n\
    implementation:\n  container:\n    image: pytorch/torchserve:0.3.0-cpu\n    command:\n\
    \    - bash\n    - -exc\n    - |\n      model_path=$0\n      handler_path=$1\n\
    \      model_name=$2\n      model_version=$3\n      output_model_archive_path=$4\n\
    \      \n      mkdir -p \"$(dirname \"$output_model_archive_path\")\"\n\n    \
    \  # torch-model-archiver needs the handler to have .py extension\n      cp \"\
    $handler_path\" handler.py\n      torch-model-archiver --model-name \"$model_name\"\
    \ --version \"$model_version\" --serialized-file \"$model_path\" --handler handler.py\n\
    \      \n      # torch-model-archiver does not allow specifying the output path,\
    \ but always writes to \"${model_name}.<format>\"\n      expected_model_archive_path=\"\
    ${model_name}.mar\"\n      mv \"$expected_model_archive_path\" \"$output_model_archive_path\"\
    \n\n    - {inputPath: Model}\n    - {inputPath: Handler}\n    - {inputValue: Model\
    \ name}\n    - {inputValue: Model version}\n    - {outputPath: Model archive}"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml',
    digest: 40f3185eb61e9727f41a4e0c05dd3d3b44bd802aa0f378cfc31756560033949a}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Train pytorch model from csv
    description: Trains PyTorch model
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml'
    inputs:
    - {name: model, type: PyTorchScriptModule}
    - {name: training_data, type: CSV}
    - {name: label_column_name, type: String}
    - {name: loss_function_name, type: String, default: mse_loss, optional: true}
    - {name: number_of_epochs, type: Integer, default: '1', optional: true}
    - {name: learning_rate, type: Float, default: '0.1', optional: true}
    - {name: optimizer_name, type: String, default: Adadelta, optional: true}
    - {name: optimizer_parameters, type: JsonObject, optional: true}
    - {name: batch_size, type: Integer, default: '32', optional: true}
    - {name: batch_log_interval, type: Integer, default: '100', optional: true}
    - {name: random_seed, type: Integer, default: '0', optional: true}
    outputs:
    - {name: trained_model, type: PyTorchScriptModule}
    implementation:
      container:
        image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'pandas==1.1.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location 'pandas==1.1.5' --user) && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp)
          printf "%s" "$0" > "$program_path"
          python3 -u "$program_path" "$@"
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def train_pytorch_model_from_csv(
              model_path,
              training_data_path,
              trained_model_path,
              label_column_name,
              loss_function_name = 'mse_loss',
              number_of_epochs = 1,
              learning_rate = 0.1,
              optimizer_name = 'Adadelta',
              optimizer_parameters = None,
              batch_size = 32,
              batch_log_interval = 100,
              random_seed = 0,
          ):
              '''Trains PyTorch model'''
              import pandas
              import torch

              torch.manual_seed(random_seed)

              use_cuda = torch.cuda.is_available()
              device = torch.device("cuda" if use_cuda else "cpu")

              model = torch.jit.load(model_path)
              model.to(device)
              model.train()

              optimizer_class = getattr(torch.optim, optimizer_name, None)
              if not optimizer_class:
                  raise ValueError(f'Optimizer "{optimizer_name}" was not found.')

              optimizer_parameters = optimizer_parameters or {}
              optimizer_parameters['lr'] = learning_rate
              optimizer = optimizer_class(model.parameters(), **optimizer_parameters)

              loss_function = getattr(torch, loss_function_name, None) or getattr(torch.nn, loss_function_name, None) or getattr(torch.nn.functional, loss_function_name, None)
              if not loss_function:
                  raise ValueError(f'Loss function "{loss_function_name}" was not found.')

              class CsvDataset(torch.utils.data.Dataset):

                  def __init__(self, file_path, label_column_name, drop_nan_clumns_or_rows = 'columns'):
                      dataframe = pandas.read_csv(file_path)
                      # Preventing error: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object
                      if drop_nan_clumns_or_rows == 'columns':
                          non_nan_data = dataframe.dropna(axis='columns')
                          removed_columns = set(dataframe.columns) - set(non_nan_data.columns)
                          if removed_columns:
                              print('Skipping columns with NaNs: ' + str(removed_columns))
                          dataframe = non_nan_data
                      if drop_nan_clumns_or_rows == 'rows':
                          non_nan_data = dataframe.dropna(axis='index')
                          number_of_removed_rows = len(dataframe) - len(non_nan_data)
                          if number_of_removed_rows:
                              print(f'Skipped {number_of_removed_rows} rows with NaNs.')
                          dataframe = non_nan_data
                      numerical_data = dataframe.select_dtypes(include='number')
                      non_numerical_data = dataframe.select_dtypes(exclude='number')
                      if not non_numerical_data.empty:
                          print('Skipping non-number columns:')
                          print(non_numerical_data.dtypes)
                      self._dataframe = dataframe
                      self.labels = numerical_data[[label_column_name]]
                      self.features = numerical_data.drop(columns=[label_column_name])

                  def __len__(self):
                      return len(self._dataframe)

                  def __getitem__(self, index):
                      return [self.features.loc[index].to_numpy(dtype='float32'), self.labels.loc[index].to_numpy(dtype='float32')]

              dataset = CsvDataset(
                  file_path=training_data_path,
                  label_column_name=label_column_name,
              )
              train_loader = torch.utils.data.DataLoader(
                  dataset=dataset,
                  batch_size=batch_size,
                  shuffle=True,
              )

              last_full_batch_loss = None
              for epoch in range(1, number_of_epochs + 1):
                  for batch_idx, (data, target) in enumerate(train_loader):
                      data, target = data.to(device), target.to(device)
                      optimizer.zero_grad()
                      output = model(data)
                      loss = loss_function(output, target)
                      loss.backward()
                      optimizer.step()
                      if len(data) == batch_size:
                          last_full_batch_loss = loss.item()
                      if batch_idx % batch_log_interval == 0:
                          print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                              epoch, batch_idx * len(data), len(train_loader.dataset),
                              100. * batch_idx / len(train_loader), loss.item()))
                  print(f'Training epoch {epoch} completed. Last full batch loss: {last_full_batch_loss:.6f}')

              # print(optimizer.state_dict())
              model.save(trained_model_path)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Train pytorch model from csv', description='Trains PyTorch model')
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--loss-function-name", dest="loss_function_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--number-of-epochs", dest="number_of_epochs", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimizer-name", dest="optimizer_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--optimizer-parameters", dest="optimizer_parameters", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--batch-log-interval", dest="batch_log_interval", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--trained-model", dest="trained_model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = train_pytorch_model_from_csv(**_parsed_args)
        args:
        - --model
        - {inputPath: model}
        - --training-data
        - {inputPath: training_data}
        - --label-column-name
        - {inputValue: label_column_name}
        - if:
            cond: {isPresent: loss_function_name}
            then:
            - --loss-function-name
            - {inputValue: loss_function_name}
        - if:
            cond: {isPresent: number_of_epochs}
            then:
            - --number-of-epochs
            - {inputValue: number_of_epochs}
        - if:
            cond: {isPresent: learning_rate}
            then:
            - --learning-rate
            - {inputValue: learning_rate}
        - if:
            cond: {isPresent: optimizer_name}
            then:
            - --optimizer-name
            - {inputValue: optimizer_name}
        - if:
            cond: {isPresent: optimizer_parameters}
            then:
            - --optimizer-parameters
            - {inputValue: optimizer_parameters}
        - if:
            cond: {isPresent: batch_size}
            then:
            - --batch-size
            - {inputValue: batch_size}
        - if:
            cond: {isPresent: batch_log_interval}
            then:
            - --batch-log-interval
            - {inputValue: batch_log_interval}
        - if:
            cond: {isPresent: random_seed}
            then:
            - --random-seed
            - {inputValue: random_seed}
        - --trained-model
        - {outputPath: trained_model}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/sample/C%23_script/component.yaml',
    digest: 5b7c2177d98493e20df37d35d347d0e012704d2961d6233e63b7e8b8d340d79c}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Filter text
    inputs:
    - {name: Text}
    - {name: Pattern, default: '.*'}
    outputs:
    - {name: Filtered text}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/sample/C#_script/component.yaml'
    implementation:
      container:
        image: mcr.microsoft.com/dotnet/sdk:5.0
        command:
        - sh
        - -ec
        - |
          dotnet tool install dotnet-script --tool-path /usr/bin
          "$0" "$@"
        - dotnet
        - script
        - eval
        - |
          string textPath = Args[0];
          string pattern = Args[1];
          string filteredTextPath = Args[2];

          var regex = new System.Text.RegularExpressions.Regex(pattern);
          Directory.CreateDirectory(Path.GetDirectoryName(filteredTextPath));
          using(var writer = new StreamWriter(filteredTextPath)) {
              foreach (var line in File.ReadLines(textPath)) {
                  if (regex.IsMatch(line)) {
                      writer.WriteLine(line);
                  }
              }
          }
        - --
        - {inputPath: Text}
        - {inputValue: Pattern}
        - {outputPath: Filtered text}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/sample/keras/train_classifier/component.yaml',
    digest: 1b2acf5ff1214e59ca89511122b02172f70ee064be44b978b939d1bf43bed69d}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Keras - Train classifier
    description: Trains classifier using Keras sequential model
    inputs:
      - {name: training_set_features_path, type: {GcsPath: {data_type: TSV}}, description: 'Local or GCS path to the training set features table.'}
      - {name: training_set_labels_path, type: {GcsPath: {data_type: TSV}}, description: 'Local or GCS path to the training set labels (each label is a class index from 0 to num-classes - 1).'}
      - {name: output_model_uri, type: {GcsPath: {data_type: Keras model}}, description: 'Local or GCS path specifying where to save the trained model. The model (topology + weights + optimizer state) is saved in HDF5 format and can be loaded back by calling keras.models.load_model'} #Remove GcsUri and move to outputs once artifact passing support is checked in.
      - {name: model_config, type: {GcsPath: {data_type: Keras model config json}}, description: 'JSON string containing the serialized model structure. Can be obtained by calling model.to_json() on a Keras model.'}
      - {name: number_of_classes, type: Integer, description: 'Number of classifier classes.'}
      - {name: number_of_epochs, type: Integer, default: '100', description: 'Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided.'}
      - {name: batch_size, type: Integer, default: '32', description: 'Number of samples per gradient update.'}
    outputs:
      - {name: output_model_uri, type: {GcsPath: {data_type: Keras model}}, description: 'GCS path where the trained model has been saved. The model (topology + weights + optimizer state) is saved in HDF5 format and can be loaded back by calling keras.models.load_model'} #Remove GcsUri and make it a proper output once artifact passing support is checked in.
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/sample/keras/train_classifier/component.yaml'
    implementation:
      container:
        image: gcr.io/ml-pipeline/sample/keras/train_classifier
        command: [python3, /pipelines/component/src/train.py]
        args: [
          --training-set-features-path, {inputValue: training_set_features_path},
          --training-set-labels-path, {inputValue: training_set_labels_path},
          --output-model-path, {inputValue: output_model_uri},
          --model-config-json, {inputValue: model_config},
          --num-classes, {inputValue: number_of_classes},
          --num-epochs, {inputValue: number_of_epochs},
          --batch-size, {inputValue: batch_size},

          --output-model-path-file, {outputPath: output_model_uri},
        ]
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/sample/Python_script/component.yaml',
    digest: b1f639ff01ae48a2792618ea8d081e9b4c8380034ca28d0e3fcaf379b1460fb9}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Filter text
    inputs:
    - {name: Text}
    - {name: Pattern, default: '.*'}
    outputs:
    - {name: Filtered text}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/sample/Python_script/component.yaml'
    implementation:
      container:
        image: python:3.8
        command:
        - sh
        - -ec
        - |
          # This is how additional packages can be installed dynamically
          python3 -m pip install pip six
          # Run the rest of the command after installing the packages.
          "$0" "$@"
        - python3
        - -u  # Auto-flush. We want the logs to appear in the console immediately.
        - -c  # Inline scripts are easy, but have size limitaions and the error traces do not show source lines.
        - |
          import os
          import re
          import sys

          text_path = sys.argv[1]
          pattern = sys.argv[2]
          filtered_text_path = sys.argv[3]

          regex = re.compile(pattern)

          os.makedirs(os.path.dirname(filtered_text_path), exist_ok=True)
          with open(text_path, 'r') as reader:
              with open(filtered_text_path, 'w') as writer:
                  for line in reader:
                      if regex.search(line):
                          writer.write(line)
        - {inputPath: Text}
        - {inputValue: Pattern}
        - {outputPath: Filtered text}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/sample/R_script/component.yaml',
    digest: 5b83bd22416a509ac7a165a18ddf73831635f9c49319ff8494aeb429796607ef}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Filter text
    inputs:
    - {name: Text}
    - {name: Pattern, default: '.*'}
    outputs:
    - {name: Filtered text}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/sample/R_script/component.yaml'
    implementation:
      container:
        image: r-base:4.0.2
        command:
        - Rscript
        - -e
        - |
          args <- commandArgs(trailingOnly = TRUE)
          textPath <- args[1]
          pattern <- args[2]
          filteredTextPath <- args[3]

          dir.create(dirname(filteredTextPath), showWarnings = FALSE, recursive = TRUE)

          inputFile = file(textPath, "r")
          outputFile = file(filteredTextPath, "w")
          while ( TRUE ) {
              lines = readLines(inputFile, n = 1)
              if ( length(lines) == 0 ) {
                  break
              }
              if ( grepl(pattern = pattern, lines) ) {
                  writeLines(lines, outputFile)
              }
          }
          close(outputFile)
          close(inputFile)
        - {inputPath: Text}
        - {inputValue: Pattern}
        - {outputPath: Filtered text}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/sample/Shell_script/component.yaml',
    digest: 05e3bb6c4b929c85c2061067d66b2fbe34d5f2d79ba73886dfb6c33547374dea}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: "name: Filter text using shell and grep\ninputs:\n- {name: Text}\n- {name:\
    \ Pattern, default: '.*'}\noutputs:\n- {name: Filtered text}\nmetadata:\n  annotations:\n\
    \    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location:\
    \ 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/sample/Shell_script/component.yaml'\n\
    implementation:\n  container:\n    image: alpine\n    command:\n    - sh\n   \
    \ - -ec\n    - |\n      text_path=$0\n      pattern=$1\n      filtered_text_path=$2\n\
    \      mkdir -p \"$(dirname \"$filtered_text_path\")\"\n      \n      grep \"\
    $pattern\" < \"$text_path\" > \"$filtered_text_path\"\n    - {inputPath: Text}\n\
    \    - {inputValue: Pattern}\n    - {outputPath: Filtered text}\n"
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/tables/Remove_header/component.yaml',
    digest: 5e8bc75d0817daeaa25e15ae866a7483946fcfac7ce1d5f998a2c77d6fa1836b}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Remove header
    description: Remove the header line from CSV and TSV data (unconditionally)
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tables/Remove_header/component.yaml'
    inputs:
    - name: table
    outputs:
    - name: table
    implementation:
      container:
        image: alpine
        command:
        - sh
        - -exc
        - |
          mkdir -p "$(dirname "$1")"
          tail -n +2 <"$0" >"$1"
        - inputPath: table
        - outputPath: table
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/tensorflow/tensorboard/prepare_tensorboard/component.yaml',
    digest: da8abd8014361176885fe2c0d742f54036c3c7536ad18e5d173d7ac4d0391125}
  annotations:
    GitHub commit:
      sha: 6210648f30b2b3a8c01cc10be338da98300efb6b
      html_url: https://github.com/Ark-kun/pipeline_components/commit/6210648f30b2b3a8c01cc10be338da98300efb6b
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/6210648f30b2b3a8c01cc10be338da98300efb6b
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        committer: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-08-03T09:07:03Z'}
        message: Switched from echo to printf to avoid trailing newlines and possible
          data mangling
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Create Tensorboard visualization
    description: |
      Pre-creates Tensorboard visualization for a given Log dir URI.
      This way the Tensorboard can be viewed before the training completes.
      The output Log dir URI should be passed to a trainer component that will write Tensorboard logs to that directory.
    inputs:
    - {name: Log dir URI}
    outputs:
    - {name: mlpipeline-ui-metadata, type: kfp.v1.ui-metadata}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/tensorboard/prepare_tensorboard/component.yaml'
    implementation:
      container:
        image: alpine
        command:
        - sh
        - -ex
        - -c
        - |
          log_dir="$0"
          output_log_dir_path="$1"
          output_metadata_path="$2"
          mkdir -p "$(dirname "$output_log_dir_path")"
          mkdir -p "$(dirname "$output_metadata_path")"
          printf "%s" "$log_dir" > "$output_log_dir_path"
          echo '
              {
                "outputs" : [{
                  "type": "tensorboard",
                  "source": "'"$log_dir"'"
                }]
              }
          ' >"$output_metadata_path"
        - {inputValue: Log dir URI}
        - {outputPath: Log dir URI}
        - {outputPath: MLPipeline UI Metadata}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/web/Download/component.yaml',
    digest: 2f61f2edf713f214934bd286791877a1a3a37f31a4de4368b90e3b76743f1523}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Download data
    inputs:
    - {name: Url, type: URI}
    - {name: curl options, type: string, default: '--location', description: 'Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html'}
    outputs:
    - {name: Data}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml'
    implementation:
      container:
        # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
        image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
        command:
        - sh
        - -exc
        - |
          url="$0"
          output_path="$1"
          curl_options="$2"

          mkdir -p "$(dirname "$output_path")"
          curl --get "$url" --output "$output_path" $curl_options
        - inputValue: Url
        - outputPath: Data
        - inputValue: curl options
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Cross_validation_for_regression/from_CSV/component.yaml',
    digest: c41d25fbb669f35b31288ad449927262d4ccc0060277822855f4d96403552ec8}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost 5 fold cross validation for regression
    inputs:
    - {name: data, type: CSV}
    - {name: label_column, type: Integer, default: '0', optional: true}
    - {name: objective, type: String, default: 'reg:squarederror', optional: true}
    - {name: num_iterations, type: Integer, default: '200', optional: true}
    outputs:
    - {name: mean_absolute_error, type: Float}
    - {name: mean_squared_error, type: Float}
    - {name: root_mean_squared_error, type: Float}
    - {name: metrics, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Cross_validation_for_regression/from_CSV/component.yaml'
    implementation:
      graph:
        tasks:
          Split table into folds:
            componentRef: {digest: 9956223bcecc7294ca1afac39b60ada4a935a571d817c3dfbf2ea4a211afe3d1,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/e9b4b29b22a5120daf95b581b0392cd461a906f0/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'}
            arguments:
              table:
                graphInput: {inputName: data}
          Xgboost train:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_1, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_1, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_1, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format, type: CSV}
          Calculate regression metrics from csv:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict, type: Text}
          Xgboost train 2:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_2, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 2:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_2, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 2, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format 2:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_2, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 2:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 2, type: CSV}
          Calculate regression metrics from csv 2:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 2}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 2, type: Text}
          Xgboost train 3:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_3, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 3:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_3, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 3, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format 3:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_3, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 3:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 3, type: CSV}
          Calculate regression metrics from csv 3:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 3}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 3, type: Text}
          Xgboost train 4:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_4, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 4:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_4, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 4, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format 4:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_4, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 4:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 4, type: CSV}
          Calculate regression metrics from csv 4:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 4}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 4, type: Text}
          Xgboost train 5:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_5, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 5:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_5, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 5, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format 5:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_5, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 5:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 5, type: CSV}
          Calculate regression metrics from csv 5:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 5}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 5, type: Text}
          Aggregate regression metrics from csv:
            componentRef: {digest: 3e128130521eff8d43764f3dcb037316cdd6490ad2878df5adef416f7c2f3c19,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7ea9363fe201918d419fecdc00d1275e657ff712/components/ml_metrics/Aggregate_regression_metrics/component.yaml'}
            arguments:
              metrics_1:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv, type: JsonObject}
              metrics_2:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 2, type: JsonObject}
              metrics_3:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 3, type: JsonObject}
              metrics_4:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 4, type: JsonObject}
              metrics_5:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 5, type: JsonObject}
        outputValues:
          mean_absolute_error:
            taskOutput: {outputName: mean_absolute_error, taskId: Aggregate regression
                metrics from csv, type: Float}
          mean_squared_error:
            taskOutput: {outputName: mean_squared_error, taskId: Aggregate regression
                metrics from csv, type: Float}
          root_mean_squared_error:
            taskOutput: {outputName: root_mean_squared_error, taskId: Aggregate regression
                metrics from csv, type: Float}
          metrics:
            taskOutput: {outputName: metrics, taskId: Aggregate regression metrics from
                csv, type: JsonObject}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/component.yaml',
    digest: 078a0ecebb80c2743b13db45aeb3c26cde4e9b0c500bd429c7db1cd2bc565d9a}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost predict
    description: |-
      Make predictions using a trained XGBoost model.

          Args:
              data_path: Path for the feature data in CSV format.
              model_path: Path for the trained model in binary XGBoost format.
              predictions_path: Output path for the predictions.
              label_column: Column containing the label data.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: CSV}
    - {name: model, type: XGBoostModel}
    - {name: label_column, type: Integer, optional: true}
    outputs:
    - {name: predictions, type: Predictions}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def xgboost_predict(
              data_path,  # Also supports LibSVM
              model_path,
              predictions_path,
              label_column = None,
          ):
              '''Make predictions using a trained XGBoost model.

              Args:
                  data_path: Path for the feature data in CSV format.
                  model_path: Path for the trained model in binary XGBoost format.
                  predictions_path: Output path for the predictions.
                  label_column: Column containing the label data.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pathlib import Path

              import numpy
              import pandas
              import xgboost

              df = pandas.read_csv(
                  data_path,
              )

              if label_column is not None:
                  df = df.drop(columns=[df.columns[label_column]])

              testing_data = xgboost.DMatrix(
                  data=df,
              )

              model = xgboost.Booster(model_file=model_path)

              predictions = model.predict(testing_data)

              Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
              numpy.savetxt(predictions_path, predictions)

          import argparse
          _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in CSV format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column: Column containing the label data.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = xgboost_predict(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --model
        - {inputPath: model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - --predictions
        - {outputPath: predictions}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/from_ApacheParquet/component.yaml',
    digest: 5c9039ce4cc52f37ea115a998cde6afaa1e53975e72967d622f0cd635f66cec7}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost predict
    description: |-
      Make predictions using a trained XGBoost model.

          Args:
              data_path: Path for the feature data in Apache Parquet format.
              model_path: Path for the trained model in binary XGBoost format.
              predictions_path: Output path for the predictions.
              label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: data, type: ApacheParquet}
    - {name: model, type: XGBoostModel}
    - {name: label_column_name, type: String, optional: true}
    outputs:
    - {name: predictions, type: Predictions}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/from_ApacheParquet/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'xgboost==1.1.1' 'pandas==1.0.5' 'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5'
          'pyarrow==0.17.1' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def xgboost_predict(
              data_path,
              model_path,
              predictions_path,
              label_column_name = None,
          ):
              '''Make predictions using a trained XGBoost model.

              Args:
                  data_path: Path for the feature data in Apache Parquet format.
                  model_path: Path for the trained model in binary XGBoost format.
                  predictions_path: Output path for the predictions.
                  label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              from pathlib import Path

              import numpy
              import pandas
              import xgboost

              # Loading data
              df = pandas.read_parquet(data_path)
              if label_column_name:
                  df = df.drop(columns=[label_column_name])

              evaluation_data = xgboost.DMatrix(
                  data=df,
              )

              # Training
              model = xgboost.Booster(model_file=model_path)

              predictions = model.predict(evaluation_data)

              Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
              numpy.savetxt(predictions_path, predictions)

          import argparse
          _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in Apache Parquet format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = xgboost_predict(**_parsed_args)
        args:
        - --data
        - {inputPath: data}
        - --model
        - {inputPath: model}
        - if:
            cond: {isPresent: label_column_name}
            then:
            - --label-column-name
            - {inputValue: label_column_name}
        - --predictions
        - {outputPath: predictions}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/component.yaml',
    digest: 58d279448fda37f1ad85d39751b987bcecaa950281287fdac756315d186f03a3}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost train
    description: |-
      Train an XGBoost model.

          Args:
              training_data_path: Path for the training data in CSV format.
              model_path: Output path for the trained model in binary XGBoost format.
              model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
              starting_model_path: Path for the existing trained model to start from.
              label_column: Column containing the label data.
              num_boost_rounds: Number of boosting iterations.
              booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
              objective: The learning task and the corresponding learning objective.
                  See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                  The most common values are:
                  "reg:squarederror" - Regression with squared loss (default).
                  "reg:logistic" - Logistic regression.
                  "binary:logistic" - Logistic regression for binary classification, output probability.
                  "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                  "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                  "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: training_data, type: CSV}
    - {name: starting_model, type: XGBoostModel, optional: true}
    - {name: label_column, type: Integer, default: '0', optional: true}
    - {name: num_iterations, type: Integer, default: '10', optional: true}
    - {name: booster_params, type: JsonObject, optional: true}
    - {name: objective, type: String, default: 'reg:squarederror', optional: true}
    - {name: booster, type: String, default: gbtree, optional: true}
    - {name: learning_rate, type: Float, default: '0.3', optional: true}
    - {name: min_split_loss, type: Float, default: '0', optional: true}
    - {name: max_depth, type: Integer, default: '6', optional: true}
    outputs:
    - {name: model, type: XGBoostModel}
    - {name: model_config, type: XGBoostModelConfig}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5'
          --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def xgboost_train(
              training_data_path,  # Also supports LibSVM
              model_path,
              model_config_path,
              starting_model_path = None,

              label_column = 0,
              num_iterations = 10,
              booster_params = None,

              # Booster parameters
              objective = 'reg:squarederror',
              booster = 'gbtree',
              learning_rate = 0.3,
              min_split_loss = 0,
              max_depth = 6,
          ):
              '''Train an XGBoost model.

              Args:
                  training_data_path: Path for the training data in CSV format.
                  model_path: Output path for the trained model in binary XGBoost format.
                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column: Column containing the label data.
                  num_boost_rounds: Number of boosting iterations.
                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                  objective: The learning task and the corresponding learning objective.
                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                      The most common values are:
                      "reg:squarederror" - Regression with squared loss (default).
                      "reg:logistic" - Logistic regression.
                      "binary:logistic" - Logistic regression for binary classification, output probability.
                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import pandas
              import xgboost

              df = pandas.read_csv(
                  training_data_path,
              )

              training_data = xgboost.DMatrix(
                  data=df.drop(columns=[df.columns[label_column]]),
                  label=df[df.columns[label_column]],
              )

              booster_params = booster_params or {}
              booster_params.setdefault('objective', objective)
              booster_params.setdefault('booster', booster)
              booster_params.setdefault('learning_rate', learning_rate)
              booster_params.setdefault('min_split_loss', min_split_loss)
              booster_params.setdefault('max_depth', max_depth)

              starting_model = None
              if starting_model_path:
                  starting_model = xgboost.Booster(model_file=starting_model_path)

              model = xgboost.train(
                  params=booster_params,
                  dtrain=training_data,
                  num_boost_round=num_iterations,
                  xgb_model=starting_model
              )

              # Saving the model in binary format
              model.save_model(model_path)

              model_config_str = model.save_config()
              with open(model_config_path, 'w') as model_config_file:
                  model_config_file.write(model_config_str)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = xgboost_train(**_parsed_args)
        args:
        - --training-data
        - {inputPath: training_data}
        - if:
            cond: {isPresent: starting_model}
            then:
            - --starting-model
            - {inputPath: starting_model}
        - if:
            cond: {isPresent: label_column}
            then:
            - --label-column
            - {inputValue: label_column}
        - if:
            cond: {isPresent: num_iterations}
            then:
            - --num-iterations
            - {inputValue: num_iterations}
        - if:
            cond: {isPresent: booster_params}
            then:
            - --booster-params
            - {inputValue: booster_params}
        - if:
            cond: {isPresent: objective}
            then:
            - --objective
            - {inputValue: objective}
        - if:
            cond: {isPresent: booster}
            then:
            - --booster
            - {inputValue: booster}
        - if:
            cond: {isPresent: learning_rate}
            then:
            - --learning-rate
            - {inputValue: learning_rate}
        - if:
            cond: {isPresent: min_split_loss}
            then:
            - --min-split-loss
            - {inputValue: min_split_loss}
        - if:
            cond: {isPresent: max_depth}
            then:
            - --max-depth
            - {inputValue: max_depth}
        - --model
        - {outputPath: model}
        - --model-config
        - {outputPath: model_config}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/from_ApacheParquet/component.yaml',
    digest: 671fa7e8e0db10d7d060f2126078104ea382406ceec568f6197e5cfa7b06d4ef}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost train
    description: |-
      Train an XGBoost model.

          Args:
              training_data_path: Path for the training data in Apache Parquet format.
              model_path: Output path for the trained model in binary XGBoost format.
              model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
              starting_model_path: Path for the existing trained model to start from.
              label_column_name: Name of the column containing the label data.
              num_boost_rounds: Number of boosting iterations.
              booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
              objective: The learning task and the corresponding learning objective.
                  See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                  The most common values are:
                  "reg:squarederror" - Regression with squared loss (default).
                  "reg:logistic" - Logistic regression.
                  "binary:logistic" - Logistic regression for binary classification, output probability.
                  "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                  "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                  "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

          Annotations:
              author: Alexey Volkov <alexey.volkov@ark-kun.com>
    inputs:
    - {name: training_data, type: ApacheParquet}
    - {name: label_column_name, type: String}
    - {name: starting_model, type: XGBoostModel, optional: true}
    - {name: num_iterations, type: Integer, default: '10', optional: true}
    - {name: booster_params, type: JsonObject, optional: true}
    - {name: objective, type: String, default: 'reg:squarederror', optional: true}
    - {name: booster, type: String, default: gbtree, optional: true}
    - {name: learning_rate, type: Float, default: '0.3', optional: true}
    - {name: min_split_loss, type: Float, default: '0', optional: true}
    - {name: max_depth, type: Integer, default: '6', optional: true}
    outputs:
    - {name: model, type: XGBoostModel}
    - {name: model_config, type: XGBoostModelConfig}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/from_ApacheParquet/component.yaml'
    implementation:
      container:
        image: python:3.7
        command:
        - sh
        - -c
        - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          'xgboost==1.1.1' 'pandas==1.0.5' 'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5'
          'pyarrow==0.17.1' --user) && "$0" "$@"
        - python3
        - -u
        - -c
        - |
          def _make_parent_dirs_and_return_path(file_path: str):
              import os
              os.makedirs(os.path.dirname(file_path), exist_ok=True)
              return file_path

          def xgboost_train(
              training_data_path,
              model_path,
              model_config_path,
              label_column_name,

              starting_model_path = None,

              num_iterations = 10,
              booster_params = None,

              # Booster parameters
              objective = 'reg:squarederror',
              booster = 'gbtree',
              learning_rate = 0.3,
              min_split_loss = 0,
              max_depth = 6,
          ):
              '''Train an XGBoost model.

              Args:
                  training_data_path: Path for the training data in Apache Parquet format.
                  model_path: Output path for the trained model in binary XGBoost format.
                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                  starting_model_path: Path for the existing trained model to start from.
                  label_column_name: Name of the column containing the label data.
                  num_boost_rounds: Number of boosting iterations.
                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                  objective: The learning task and the corresponding learning objective.
                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                      The most common values are:
                      "reg:squarederror" - Regression with squared loss (default).
                      "reg:logistic" - Logistic regression.
                      "binary:logistic" - Logistic regression for binary classification, output probability.
                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

              Annotations:
                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
              '''
              import pandas
              import xgboost

              # Loading data
              df = pandas.read_parquet(training_data_path)
              training_data = xgboost.DMatrix(
                  data=df.drop(columns=[label_column_name]),
                  label=df[[label_column_name]],
              )
              # Training
              booster_params = booster_params or {}
              booster_params.setdefault('objective', objective)
              booster_params.setdefault('booster', booster)
              booster_params.setdefault('learning_rate', learning_rate)
              booster_params.setdefault('min_split_loss', min_split_loss)
              booster_params.setdefault('max_depth', max_depth)

              starting_model = None
              if starting_model_path:
                  starting_model = xgboost.Booster(model_file=starting_model_path)

              model = xgboost.train(
                  params=booster_params,
                  dtrain=training_data,
                  num_boost_round=num_iterations,
                  xgb_model=starting_model
              )

              # Saving the model in binary format
              model.save_model(model_path)

              model_config_str = model.save_config()
              with open(model_config_path, 'w') as model_config_file:
                  model_config_file.write(model_config_str)

          import json
          import argparse
          _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in Apache Parquet format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column_name: Name of the column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
          _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
          _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
          _parsed_args = vars(_parser.parse_args())

          _outputs = xgboost_train(**_parsed_args)
        args:
        - --training-data
        - {inputPath: training_data}
        - --label-column-name
        - {inputValue: label_column_name}
        - if:
            cond: {isPresent: starting_model}
            then:
            - --starting-model
            - {inputPath: starting_model}
        - if:
            cond: {isPresent: num_iterations}
            then:
            - --num-iterations
            - {inputValue: num_iterations}
        - if:
            cond: {isPresent: booster_params}
            then:
            - --booster-params
            - {inputValue: booster_params}
        - if:
            cond: {isPresent: objective}
            then:
            - --objective
            - {inputValue: objective}
        - if:
            cond: {isPresent: booster}
            then:
            - --booster
            - {inputValue: booster}
        - if:
            cond: {isPresent: learning_rate}
            then:
            - --learning-rate
            - {inputValue: learning_rate}
        - if:
            cond: {isPresent: min_split_loss}
            then:
            - --min-split-loss
            - {inputValue: min_split_loss}
        - if:
            cond: {isPresent: max_depth}
            then:
            - --max-depth
            - {inputValue: max_depth}
        - --model
        - {outputPath: model}
        - --model-config
        - {outputPath: model_config}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train_and_cross-validate_regression/from_CSV/component.yaml',
    digest: 2d4a01eaf55558dc107ed32e7c1f15608360c98ced69f99d5e637cbacdb07fba}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost train and cv regression on csv
    inputs:
    - {name: data, type: CSV}
    - {name: label_column, type: Integer, default: '0', optional: true}
    - {name: objective, type: String, default: 'reg:squarederror', optional: true}
    - {name: num_iterations, type: Integer, default: '200', optional: true}
    outputs:
    - {name: model, type: XGBoostModel}
    - {name: training_mean_absolute_error, type: Float}
    - {name: training_mean_squared_error, type: Float}
    - {name: training_root_mean_squared_error, type: Float}
    - {name: training_metrics, type: JsonObject}
    - {name: cv_mean_absolute_error, type: Float}
    - {name: cv_mean_squared_error, type: Float}
    - {name: cv_root_mean_squared_error, type: Float}
    - {name: cv_metrics, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train_and_cross-validate_regression/from_CSV/component.yaml'
    implementation:
      graph:
        tasks:
          Xgboost train:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                graphInput: {inputName: data}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                graphInput: {inputName: data}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                graphInput: {inputName: data}
              transform_code: df = df[["tips"]]
          Remove header:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format, type: CSV}
          Calculate regression metrics from csv:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict, type: Text}
          Split table into folds:
            componentRef: {digest: 9956223bcecc7294ca1afac39b60ada4a935a571d817c3dfbf2ea4a211afe3d1,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/e9b4b29b22a5120daf95b581b0392cd461a906f0/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'}
            arguments:
              table:
                graphInput: {inputName: data}
          Pandas Transform DataFrame in CSV format 2:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_3, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 2:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 2, type: CSV}
          Xgboost train 2:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_1, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 2:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_1, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 2, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format 3:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_2, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 3:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 3, type: CSV}
          Xgboost train 3:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_4, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Pandas Transform DataFrame in CSV format 4:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_4, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 4:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 4, type: CSV}
          Xgboost predict 3:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_4, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 3, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Calculate regression metrics from csv 2:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 4}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 3, type: Text}
          Pandas Transform DataFrame in CSV format 5:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_1, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 5:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 5, type: CSV}
          Calculate regression metrics from csv 3:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 5}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 2, type: Text}
          Xgboost train 4:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_2, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 4:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_2, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 4, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Calculate regression metrics from csv 4:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 3}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 4, type: Text}
          Xgboost train 5:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_5, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 5:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_5, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 5, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Xgboost train 6:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                taskOutput: {outputName: train_3, taskId: Split table into folds, type: CSV}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict 6:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                taskOutput: {outputName: test_3, taskId: Split table into folds, type: CSV}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train 6, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Calculate regression metrics from csv 5:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 2}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 6, type: Text}
          Pandas Transform DataFrame in CSV format 6:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: test_5, taskId: Split table into folds, type: CSV}
              transform_code: df = df[["tips"]]
          Remove header 6:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format 6, type: CSV}
          Calculate regression metrics from csv 6:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header 6}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict 5, type: Text}
          Aggregate regression metrics from csv:
            componentRef: {digest: 3e128130521eff8d43764f3dcb037316cdd6490ad2878df5adef416f7c2f3c19,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7ea9363fe201918d419fecdc00d1275e657ff712/components/ml_metrics/Aggregate_regression_metrics/component.yaml'}
            arguments:
              metrics_1:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 3, type: JsonObject}
              metrics_2:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 4, type: JsonObject}
              metrics_3:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 5, type: JsonObject}
              metrics_4:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 2, type: JsonObject}
              metrics_5:
                taskOutput: {outputName: metrics, taskId: Calculate regression metrics
                    from csv 6, type: JsonObject}
        outputValues:
          model:
            taskOutput: {outputName: model, taskId: Xgboost train, type: XGBoostModel}
          training_mean_absolute_error:
            taskOutput: {outputName: mean_absolute_error, taskId: Calculate regression
                metrics from csv, type: Float}
          training_mean_squared_error:
            taskOutput: {outputName: mean_squared_error, taskId: Calculate regression
                metrics from csv, type: Float}
          training_root_mean_squared_error:
            taskOutput: {outputName: root_mean_squared_error, taskId: Calculate regression
                metrics from csv, type: Float}
          training_metrics:
            taskOutput: {outputName: metrics, taskId: Calculate regression metrics from
                csv, type: JsonObject}
          cv_mean_absolute_error:
            taskOutput: {outputName: mean_absolute_error, taskId: Aggregate regression
                metrics from csv, type: Float}
          cv_mean_squared_error:
            taskOutput: {outputName: mean_squared_error, taskId: Aggregate regression
                metrics from csv, type: Float}
          cv_root_mean_squared_error:
            taskOutput: {outputName: root_mean_squared_error, taskId: Aggregate regression
                metrics from csv, type: Float}
          cv_metrics:
            taskOutput: {outputName: metrics, taskId: Aggregate regression metrics from
                csv, type: JsonObject}
- componentRef: {url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train_regression_and_calculate_metrics/from_CSV/component.yaml',
    digest: 7cc30370496b42107e5684738798d7893c02681b0a2579e288240655495271c4}
  annotations:
    GitHub commit:
      sha: d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      html_url: https://github.com/Ark-kun/pipeline_components/commit/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
      commit:
        url: https://api.github.com/repos/Ark-kun/pipeline_components/git/commits/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc
        author: {name: Alexey Volkov, email: alexey.volkov@ark-kun.com, date: '2021-07-16T04:28:12Z'}
        committer: {name: GitHub, email: noreply@github.com, date: '2021-07-16T04:28:12Z'}
        message: |-
          chore(components): Added canonical location for some components (#6060)

          * Components - Added canonical location for some components

          * Fixed the component.py files
      author: {login: 'https://api.github.com/users/Ark-kun', html_url: 'https://github.com/Ark-kun',
        avatar_url: 'https://avatars.githubusercontent.com/u/1829149?v=4'}
  data: |
    name: Xgboost train regression and calculate metrics on csv
    inputs:
    - {name: training_data, type: CSV}
    - {name: testing_data, type: CSV}
    - {name: label_column, type: Integer, default: '0', optional: true}
    - {name: objective, type: String, default: 'reg:squarederror', optional: true}
    - {name: num_iterations, type: Integer, default: '200', optional: true}
    outputs:
    - {name: model, type: XGBoostModel}
    - {name: mean_absolute_error, type: Float}
    - {name: mean_squared_error, type: Float}
    - {name: root_mean_squared_error, type: Float}
    - {name: metrics, type: JsonObject}
    metadata:
      annotations:
        author: Alexey Volkov <alexey.volkov@ark-kun.com>
        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train_regression_and_calculate_metrics/from_CSV/component.yaml'
    implementation:
      graph:
        tasks:
          Xgboost train:
            componentRef: {digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml'}
            arguments:
              training_data:
                graphInput: {inputName: training_data}
              label_column:
                graphInput: {inputName: label_column}
              num_iterations:
                graphInput: {inputName: num_iterations}
              objective:
                graphInput: {inputName: objective}
          Xgboost predict:
            componentRef: {digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml'}
            arguments:
              data:
                graphInput: {inputName: testing_data}
              model:
                taskOutput: {outputName: model, taskId: Xgboost train, type: XGBoostModel}
              label_column:
                graphInput: {inputName: label_column}
          Pandas Transform DataFrame in CSV format:
            componentRef: {digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'}
            arguments:
              table:
                graphInput: {inputName: testing_data}
              transform_code: df = df[["tips"]]
          Remove header:
            componentRef: {digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml'}
            arguments:
              table:
                taskOutput: {outputName: transformed_table, taskId: Pandas Transform DataFrame
                    in CSV format, type: CSV}
          Calculate regression metrics from csv:
            componentRef: {digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995,
              url: 'https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml'}
            arguments:
              true_values:
                taskOutput: {outputName: table, taskId: Remove header}
              predicted_values:
                taskOutput: {outputName: predictions, taskId: Xgboost predict, type: Text}
        outputValues:
          model:
            taskOutput: {outputName: model, taskId: Xgboost train, type: XGBoostModel}
          mean_absolute_error:
            taskOutput: {outputName: mean_absolute_error, taskId: Calculate regression
                metrics from csv, type: Float}
          mean_squared_error:
            taskOutput: {outputName: mean_squared_error, taskId: Calculate regression
                metrics from csv, type: Float}
          root_mean_squared_error:
            taskOutput: {outputName: root_mean_squared_error, taskId: Calculate regression
                metrics from csv, type: Float}
          metrics:
            taskOutput: {outputName: metrics, taskId: Calculate regression metrics from
                csv, type: JsonObject}
